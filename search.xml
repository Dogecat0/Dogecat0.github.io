<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Optimizing Semantic Search: A Two-Stage Reranker</title>
      <link href="/2025/06/30/Journal%20App/rerank-agent.html"/>
      <url>/2025/06/30/Journal%20App/rerank-agent.html</url>
      
        <content type="html"><![CDATA[<p>Previously in Synapse, we have a core feature: an agentic semantic search system (Still confused what <strong>actually</strong> makes RAG, a RAG and what’s the difference between it and semantic search? They both seem to intersect. But like Andrew Ng mentioned in one of the interviews, how to define if an agent is truly autonomous? How? maybe the definition does not matter, as long as the agent works itself.) that allows users to ask natural language questions about their activities. The initial version was a standard, three-step pipeline: a <strong>Planner</strong> agent to generate keywords, a <strong>Retrieval</strong> step using those keywords against the database, and a <strong>Synthesizer</strong> agent to generate a summary.</p><p>This worked, but it suffered from the classic limitation of keyword search—it lacked true semantic understanding. A query for “time spent on backend improvements” might miss an entry described as “refactored the authentication service.”</p><span id="more"></span><h3 id="The-Tempting-Anti-Pattern-Full-Database-Scan-with-an-LLM">The Tempting Anti-Pattern: Full Database Scan with an LLM</h3><p>The first idea that came up was straightforward: why not use the LLM’s intelligence for retrieval itself? The proposal was to iterate through our entire database in small batches, sending each batch to an LLM with the prompt, “Are any of these entries relevant to the user’s query?”</p><p>It’s easy to be captivated by the power of a new tool. However, after having some conversion with LLMs, this approach, while seems clever, is a production anti-pattern for several critical reasons:</p><ol><li><strong>Scalability:</strong> The number of LLM calls scales linearly with database size (<code>O(n)</code>). A user with a year’s worth of entries would face an unusable system, with search times stretching into minutes.</li><li><strong>Latency:</strong> The process is inherently sequential. The cumulative network and inference latency would create a terrible user experience.</li><li><strong>Cost:</strong> Whether using a local model (compute cycles) or a cloud API (dollars), this is an incredibly expensive way to search.</li></ol><p>The core mistake here is using a slow, high-precision tool (the LLM) for a fast, high-recall task (finding candidates).</p><h3 id="The-new-Approach-A-Two-Stage-Reranker">The new Approach: A Two-Stage Reranker</h3><p>A more robust and scalable architecture is a <strong>two-stage retrieval pipeline</strong> that incorporates a <strong>reranker</strong>. This pattern leverages the strengths of both the database and the LLM.</p><p>The new pipeline looks like this:</p><ol><li><strong>Planner Agent (Unchanged):</strong> Generate keywords from the user’s query.</li><li><strong>Broad Retrieval (High Recall):</strong> Use the keywords to query the database. Instead of fetching just 20 results, we now fetch a much larger set of candidates, say <code>take: 200</code>. This step is optimized for speed and recall—we’d rather have a few irrelevant results than miss a relevant one. This is what databases excel at.</li><li><strong>Reranker Agent (High Precision):</strong> This is the new, crucial step. We send the user’s query and the 200 candidate activities to a new LLM agent. Its <em>only</em> job is to analyze this pre-filtered list and return a sorted list of the most relevant activity IDs. This is where a better semantic analysis happens, but on a small, manageable dataset.</li><li><strong>Synthesizer Agent (Unchanged):</strong> The top 20 re-ranked results are passed to the final agent to generate the summary.</li></ol><p>This architecture is better because it aligns the tool with the task. The database handles the massive-scale filtering, and the LLM performs its nuanced understanding on a small, relevant subset of data.</p><h3 id="Refining-the-Implementation-Batching-the-Reranker">Refining the Implementation: Batching the Reranker</h3><p>In the final implementation, the reranker was taken a step further. Instead of sending <em>a defined max number</em> of candidates (say, 200) to the LLM in one massive prompt—which risks hitting context limits or the “lost in the middle” problem—the <code>rerankActivities</code> function now processes the candidates in batches.</p><p>It loops through the 200 candidates, sending 10 at a time to the reranker agent. It then aggregates the relevant IDs from all batches. This makes the reranking process more reliable and robust, ensuring consistent performance even if the number of initial candidates grows.</p><p>Now the new search system is more reliable and less prone to context issues, e.g, model timeout. It can handle larger datasets without overwhelming the LLM, while still providing high-quality, semantically relevant results.</p>]]></content>
      
      
      
        <tags>
            
            <tag> LLM </tag>
            
            <tag> Architecture </tag>
            
            <tag> AI </tag>
            
            <tag> RAG </tag>
            
            <tag> Search </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Building an Agentic RAG Pipeline for Journal Analysis</title>
      <link href="/2025/06/29/Journal%20App/agentic-rag-pipeline.html"/>
      <url>/2025/06/29/Journal%20App/agentic-rag-pipeline.html</url>
      
        <content type="html"><![CDATA[<p>Today, I implemented one of the new features proposed on roadmap in my personal journal analysis app: a semantic search and synthesis engine. Instead of a traditional keyword search, I opted for an “Agentic RAG” (Retrieval-Augmented Generation) pipeline. This approach leverages a local LLM (<code>gemma3n:latest</code>) not just for generating text, but for orchestrating the entire search process. This post outlines the design thinking behind this implementation and its future potential.</p><span id="more"></span><h3 id="The-Problem-Bridging-Human-Intent-and-Database-Queries">The Problem: Bridging Human Intent and Database Queries</h3><p>A user usually asks, <em>“What was my progress on the API refactor?”</em> A database, however, requires precise, literal terms. A direct search for “API” and “refactor” would miss relevant entries logged as “updated endpoints” or “fixed backend routes.”</p><p>The core challenge was to translate the user’s high-level <em>intent</em> into a low-level, effective database query without the overhead of maintaining a dedicated vector database.</p><h3 id="The-Solution-A-Two-Agent-Pipeline">The Solution: A Two-Agent Pipeline</h3><p>The current solution was to split the task between two specialized “agents,” each powered by a targeted LLM prompt.</p><p><strong>1. The Query Planner Agent:</strong><br>This is the first and most critical step. Its sole responsibility is to act as a “semantic bridge.”</p><ul><li><strong>Input:</strong> The user’s raw, natural language query.</li><li><strong>Task:</strong> Transform the query into a structured JSON array of effective search keywords.</li><li><strong>Prompt Engineering:</strong> The prompt instructs the LLM to identify core concepts, generate synonyms and related technical terms (e.g., “API” -&gt; <code>[&quot;endpoint&quot;, &quot;route&quot;, &quot;backend&quot;]</code>), and discard conversational stop words.</li><li><strong>Output:</strong> A clean array of keywords, like <code>[&quot;api&quot;, &quot;refactor&quot;, &quot;endpoint&quot;, &quot;route&quot;]</code>.</li></ul><p>This agent turns a fuzzy query into a high-signal set of search terms, dramatically improving retrieval recall.</p><p><strong>2. The Synthesizer Agent:</strong><br>Once the Planner provides the keywords, we perform a full-text search against the <code>description</code> and <code>notes</code> fields in our PostgreSQL database using Prisma. The top 20 most recent results are then passed to the second agent.</p><ul><li><strong>Input:</strong> The original user query and the context of the top 20 retrieved journal activities.</li><li><strong>Task:</strong> Generate a concise, markdown-formatted summary that directly answers the user’s query.</li><li><strong>Prompt Engineering:</strong> The prompt instructs the model to base its answer <em>only</em> on the provided context, preventing hallucination, and to structure the output for clear presentation.</li><li><strong>Output:</strong> A human-readable summary, ready for the UI.</li></ul><p>This entire pipeline is exposed via a single <code>POST</code> endpoint at <code>/api/journal/search</code>, which is consumed by a new Next.js page with a simple React frontend.</p><h3 id="Design-Trade-offs-and-Future-Optimizations">Design Trade-offs and Future Optimizations</h3><p>This architecture represents a deliberate set of trade-offs:</p><ul><li><strong>Simplicity over Precision:</strong> We avoided the complexity of <code>pgvector</code> and embedding backfills by using the LLM’s reasoning for the initial search. This is simpler to implement but may be less precise than a true vector similarity search.</li><li><strong>Speed over Comprehensiveness:</strong> By limiting the retrieval step to the top 20 activities (<code>take: 20</code>), we ensure a fast response and keep the LLM context manageable. However, this means the summary is based on a subset of all possible relevant data.</li></ul><p>This leads to a clear path for future optimization:</p><p><strong>Map-Reduce Summarization for Full-Database Analysis:</strong><br>To overcome the <code>take: 20</code> limitation, the next step for optimization is to implement a batch-processing pattern.</p><ol><li><strong>Full Retrieval:</strong> Fetch <em>all</em> matching activities from the database.</li><li><strong>Map:</strong> Break the results into manageable batches (e.g., of 20). Send each batch to the LLM to generate an intermediate summary.</li><li><strong>Reduce:</strong> Combine all the intermediate summaries and send them to the LLM (a new agent) one final time to produce a single, cohesive, and highly accurate final answer.</li></ol><p>This “Map-Reduce” approach would trade latency for a far more comprehensive analysis, making the tool even more powerful. It’s an exciting next step for the project.</p>]]></content>
      
      
      <categories>
          
          <category> Journal App </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LLM </tag>
            
            <tag> Agentic RAG </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>From .txt to Web App: Building a Journaling System with Gemini&#39;s Help</title>
      <link href="/2025/06/27/Journal%20App/journal-app.html"/>
      <url>/2025/06/27/Journal%20App/journal-app.html</url>
      
        <content type="html"><![CDATA[<p>For a while now, I’ve maintained a daily journal in a simple text file. It’s a straightforward system: I write the date, then list my activities under <code>WORK:</code> and <code>LIFE:</code> headers. This approach is simple and low-friction, but as the file grew, its limitations became obvious. Searching for specific activities was a <code>grep</code>-and-pray operation, and any form of analysis was purely manual. The data had no structure.</p><span id="more"></span><p>To solve this, I decided to build a simple web application. The goal was to provide a clean interface for entering and viewing journal entries, storing the data in a structured way, and paving the road for future analysis. I used Gemini as a development assistant throughout the process, particularly for the most tedious part: data migration.</p><h3 id="The-Technical-Stack">The Technical Stack</h3><p>Gemini helped me to keep the stack simple and modern, centered around TypeScript (which I never used to create app before):</p><ul><li><strong>Framework:</strong> Next.js (App Router)</li><li><strong>Styling:</strong> Tailwind CSS</li><li><strong>ORM/Database:</strong> Prisma with SQLite</li><li><strong>Data Processing:</strong> A Node.js script utilizing a local LLM (Google’s Gemma model, served via Ollama).</li></ul><p>The choice of SQLite via Prisma is intentional. It’s file-based, requires zero setup, and is more than sufficient for a personal project like this.</p><h3 id="The-First-Hurdle-Migrating-Unstructured-Text">The First Hurdle: Migrating Unstructured Text</h3><p>One of the biggest challenge was migrating my historical journal entries from a single <code>.txt</code> file into the structured database defined by Prisma schema.</p><p>Manually parsing each entry, extracting descriptions, estimating durations, and formatting notes would have been a significant time sink and incredibly error-prone. This is where I decided to leverage a small, local-friendly LLM to help me with this simple task.</p><h3 id="Using-an-LLM-for-Structured-Data-Extraction">Using an LLM for Structured Data Extraction</h3><p>I asked Gemini to write a script, import-journal.mjs, to automate the migration. The core idea is to feed each day’s raw text to an LLM and have it return a structured JSON object that I can directly insert into my database.<br>The key to making this reliable is prompt engineering combined with JSON Schema enforcement. Instead of just asking the model to “extract the data,” I provided a strict schema for the output.<br>First, I defined the expected output structure using Zod, which can then be converted to a JSON Schema.</p><pre class="line-numbers language-javascript" data-language="javascript"><code class="language-javascript"><span class="token comment">// Zod schema for the LLM's output</span><span class="token keyword">const</span> JournalDataSchema <span class="token operator">=</span> z<span class="token punctuation">.</span><span class="token function">object</span><span class="token punctuation">(</span><span class="token punctuation">&#123;</span>  <span class="token literal-property property">workActivities</span><span class="token operator">:</span> z<span class="token punctuation">.</span><span class="token function">array</span><span class="token punctuation">(</span>ActivitySchema<span class="token punctuation">)</span><span class="token punctuation">,</span>  <span class="token literal-property property">lifeActivities</span><span class="token operator">:</span> z<span class="token punctuation">.</span><span class="token function">array</span><span class="token punctuation">(</span>ActivitySchema<span class="token punctuation">)</span><span class="token punctuation">&#125;</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">required</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>Then, I used the openai library (pointed at my local Ollama endpoint) and its response_format feature to force the model’s output to conform to this schema. This ensures data reliability.</p><pre class="line-numbers language-javascript" data-language="javascript"><code class="language-javascript"><span class="token keyword">const</span> completion <span class="token operator">=</span> <span class="token keyword">await</span> openai<span class="token punctuation">.</span>chat<span class="token punctuation">.</span>completions<span class="token punctuation">.</span><span class="token function">create</span><span class="token punctuation">(</span><span class="token punctuation">&#123;</span>    <span class="token literal-property property">model</span><span class="token operator">:</span> <span class="token string">'gemma3n:latest'</span><span class="token punctuation">,</span> <span class="token comment">// My local model</span>    <span class="token literal-property property">messages</span><span class="token operator">:</span> <span class="token punctuation">[</span><span class="token punctuation">&#123;</span> <span class="token literal-property property">role</span><span class="token operator">:</span> <span class="token string">'user'</span><span class="token punctuation">,</span> <span class="token literal-property property">content</span><span class="token operator">:</span> prompt <span class="token punctuation">&#125;</span><span class="token punctuation">]</span><span class="token punctuation">,</span>    <span class="token literal-property property">temperature</span><span class="token operator">:</span> <span class="token number">0.1</span><span class="token punctuation">,</span>    <span class="token literal-property property">response_format</span><span class="token operator">:</span> <span class="token punctuation">&#123;</span>        <span class="token literal-property property">type</span><span class="token operator">:</span> <span class="token string">"json_schema"</span><span class="token punctuation">,</span>        <span class="token literal-property property">json_schema</span><span class="token operator">:</span> <span class="token punctuation">&#123;</span>            <span class="token literal-property property">name</span><span class="token operator">:</span> <span class="token string">'journal_data'</span><span class="token punctuation">,</span>            <span class="token literal-property property">schema</span><span class="token operator">:</span> z<span class="token punctuation">.</span><span class="token function">toJSONSchema</span><span class="token punctuation">(</span>JournalDataSchema<span class="token punctuation">)</span><span class="token punctuation">,</span>            <span class="token literal-property property">strict</span><span class="token operator">:</span> <span class="token boolean">true</span><span class="token punctuation">,</span>        <span class="token punctuation">&#125;</span><span class="token punctuation">,</span>    <span class="token punctuation">&#125;</span><span class="token punctuation">,</span><span class="token punctuation">&#125;</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="Use-of-Ollama">Use of Ollama</h3><p>Ollama is a powerful tool that allows me to run LLMs locally. It’s a quite straightforward set up, and especially for simple tasks like this where I just need to process text data with no necessity to pay for API calls.</p><h3 id="The-Journal-Application">The Journal Application</h3><p>With the data sorted, building the app itself was straightforward.</p><ol><li>API Endpoint (/api/journal): A single route handles GET and POST requests. The POST logic uses prisma.journalEntry.upsert which elegantly handles both creating a new entry and overwriting an existing one if the user is editing. This is controlled by a force flag from the client.</li><li>New/Edit Entry Form (/journal/new): A form that dynamically adds or removes activity blocks for both “Work” and “Life” categories. When a date is selected, it first checks if an entry for that date already exists. If so, it populates the form with existing data, switching to an “edit” mode.</li><li>View Entries (/journal/view): A two-pane layout. On the left, a timeline of all journal entries. Clicking an entry displays its full, formatted details on the right. This makes browsing history much more efficient than scrolling through a text file.</li></ol><h3 id="Next-Steps-Quantifying-Life-with-ML">Next Steps: Quantifying Life with ML</h3><p>Now that I have clean, structured way to input data, the real work can begin. The current application is a solid foundation, but the end goal is to use this data for more advanced analysis <strong>(yes, life analysis, sounds mad but who does not want a tangible way to measure their life if possible? Afterall life is like playing games at most times)</strong> and to improve the user experience. Here’s the (<strong>fantasized</strong>) roadmap:</p><ol><li><p><strong>LLM-Powered Semantic Search and Progress Synthesis:</strong> A standard keyword search is limited. I plan to implement a feature where a user can ask a natural language question like, “Show me my progress on the API refactor.” This would trigger a Retrieval-Augmented Generation (RAG) pipeline. First, the query would be converted to a vector embedding to perform a semantic search across all journal entries, finding relevant activities even if they don’t use the exact keywords. Then, the retrieved entries would be fed to an LLM to generate a concise summary of the progress, blockers, and time spent on that top</p></li><li><p><strong>Correlation Analysis and Reporting:</strong><br>With a growing dataset, I can build dashboards to find correlations. For example, does more time spent on “meetings” impact time allocated to “deep work” or “exercise”? A model could also generate automated weekly reports summarizing time allocation and highlighting trends.</p></li><li><p><strong>Voice-to-Data Pipeline for Hands-Free Entry:</strong> Typing is a point of friction. The next major UX improvement will be voice input. The plan is to integrate a speech-to-text model (like a self-hosted Whisper or a cloud API). The pipeline would be simple and modular:<br>User records their journal entry via microphone.<br>The audio is transcribed to raw text.<br>This raw text is fed into the exact same LLM data extraction endpoint I’ve already built.<br>This reuses the core backend logic, making it an efficient way to add a powerful new input method.</p></li></ol><p>As I am writing this blog and building the app, I’m constantly feeling the leverage that LLM can provide to solve a personal problem to individuals like you and me. The massive productivity boost let me focus on what truly matters: reflecting on my experiences and learning from them. :)</p>]]></content>
      
      
      <categories>
          
          <category> Journal App </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LLM </tag>
            
            <tag> Engineering </tag>
            
            <tag> Gemini </tag>
            
            <tag> Next.js </tag>
            
            <tag> Journaling </tag>
            
            <tag> Voice Input </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Containerization: A Case Study in Environment and Configuration Management</title>
      <link href="/2025/06/24/AI%20LLM/containerization.html"/>
      <url>/2025/06/24/AI%20LLM/containerization.html</url>
      
        <content type="html"><![CDATA[<p>Today’s objective was to containerize the SEC filing analysis application to ensure a consistent and reproducible runtime environment. This process involved not only creating the necessary Docker artifacts but also refactoring the application’s configuration to adhere to best practices for handling secrets and environment-specific variables.</p><span id="more"></span><h3 id="Initial-Containerization-and-Refactoring">Initial Containerization and Refactoring</h3><p>The first phase of the work focused on establishing the foundational components for containerization and improving the project’s production readiness.</p><p>The key commits included:</p><ol><li><strong>Addition of <code>Dockerfile</code> and <code>docker-compose.yml</code>:</strong> Standard files were created to define the application’s image and orchestrate the service stack, respectively. The <code>Dockerfile</code> is based on a Python image, installs dependencies from <code>requirements.txt</code>, and defines the container’s entry point.</li><li><strong>Dependency Management for Production:</strong> The <code>psycopg2-binary</code> package was added to <code>requirements.txt</code>. This is a critical step to move from a development-time database like SQLite to a production-grade PostgreSQL database, which the Docker environment is configured to use.</li><li><strong>Configuration Handling:</strong> Environment variable management within the application’s Python code (<code>main.py</code>, <code>database.py</code>) was refactored to rely on a centralized configuration system, anticipating the need to inject variables from the Docker environment.</li><li><strong>Source Control Hygiene:</strong> The <code>.gitignore</code> file was updated to exclude a broader range of files and artifacts, such as IDE configurations, environment files (<code>.env</code>), and cache directories, keeping the repository clean.</li></ol><h3 id="The-Core-Challenge-Propagating-Secrets-to-the-Container">The Core Challenge: Propagating Secrets to the Container</h3><p>Upon the initial <code>docker-compose up</code>, the application failed with an error indicating a missing API key for the OpenAI/OpenRouter service.</p><p><strong>Investigation:</strong><br>The root cause was identified quickly: the API credentials were hardcoded in a <code>launch.json</code> file, which is used exclusively by the VS Code debugger. These variables are not present in the runtime environment when the application is executed directly or via Docker.</p><p><strong>Solution:</strong><br>The standard and most secure practice is to decouple secrets and configuration from the application code and development tools. The following steps were executed to resolve the issue:</p><ol><li><p><strong>Centralize Secrets in <code>.env</code>:</strong> All environment-specific variables, including the existing <code>DATABASE_URL</code> and the new LLM credentials (<code>LLM_MODEL</code>, <code>OPENAI_API_KEY</code>, <code>OPENAI_BASE_URL</code>), were consolidated into a <code>.env</code> file at the project root. This file is explicitly excluded from version control by <code>.gitignore</code>.</p></li><li><p><strong>Update <code>docker-compose.yml</code>:</strong> While Docker Compose automatically loads variables from a <code>.env</code> file in the same directory, it is best practice to explicitly pass them to the service for clarity and control. The <code>web</code> service definition in <code>docker-compose.yml</code> was updated to pass the LLM-related environment variables to the container.</p></li><li><p><strong>Update Application Configuration (<code>config.py</code>):</strong> The application’s Pydantic-based settings module (<code>config.py</code>) was extended. The <code>Settings</code> class was modified to load <code>LLM_MODEL</code>, <code>OPENAI_API_KEY</code>, and <code>OPENAI_BASE_URL</code> from the environment, making them available to the application logic in a structured and validated manner.</p></li></ol><p>This three-step process—<code>.env</code> -&gt; <code>docker-compose.yml</code> -&gt; <code>config.py</code>—establishes a robust and conventional pattern for managing configuration that works seamlessly across local development and containerized environments.</p><h3 id="A-Note-on-Local-Environment-Troubleshooting">A Note on Local Environment Troubleshooting</h3><p>A secondary challenge encountered during this process was related to the local Docker setup on Ubuntu. Initial commands failed with <code>permission denied... docker.sock</code>, which evolved into <code>protocol not available</code> after switching contexts.</p><p>This is a common issue for users of Docker Desktop on Linux. The CLI was initially trying to connect to the traditional Docker daemon socket (<code>/var/run/docker.sock</code>), which was not running. The correct target was the Docker Desktop VM, managed via the <code>desktop-linux</code> context.</p><p>The persistent <code>protocol not available</code> error indicated that the Docker Desktop backend service itself had entered a faulty state. The resolution was to perform a <strong>Clean / Purge data</strong> from the Docker Desktop troubleshooter. This action resets the Docker engine, deleting all images and containers, but resolves underlying data corruption.</p><h3 id="Conclusion">Conclusion</h3><p>The primary goal of containerizing the application was achieved. The key takeaway is the critical importance of a well-defined configuration management strategy. But again, like many things in software development, the process always involves unexpected challenges and debugging efforts.</p>]]></content>
      
      
      <categories>
          
          <category> SEC Analysis </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
            <tag> Docker </tag>
            
            <tag> Docker Compose </tag>
            
            <tag> Configuration Management </tag>
            
            <tag> Environment Variables </tag>
            
            <tag> Troubleshooting </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>The Illusion of Quarterly Data: Correctly Calculating Financials from SEC Filings</title>
      <link href="/2025/06/22/AI%20LLM/illustion-of-quaterly-data.html"/>
      <url>/2025/06/22/AI%20LLM/illustion-of-quaterly-data.html</url>
      
        <content type="html"><![CDATA[<p>When building financial analysis tools, one of the most common and dangerous assumptions is that the financial data you receive—whether from an API or directly from SEC filings—represents discrete, isolated time periods. A “Q2” report, for instance, should contain data only for the second quarter. Right?</p><p>Unfortunately, this is often not the case. Raw SEC filings, specifically the quarterly <code>10-Q</code> and annual <code>10-K</code> reports, follow reporting rules that can be misleading if taken at face value. In this post, I’ll walk through the challenges of parsing these documents and present a robust Python solution to derive true, discrete quarterly financial figures.</p><span id="more"></span><h3 id="The-Core-Problem-Cumulative-vs-Discrete-Data">The Core Problem: Cumulative vs. Discrete Data</h3><p>The primary challenge lies in how different financial statements are reported throughout a fiscal year.</p><ol><li><p><strong>Income Statement</strong>: This is generally straightforward. A <code>10-Q</code> income statement covers the specific three-month period. However, the <code>10-K</code> income statement covers the <strong>entire twelve-month fiscal year</strong>, not just the fourth quarter. To get true Q4 data, you must manually calculate it:<br><code>Q4 = Annual (10-K) - Q1 - Q2 - Q3</code></p></li><li><p><strong>Cash Flow Statement &amp; Statement of Changes in Equity</strong>: These are the tricky ones. They are almost always reported on a <strong>cumulative, year-to-date basis</strong>.</p><ul><li>The <strong>Q1</strong> report shows data for the first three months.</li><li>The <strong>Q2</strong> report shows data for the <strong>first six months</strong> (<code>Q1 + actual Q2</code>).</li><li>The <strong>Q3</strong> report shows data for the <strong>first nine months</strong> (<code>Q2_cumulative + actual Q3</code>).</li><li>The <strong><code>10-K</code></strong> shows data for the full twelve months.</li></ul></li></ol><p>To get the actual data for each quarter, you must perform sequential subtraction:</p><ul><li><code>Actual Q2 = Q2_cumulative - Q1_cumulative</code></li><li><code>Actual Q3 = Q3_cumulative - Q2_cumulative</code></li><li><code>Actual Q4 = Annual (10-K) - Q3_cumulative</code></li></ul><h3 id="The-First-Trap-Assuming-a-Calendar-Year">The First Trap: Assuming a Calendar Year</h3><p>My initial approach was to group all filings by their calendar year (<code>2023</code>, <code>2024</code>, etc.) and perform the calculations within each group. This failed immediately.</p><p><strong>Why it’s wrong:</strong> Many companies (like Microsoft, Adobe, and Nike) have fiscal years that don’t align with the calendar year. A company’s fiscal year might run from July to June. Grouping by calendar year would split a single fiscal year’s filings into two different groups, making all subsequent calculations incorrect.</p><h3 id="The-Golden-Principle-Sequential-Grouping-by-10-K">The Golden Principle: Sequential Grouping by <code>10-K</code></h3><p>The correct approach is to ignore calendar dates for grouping and instead use the filings’ own reporting sequence. A fiscal year of reports follows a reliable pattern:</p><blockquote><p><strong>Three <code>10-Q</code> filings are followed by one <code>10-K</code> filing.</strong></p></blockquote><p>This <code>10-K</code> serves as the delimiter for a complete fiscal period. By processing a chronologically sorted list of filings, we can group them into fiscal years by collecting all <code>10-Q</code>s until we encounter a <code>10-K</code>, which concludes that group.</p><h3 id="The-Python-Implementation">The Python Implementation</h3><p>I implemented the logic with the help of Gemini in our FastAPI backend. The goal is to create a function, <code>_calculate_quarterly_figures</code>, that takes a list of formatted financial statement objects and returns a new list with the data adjusted.</p><p>First, a simple helper to subtract metrics from one statement dictionary using another.</p><p>Then, the main function that implements the sequential grouping and then applies our calculation rules.</p><p>The actual implementation is <code>straightforward</code> with the power of Gemini, but the logic is crucial.</p><h3 id="Key-Takeaways">Key Takeaways</h3><ol><li><strong>Understand Reporting Rules</strong>: Always be aware of how financial statements are reported. Don’t assume that a <code>10-Q</code> or <code>10-K</code> contains only the data which is well defined and formatted. Always check the reporting period and cumulative nature (or other kind) of the data.</li><li><strong>Group by Fiscal Year, Not Calendar Year</strong>: Use the <code>10-K</code> as the delimiter for fiscal years. This ensures that you correctly group all filings related to a single fiscal year.</li></ol>]]></content>
      
      
      <categories>
          
          <category> SEC Analysis </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
            <tag> FastAPI </tag>
            
            <tag> Finance </tag>
            
            <tag> Data Engineering </tag>
            
            <tag> SEC </tag>
            
            <tag> 10-K </tag>
            
            <tag> 10-Q </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Mess in SEC Financial Filings: A new Challenge of Data Extraction</title>
      <link href="/2025/06/19/AI%20LLM/mess-in-financial-filings.html"/>
      <url>/2025/06/19/AI%20LLM/mess-in-financial-filings.html</url>
      
        <content type="html"><![CDATA[<p>In the world of financial data extraction, the challenge of parsing and interpreting complex documents is ever-present. As I continue to refine my financial analysis tool, I’ve encountered a new set of challenges that highlight the messy nature of financial filings. This post delves into these issues and how they impact the accuracy and reliability of data extraction.</p><span id="more"></span><h3 id="The-Messy-Reality-of-Financial-Filings">The Messy Reality of Financial Filings</h3><p>Today I faced a particularly messy issue while extracting 10-K financials from a company’s filings. (Yes, 10-Qs don’t have such issue) There is a form called XBRL, which is an XML-based format used for financial reporting. While it is designed to be machine-readable, the reality is that these filings can be incredibly complex and inconsistent.</p><p>There is a specific case where you want to extract the labels from the Index of the XBRL document. These labels are supposed to be standardized like <code>concept</code>, <code>label</code>, <code>level</code>, <code>abstract</code>, <code>dimension</code>, and <code>date</code> which in this case is in <code>yy-mm-dd</code> format.</p><p>Ideal format:</p><pre class="line-numbers language-none"><code class="language-none">Index([&#39;concept&#39;, &#39;label&#39;, &#39;2025-03-31&#39;, &#39;level&#39;, &#39;abstract&#39;,       &#39;dimension&#39;],      dtype&#x3D;&#39;object&#39;)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>But in practice, the labels can be messy. For example, I encountered a case where the label for the date was formatted as <code>2025-03-31</code> but also had an additional label <code>2024-12-31</code> that was not in the expected format. And there were also labels like <code>2025-03-31 (Q1)</code> which were not in the expected format either.</p><p>Real format copied from my logs:</p><pre class="line-numbers language-none"><code class="language-none">Columns in statement:[&#39;concept&#39;, &#39;label&#39;, &#39;2022-12-31&#39;, &#39;2021-12-31&#39;, &#39;2020-12-31&#39;, &#39;level&#39;, &#39;abstract&#39;, &#39;dimension&#39;]Columns in statement: [&#39;concept&#39;, &#39;label&#39;, &#39;2022-06-30&#39;, &#39;level&#39;, &#39;abstract&#39;, &#39;dimension&#39;]Columns in statement: [&#39;concept&#39;, &#39;label&#39;, &#39;2022-06-30 (Q2)&#39;, &#39;2022-06-30&#39;, &#39;level&#39;, &#39;abstract&#39;, &#39;dimension&#39;]Columns in statement: [&#39;concept&#39;, &#39;label&#39;, &#39;2022-12-31&#39;, &#39;2021-12-31&#39;, &#39;level&#39;, &#39;abstract&#39;, &#39;dimension&#39;]<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><h3 id="Temporary-Solution-and-Manual-Intervention">Temporary Solution and Manual Intervention</h3><p>The solution to this problem is not straightforward. It requires a combination of heuristics and manual intervention to clean up the data. The steps I took at this moment:</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Columns in statement:"</span><span class="token punctuation">,</span> statement<span class="token punctuation">.</span>columns<span class="token punctuation">.</span>tolist<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment"># The columns are expected to be in the order:</span> <span class="token comment"># 'concept', 'label', &lt;date_columns...>, 'level', 'abstract', 'dimension'</span> <span class="token comment"># We want to keep the first date column, which represents the most recent period.</span> <span class="token comment"># Find the index of the 'level' column. If not found, this will raise a KeyError,</span> <span class="token comment"># which is caught below.</span> level_index <span class="token operator">=</span> statement<span class="token punctuation">.</span>columns<span class="token punctuation">.</span>get_loc<span class="token punctuation">(</span><span class="token string">"level"</span><span class="token punctuation">)</span> <span class="token comment"># The date columns are between 'label' (index 1) and 'level'.</span> <span class="token comment"># So they start at index 2.</span> date_columns <span class="token operator">=</span> statement<span class="token punctuation">.</span>columns<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">:</span>level_index<span class="token punctuation">]</span> <span class="token keyword">if</span> <span class="token keyword">not</span> date_columns<span class="token punctuation">.</span>empty<span class="token punctuation">:</span>     <span class="token comment"># The first date column is the most recent period, which is what we want.</span>     latest_date_col <span class="token operator">=</span> date_columns<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>     columns_to_keep <span class="token operator">=</span> <span class="token punctuation">[</span>         <span class="token string">"concept"</span><span class="token punctuation">,</span>         <span class="token string">"label"</span><span class="token punctuation">,</span>         latest_date_col<span class="token punctuation">,</span>         <span class="token string">"level"</span><span class="token punctuation">,</span>         <span class="token string">"abstract"</span><span class="token punctuation">,</span>         <span class="token string">"dimension"</span><span class="token punctuation">,</span>     <span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>At this point, I had to manually inspect the columns and decide which ones to keep. This is not ideal, but it is a reality of working with messy financial data and the more true fact is this situation may not be the last time I encounter such inconsistencies.</p><p>The best way to handle this is probably to select the biggest date column and then check if the other date columns are in the expected format. If they are not, we can either drop them or keep them for further analysis. And there is also one exception where the date column is in the format <code>2025-03-31 (Q1)</code>. In this case, we have to extract the date part from here instead of the similar column <code>2025-03-31</code> as the data in this column is not the expected ones.</p><h3 id="Final-Thoughts">Final Thoughts</h3><p>After roughly 2 hours of debugging and manual intervention, I was able to extract the right financials from the filings. Again, why 10-Qs don’t have such issue but only 10-Ks? I don’t know. But this seems become a norm for me to encounter while dealing with financial data.</p>]]></content>
      
      
      <categories>
          
          <category> SEC Analysis </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
            <tag> Data Extraction </tag>
            
            <tag> Financial Filings </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Reading Notes: Thinking, Fast and Slow by Daniel Kahneman</title>
      <link href="/2025/06/18/Reading/think-fast-and-slow.html"/>
      <url>/2025/06/18/Reading/think-fast-and-slow.html</url>
      
        <content type="html"><![CDATA[<p>This week I’ve started reading the second book in 2025 after finishing “The Psychology of Money” by Morgan Housel. The book is “Thinking, Fast and Slow” by Daniel Kahneman. This book delves into the dual systems of thought that govern our decision-making processes.</p><span id="more"></span><h3 id="Key-Concepts">Key Concepts</h3><ol><li><p><strong>Two Systems of Thinking</strong>:</p><ul><li><strong>System 1</strong>: Speed, efficiency, and reliance on heuristics, and mental shortcuts. It is responsible for our immediate impressions, gut reactions and many of the effortless mental activities. This is a legacy of our evolutionary past, where quick decisions were often necessary for survival. This system’s nature has also decided that is prone to systematical errors and biases, as it often jumps to conclusions and can be easily influenced by context and emotions.</li><li><strong>System 2</strong>: This system represents our conscious and effortful mental process. It is slow, analytical, and logical that we engage when faced with complex problems. It is responsible for self-control, careful consideration of options, and overriding the impulsive suggestions of System 1. As you might have already known now, this system is inherently lazy and often avoids engaging unless absolutely necessary.</li></ul></li><li><p><strong>WYSIATI</strong>:</p><ul><li>“What You See Is All There Is” (WYSIATI) highlights System 1’s tendency to form a coherent story based on the limited information it has, often ignoring what it doesn’t know. This can further lead to overconfidence in our judgements and a resistance to considering alternative perspectives.</li></ul></li></ol><h3 id="Reflection">Reflection</h3><p>While reading this book, I found myself in the past few years been trying to resist the influence of System 1 thinking without the realization that it is a natural part of our cognitive process. It seems human is inherently lazy and often relies on quick, intuitive judgments. Even the System 2 thinking, which is more deliberate and logical, can be influenced by the biases and shortcuts of System 1 due to its laziness nature. But this kind of self-awareness and training I think has been helpful in my decision-making process especially in the conetext of software engineering and personal finance. Compared to the previous self, I feel more equipped to recognize when System 1 is trying to take over and consciously engage System 2 to analyze the situation more thoroughly. But again, this is an ongoing process and as a human being, I think everyone should let System 1 to take over sometimes, as the book also points out that good mood and confident intuition can lead to better decisions in some cases.</p><h3 id="Chat-with-Gemini-2-5-Pro">Chat with Gemini 2.5 Pro</h3><p>I also had a chat with Gemini 2.5 pro about how does the concept of this book relates to current AI systems. Here are some findings:</p><ol><li><strong>System 1 vs. System 2 in AI</strong>:<ul><li>Current AI, particularly large language models (LLMs) and other deep learning systems, exhibits characteristics remarkably similar to System 1 thinking. They excel at pattern recognition, processing vast amounts of data to generate quick, intuitive responses. However, just like our System 1, these AI models can be prone to biases and errors, sometimes generating plausible but incorrect or nonsensical answers. They rely on heuristics and shortcuts learned from their training data, which can lead to flawed outputs as many of us already know which called “hallucinations”.</li><li>The slow, deliberate, and logical reasoning of System 2 is a major frontier in AI research. This type of thinking involves complex problem-solving, planning, and conscious, step-by-step analysis. While current AI can perform complex calculations, its ability to truly reason and understand in a flexible, human-like way is still developing.</li></ul></li><li><strong>Cognitive Biases</strong>:<ul><li>One of the most significant connections between “Thinking, Fast and Slow” and AI lies in the area of cognitive biases. Kahneman detailed numerous systematic errors in human judgment, and it’s now clear that AI can exhibit similar flaws. Interestingly, some research suggests that cognitive biases in AI don’t just stem from the data but can also arise organically from the way the models work.</li></ul></li><li><strong>Summary</strong>:<ul><li>In essence, “Thinking, Fast and Slow” provides not just a model for understanding the human mind, but also a valuable roadmap for the development and application of artificial intelligence.[15] It highlights the incredible potential of AI while also offering a crucial cautionary tale about the inherent risks of relying on purely intuitive, “fast” thinking, whether in humans or machines.</li></ul></li></ol>]]></content>
      
      
      <categories>
          
          <category> Reading </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Book Notes </tag>
            
            <tag> Psychology </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Refactoring for Resilience: Introducing a Database Caching Layer</title>
      <link href="/2025/06/14/AI%20LLM/persistent-database.html"/>
      <url>/2025/06/14/AI%20LLM/persistent-database.html</url>
      
        <content type="html"><![CDATA[<p>For our financial analysis tool, the latest series of updates focuses on an architectural enhancement: the integration of a persistent database layer for caching, performance tracking, and data retention. This post details the changes, the rationale behind them, and how they set the stage for future development.</p><span id="more"></span><h3 id="The-Problem-The-Cost-of-Being-Stateless">The Problem: The Cost of Being Stateless</h3><p>Previously, our application performed a complete, on-demand analysis for every API request. While this ensured the data was always fresh, it had significant drawbacks:</p><ol><li><strong>High Latency:</strong> Every request, even for the same company ticker, would trigger a series of intensive data processing and expensive Large Language Model (LLM) calls.</li><li><strong>Redundant Costs:</strong> Analyzing the same filings repeatedly incurred unnecessary API costs.</li><li><strong>No Historical Context:</strong> It was impossible to look back at a previous analysis or compare results over time without re-running the entire process.</li><li><strong>Poential Machine Learning Model (ML) Drift:</strong> As the underlying data changed, the application had no way to track how analyses evolved over time.</li></ol><p>The solution was clear: we needed a caching layer. Instead of a simple key-value store, I opted for a more structured solution using a SQLite database with SQLAlchemy.</p><h3 id="The-Solution-A-New-Database-Driven-Architecture">The Solution: A New Database-Driven Architecture</h3><p>This update introduces a complete database backend to store the results of comprehensive analyses. This is a foundational change that refactors the application’s core logic.</p><h4 id="Key-Components-of-the-New-Architecture">Key Components of the New Architecture:</h4><ol><li><p><strong>Database Models (<code>src/models/database_models.py</code>):</strong> I’ve introduced two SQLAlchemy models: <code>Company</code> and <code>Analysis</code>. The <code>Analysis</code> table is the star of the show, storing not just the JSON blob of a completed analysis, but also the specific parameters used to generate it (<code>include_business</code>, <code>max_10k_history</code>, etc.), performance metrics like <code>cost_usd</code> and <code>processing_time</code>, and a timestamp.</p></li><li><p><strong>Repository Pattern (<code>src/repositories/analysis_repository.py</code>):</strong> To maintain a clean separation of concerns, all direct database interactions are now handled by a new <code>AnalysisRepository</code>. This data access layer abstracts the database logic away from the API endpoints. The router is no longer responsible for how data is stored or retrieved; it simply asks the repository.</p></li><li><p><strong>Intelligent Caching Logic (<code>src/api/routers/dashboard_router.py</code>):</strong> The main <code>comprehensive_analysis</code> endpoint has been significantly enhanced. Before initiating a new analysis, it now queries the <code>AnalysisRepository</code> for an existing result that matches the exact request parameters (ticker, sections to include, history length) and is within a specified age limit (e.g., 24 hours). If a valid entry is found, it’s returned instantly, saving both time and money.</p></li><li><p><strong>Database Initialization (<code>src/db/init_db.py</code>):</strong> The application now automatically initializes the SQLite database and creates the necessary tables on startup, ensuring a smooth first-run experience.</p></li></ol><h3 id="Beyond-Caching-New-Endpoints-and-Robust-Testing">Beyond Caching: New Endpoints and Robust Testing</h3><p>This architectural shift also enabled new features and demanded a higher standard of testing.</p><ol><li><p><strong>Analysis History:</strong> I’ve added a new endpoint, <code>GET /analyses/&#123;ticker&#125;</code>, which allows users to retrieve a history of all previously run analyses for a given company. Another endpoint, <code>DELETE /analyses/&#123;ticker&#125;</code>, provides a way to clear the cache for a specific ticker.</p></li><li><p><strong>Comprehensive Testing (<code>tests/test_analysis_repository.py</code>):</strong> A change this significant requires rigorous testing. I’ve built a full test suite for the <code>AnalysisRepository</code> using <code>pytest</code>. These tests cover everything from basic save-and-retrieve operations to more complex scenarios like ensuring parameter-matching works correctly, stale data is properly filtered by <code>max_age_hours</code>, and database relationships are sound.</p></li></ol><h3 id="Retiring-Old-Code">Retiring Old Code</h3><p>With the introduction of this more sophisticated database layer, the old, file-based <code>OpenAICacheManager</code> has been deprecated. The new repository caches the entire, final analysis object, which is a much more efficient and powerful approach than caching individual, intermediate LLM calls.</p><h3 id="Quick-demo">Quick demo</h3><!-- add an image of the demo here --><img src="/2025/06/14/AI%20LLM/persistent-database/cached-googl-data-in-database.png" class="" title="Screenshot of the new database caching layer"><p class="text-center text-muted mt-2"><em>Cached GOOGL analysis result in database.</em></p><iframe width="100%" height="400" src="https://www.youtube.com/embed/cbP9WS5Qm4c"    title="SEC Filing Analysis Demo"     frameborder="0"     allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"     allowfullscreen></iframe><p class="text-center text-muted mt-2"><em>Now analysis is generated instantly.</em></p><h3 id="Final-Thoughts">Final Thoughts</h3><p>This change lays the groundwork for numerous future enhancements, such as tracking analysis changes over time, building more complex data visualizations, and providing deeper historical insights even for building machine learning models.</p>]]></content>
      
      
      <categories>
          
          <category> SEC Analysis </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
            <tag> FastAPI </tag>
            
            <tag> SQLAlchemy </tag>
            
            <tag> Caching </tag>
            
            <tag> Refactoring </tag>
            
            <tag> Software Engineering </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Seeing is Believing: A Tale of Two Demos (and the Power of Caching)</title>
      <link href="/2025/06/10/AI%20LLM/project-demo.html"/>
      <url>/2025/06/10/AI%20LLM/project-demo.html</url>
      
        <content type="html"><![CDATA[<p>In this post, I want to <em>show</em> you the demo for the project after making the caching work. Below are two demos of the exact same analysis request. The only difference is that one is a cold start, and the other benefits from the now-functional cache.</p><span id="more"></span><h3 id="Demo-1-The-Cold-Start-Without-Cache">Demo 1: The Cold Start (Without Cache)</h3><p>This first demo shows what happens when the application analyzes a ticker for the very first time.</p><p><strong>What’s happening behind the scenes:</strong></p><ol><li>The user requests an analysis for a new ticker.</li><li>The data layer connects to the SEC EDGAR database through HTTP requests.</li><li>It downloads multiple large filing documents (10-Ks and 10-Qs). This network operation is the source of the delay.</li><li>Once the data is downloaded, the AI analysis proceeds.</li><li>The filing data and LLM response data (in this case, OpenAI response) are saved to a local cache for future use.</li></ol><iframe width="100%" height="400" src="https://www.youtube.com/embed/yFU5igLJ2dk"     title="SEC Filing Analysis Demo"     frameborder="0"     allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"     allowfullscreen></iframe><p class="text-center text-muted mt-2"><em>Analysis without a pre-existing cache. The analysis roughly takes <strong>40 seconds</strong> to complete.</em></p><h3 id="Demo-2-The-Warm-Start-With-Cache">Demo 2: The Warm Start (With Cache)</h3><p>Now, let’s run the <em>exact same analysis request</em> a second time.</p><p><strong>What’s happening now:</strong></p><ol><li>The user requests the same analysis again.</li><li>The data layer first checks the local cache directory we configured in the <code>lifespan</code> function.</li><li><strong>Cache Hit!</strong> It finds the required filing documents then LLM response on the local disk.</li><li>The data is loaded directly from the disk, completely bypassing the slow internet download.</li><li>The AI analysis proceeds within <strong>10 seconds</strong>.</li></ol><iframe width="100%" height="400" src="https://www.youtube.com/embed/GT1UkYdeBbA"     title="SEC Filing Analysis Demo"     frameborder="0"     allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"     allowfullscreen></iframe><p class="text-center text-muted mt-2"><em>The exact same analysis, but this time the data is served from the local cache.</em></p><p>The difference is quite exciting. The analysis is now nearly <em>‘instantaneous’</em>.</p><p><strong>PS:</strong> Never thought this would be the first Youtube video I’d ever made and posted online, but here we are as embedding an iframe in Hexo for blogs seems quite straightforward. I hope you enjoyed the demo and found it insightful. :)</p>]]></content>
      
      
      <categories>
          
          <category> SEC Analysis </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
            <tag> FastAPI </tag>
            
            <tag> AI </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>From Sequential to Supersonic: A Developer&#39;s Journey into Parallel LLM Queries</title>
      <link href="/2025/06/09/AI%20LLM/from-sequential-to-supersonic.html"/>
      <url>/2025/06/09/AI%20LLM/from-sequential-to-supersonic.html</url>
      
        <content type="html"><![CDATA[<p>When I first started building this application, my focus was on a simple goal: use a Large Language Model (LLM) to read dense SEC filings and extract structured, easy-to-digest insights. The initial prototype was magical. I could feed it the “Business” section of a 10-K filing, and it would return a beautiful JSON object with competitive advantages, key products, and more.</p><p>But then, I started to find out each analysis takes time especially when I wanted to analyze multiple sections like Business, Management’s Discussion and Analysis (MD&amp;A), Risk Factors, and Financials. Each of these sections required a separate LLM API call, and I was making those calls one after another in a synchronous loop.</p><p>That’s when I hit the wall, together with the previous ‘cache’ implementation that wasn’t caching anything. The user experience was not ideal, and I knew I had to do something about it. So in this post I will show how to transform a sequential script to a multi-layered concurrent application that feels responsive and powerful to reduce the wait time from <strong>a couple of min</strong> to just seconds.</p><span id="more"></span><h3 id="The-Agony-of-the-Synchronous-Loop">The Agony of the Synchronous Loop</h3><p>My initial, naive implementation looked something like this in spirit:</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># The "Before" picture - a painful, sequential process</span><span class="token keyword">def</span> <span class="token function">get_comprehensive_analysis</span><span class="token punctuation">(</span>ticker<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token comment"># Each of these is a blocking LLM API call</span>    business_analysis <span class="token operator">=</span> analyze_business_section<span class="token punctuation">(</span>text<span class="token punctuation">)</span> <span class="token comment"># Wait...</span>    mda_analysis <span class="token operator">=</span> analyze_mda_section<span class="token punctuation">(</span>text<span class="token punctuation">)</span> <span class="token comment"># Wait some more...</span>    risk_analysis <span class="token operator">=</span> analyze_risk_section<span class="token punctuation">(</span>text<span class="token punctuation">)</span> <span class="token comment"># Still waiting...</span>    financials <span class="token operator">=</span> analyze_financials<span class="token punctuation">(</span>data<span class="token punctuation">)</span> <span class="token comment"># And wait again...</span>    <span class="token keyword">return</span> <span class="token punctuation">&#123;</span>        <span class="token string">"business"</span><span class="token punctuation">:</span> business_analysis<span class="token punctuation">,</span>        <span class="token string">"mda"</span><span class="token punctuation">:</span> mda_analysis<span class="token punctuation">,</span>        <span class="token string">"risks"</span><span class="token punctuation">:</span> risk_analysis<span class="token punctuation">,</span>        <span class="token string">"financials"</span><span class="token punctuation">:</span> financials<span class="token punctuation">,</span>    <span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>A full analysis could take up to <strong>three and half minute</strong>. This wasn’t just a bad user experience; it was an inefficient use of resources. I knew there had to be a better way. This realization kickstarted my journey into the world of Python concurrency.</p><h3 id="Level-1-High-Level-Concurrency-with-asyncio">Level 1: High-Level Concurrency with <code>asyncio</code></h3><p>The first and most obvious optimization was to recognize that the analyses for Business, MD&amp;A, and Risks were completely independent of each other. There was no reason to wait for one to finish before starting the next. This is a classic use case for asynchronous programming.</p><p>Since my application uses FastAPI, <code>asyncio</code> was the natural choice. By converting my analysis functions to <code>async def</code> and using <code>asyncio.gather</code>, I could fire off all the top-level requests at once.</p><p>My <code>dashboard_router.py</code> now orchestrates this nicely:</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># src/api/routers/dashboard_router.py (simplified)</span><span class="token keyword">import</span> asyncio<span class="token keyword">from</span> src<span class="token punctuation">.</span>analysis<span class="token punctuation">.</span>business <span class="token keyword">import</span> BusinessAnalyzer<span class="token keyword">from</span> src<span class="token punctuation">.</span>analysis<span class="token punctuation">.</span>mda <span class="token keyword">import</span> MDAAnalyzer<span class="token keyword">from</span> src<span class="token punctuation">.</span>analysis<span class="token punctuation">.</span>risk_factor <span class="token keyword">import</span> RiskFactorAnalyzer<span class="token keyword">async</span> <span class="token keyword">def</span> <span class="token function">comprehensive_analysis</span><span class="token punctuation">(</span>ticker<span class="token punctuation">:</span> <span class="token builtin">str</span><span class="token punctuation">,</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token comment"># ... (load company data)</span>    tasks <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>    task_names <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>    <span class="token comment"># Create analyzers</span>    business_analyzer <span class="token operator">=</span> BusinessAnalyzer<span class="token punctuation">(</span>use_concurrent<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>    mda_analyzer <span class="token operator">=</span> MDAAnalyzer<span class="token punctuation">(</span>use_concurrent<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>    risk_analyzer <span class="token operator">=</span> RiskFactorAnalyzer<span class="token punctuation">(</span>use_concurrent<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>    <span class="token comment"># Schedule all top-level analyses to run concurrently</span>    <span class="token keyword">if</span> include_business<span class="token punctuation">:</span>        tasks<span class="token punctuation">.</span>append<span class="token punctuation">(</span>extract_section_analysis_async<span class="token punctuation">(</span>company_manager<span class="token punctuation">,</span> <span class="token string">"Business"</span><span class="token punctuation">,</span> business_analyzer<span class="token punctuation">)</span><span class="token punctuation">)</span>        task_names<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token string">"business"</span><span class="token punctuation">)</span>    <span class="token keyword">if</span> include_mda<span class="token punctuation">:</span>        tasks<span class="token punctuation">.</span>append<span class="token punctuation">(</span>extract_section_analysis_async<span class="token punctuation">(</span>company_manager<span class="token punctuation">,</span> <span class="token string">"Management's Discussion and Analysis (MD&amp;A)"</span><span class="token punctuation">,</span> mda_analyzer<span class="token punctuation">)</span><span class="token punctuation">)</span>        task_names<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token string">"mda"</span><span class="token punctuation">)</span>    <span class="token keyword">if</span> include_risk_factors<span class="token punctuation">:</span>        tasks<span class="token punctuation">.</span>append<span class="token punctuation">(</span>extract_section_analysis_async<span class="token punctuation">(</span>company_manager<span class="token punctuation">,</span> <span class="token string">"Risk Factors"</span><span class="token punctuation">,</span> risk_analyzer<span class="token punctuation">)</span><span class="token punctuation">)</span>        task_names<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token string">"risk_factors"</span><span class="token punctuation">)</span>    <span class="token comment"># Run all tasks in parallel and wait for them all to complete</span>    results <span class="token operator">=</span> <span class="token keyword">await</span> asyncio<span class="token punctuation">.</span>gather<span class="token punctuation">(</span><span class="token operator">*</span>tasks<span class="token punctuation">)</span>    <span class="token comment"># ... (process results)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>The result?</strong> The total wait time was no longer the <em>sum</em> of all analysis times, but rather the time of the <em>single longest</em> analysis. Now without cache, a full analysis could take up to just <strong>40 seconds</strong>. A huge win!</p><h3 id="Level-2-Deeper-Parallelism-with-ThreadPoolExecutor">Level 2: Deeper Parallelism with <code>ThreadPoolExecutor</code></h3><p>As I celebrated my newfound speed, a new question emerged: could I go even deeper? A single analysis, like the “Business” section, is itself a request for multiple, independent pieces of information:</p><ul><li>Operational Overview</li><li>Key Products</li><li>Competitive Advantages</li><li>Strategic Initiatives</li><li>Business Segments</li><li>And more…</li></ul><p>My initial prompt was a massive, one-body instruction asking the LLM to extract everything at once. While it worked, it was brittle. The LLM would sometimes miss a section or get the format slightly wrong.</p><p>This led to my second “aha!” moment: what if I broke that one giant API call into several smaller, parallel calls?</p><p>This is where <code>concurrent.futures.ThreadPoolExecutor</code> came in. While <code>asyncio</code> is perfect for managing <code>async/await</code> coroutines, the <code>ThreadPoolExecutor</code> lets you run any function in a separate thread. This was perfect for my analyzer classes.</p><p>Here’s how I implemented it in my <code>BusinessAnalyzer</code>:</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># src/analysis/business.py (simplified)</span><span class="token keyword">import</span> concurrent<span class="token punctuation">.</span>futures<span class="token keyword">class</span> <span class="token class-name">BusinessAnalyzer</span><span class="token punctuation">(</span>BaseAnalyzer<span class="token punctuation">[</span>BusinessAnalysisSection<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> use_concurrent<span class="token punctuation">:</span> <span class="token builtin">bool</span> <span class="token operator">=</span> <span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>use_concurrent <span class="token operator">=</span> use_concurrent        <span class="token keyword">if</span> self<span class="token punctuation">.</span>use_concurrent<span class="token punctuation">:</span>            <span class="token comment"># Each component gets its own specialized analyzer and prompt</span>            self<span class="token punctuation">.</span>_init_component_analyzers<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">extract_concurrent</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> text<span class="token punctuation">:</span> <span class="token builtin">str</span><span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> BusinessAnalysisSection<span class="token punctuation">:</span>        self<span class="token punctuation">.</span>logger<span class="token punctuation">.</span>info<span class="token punctuation">(</span><span class="token string">"Starting concurrent business analysis extraction"</span><span class="token punctuation">)</span>        component_results <span class="token operator">=</span> <span class="token punctuation">&#123;</span><span class="token punctuation">&#125;</span>        tasks <span class="token operator">=</span> <span class="token punctuation">&#123;</span>            <span class="token string">"operational_overview"</span><span class="token punctuation">:</span> self<span class="token punctuation">.</span>_extract_component<span class="token punctuation">,</span>            <span class="token string">"key_products"</span><span class="token punctuation">:</span> self<span class="token punctuation">.</span>_extract_component<span class="token punctuation">,</span>            <span class="token string">"competitive_advantages"</span><span class="token punctuation">:</span> self<span class="token punctuation">.</span>_extract_component<span class="token punctuation">,</span>            <span class="token comment"># ... and so on for all sub-sections</span>        <span class="token punctuation">&#125;</span>        <span class="token comment"># Use a thread pool to run all sub-extractions in parallel</span>        <span class="token keyword">with</span> concurrent<span class="token punctuation">.</span>futures<span class="token punctuation">.</span>ThreadPoolExecutor<span class="token punctuation">(</span>max_workers<span class="token operator">=</span><span class="token number">8</span><span class="token punctuation">)</span> <span class="token keyword">as</span> executor<span class="token punctuation">:</span>            future_to_component <span class="token operator">=</span> <span class="token punctuation">&#123;</span>                executor<span class="token punctuation">.</span>submit<span class="token punctuation">(</span>task<span class="token punctuation">,</span> component<span class="token punctuation">,</span> text<span class="token punctuation">)</span><span class="token punctuation">:</span> component                <span class="token keyword">for</span> component<span class="token punctuation">,</span> task <span class="token keyword">in</span> tasks<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span>            <span class="token punctuation">&#125;</span>            <span class="token keyword">for</span> future <span class="token keyword">in</span> concurrent<span class="token punctuation">.</span>futures<span class="token punctuation">.</span>as_completed<span class="token punctuation">(</span>future_to_component<span class="token punctuation">)</span><span class="token punctuation">:</span>                component_name <span class="token operator">=</span> future_to_component<span class="token punctuation">[</span>future<span class="token punctuation">]</span>                <span class="token keyword">try</span><span class="token punctuation">:</span>                    result <span class="token operator">=</span> future<span class="token punctuation">.</span>result<span class="token punctuation">(</span><span class="token punctuation">)</span>                    component_results<span class="token punctuation">[</span>component_name<span class="token punctuation">]</span> <span class="token operator">=</span> result                <span class="token keyword">except</span> Exception <span class="token keyword">as</span> e<span class="token punctuation">:</span>                    self<span class="token punctuation">.</span>logger<span class="token punctuation">.</span>error<span class="token punctuation">(</span><span class="token string">"Error extracting %s: %s"</span><span class="token punctuation">,</span> component_name<span class="token punctuation">,</span> <span class="token builtin">str</span><span class="token punctuation">(</span>e<span class="token punctuation">)</span><span class="token punctuation">)</span>                    <span class="token comment"># Crucially, I added a fallback to the single-prompt method</span>                    <span class="token keyword">return</span> self<span class="token punctuation">.</span>extract<span class="token punctuation">(</span>text<span class="token punctuation">)</span> <span class="token comment"># Fallback to standard extraction</span>        <span class="token keyword">return</span> BusinessAnalysisSection<span class="token punctuation">(</span><span class="token operator">**</span>component_results<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>This two-tiered approach was a game-changer:</p><ol><li><strong><code>asyncio.gather</code></strong> orchestrates the high-level analysis tasks (Business, MD&amp;A, etc.).</li><li>Within each of those tasks, a <strong><code>ThreadPoolExecutor</code></strong> runs multiple, smaller, more focused LLM queries in parallel.</li></ol><p>This not only made the application even faster but also brought some unexpected benefits:</p><ul><li><strong>Increased Reliability:</strong> Smaller, focused prompts are less likely to confuse the LLM.</li><li><strong>Improved Resilience:</strong> If the “Key Products” extraction fails, the other business components might still succeed. My code even includes a fallback to the “big prompt” method if the concurrent approach fails, making the system more robust.</li><li><strong>Better Maintainability:</strong> It’s far easier to debug and refine a small prompt for “Key Products” than it is to tweak one monolithic prompt.</li><li><strong>Enhanced LLM Response Quality:</strong> Smaller, focused prompts often yield more accurate, deeper insights, as the LLM can concentrate on a specific aspect of the business without being overwhelmed by too much context.</li></ul><h3 id="A-Practical-Lesson-Wrapping-Lists-for-LLM-Sanity">A Practical Lesson: Wrapping Lists for LLM Sanity</h3><p>A quick but important lesson I learned was about JSON schema design for LLM outputs. I initially asked the model to return a root-level JSON array for things like a list of products (e.g., <code>[&#123;&quot;name&quot;: ...&#125;, &#123;&quot;name&quot;: ...&#125;]</code>). This was surprisingly flaky.</p><p>The fix was simple but powerful: I wrapped the list in an object.</p><p>Instead of <code>List[KeyProduct]</code>, my Pydantic model became:</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># src/models/business_model.py</span><span class="token keyword">class</span> <span class="token class-name">KeyProductList</span><span class="token punctuation">(</span>BaseModel<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""Wrapper model for a list of key products."""</span>    items<span class="token punctuation">:</span> List<span class="token punctuation">[</span>KeyProduct<span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>Now, I ask the LLM for <code>&#123;&quot;items&quot;: [...]&#125;</code>. This simple change made the output parsing significantly more reliable.</p><h3 id="Conclusion-Concurrency-is-a-Superpower-for-LLM-Apps">Conclusion: Concurrency is a Superpower for LLM Apps</h3><p>My journey from a slow, sequential script to a multi-layered concurrent application was incredibly rewarding. It transformed the user experience and taught me a valuable lesson: for I/O-bound applications like those interacting with LLMs, concurrency isn’t a “nice-to-have”—it’s a core architectural requirement for building something fast, reliable, and scalable.</p><p>By combining the strengths of <code>asyncio</code> for top-level task management and <code>ThreadPoolExecutor</code> for deeper, parallel sub-tasks, I was able to build a system that feels responsive and powerful, turning long waits into a few moments of processing. If you’re building with LLMs, I can’t recommend enough that you dive into Python’s concurrency toolset.</p>]]></content>
      
      
      <categories>
          
          <category> SEC Analysis </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
            <tag> FastAPI </tag>
            
            <tag> LLM </tag>
            
            <tag> Concurrency </tag>
            
            <tag> Performance </tag>
            
            <tag> asyncio </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Debugging the Engine: Fixing a Broken Filing Cache</title>
      <link href="/2025/06/08/AI%20LLM/refining-the-filing-cache.html"/>
      <url>/2025/06/08/AI%20LLM/refining-the-filing-cache.html</url>
      
        <content type="html"><![CDATA[<p>After running a few analyses, I noticed from the logs that with the existing edgar caching strategy, it still made HTTP requests and my terminal logs showed that the app was re-downloading the same SEC filings over and over.</p><p>The caching layer, which was supposed to prevent this, was clearly not working. This post is a chronicle of a classic developer experience: realizing a core feature isn’t working as intended and diving in to fix it.</p><span id="more"></span><h3 id="The-Problem-A-Cache-That-Wasn’t-Caching">The Problem: A Cache That Wasn’t Caching</h3><p>The initial implementation of my <code>CompanyDataManager</code> and <code>EdgarCache</code> were designed to be simple. The idea was that it would use a cached HTTP client from <code>hishel</code> to avoid repeatedly hitting the SEC’s EDGAR database. However, I made a critical architectural mistake.</p><p>The <code>edgar</code> library needs its HTTP client caching <code>edgar.httpclient_cache</code> to be initialized properly to fetch resources like <code>company_tickers.json</code>.</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">edgar<span class="token punctuation">.</span>httpclient_cache<span class="token punctuation">.</span>install_cached_client<span class="token punctuation">(</span>    cache_directory<span class="token operator">=</span>cache_directory<span class="token punctuation">,</span>    controller_args<span class="token operator">=</span><span class="token punctuation">&#123;</span>        <span class="token string">"force_cache"</span><span class="token punctuation">:</span> <span class="token boolean">True</span><span class="token punctuation">,</span>    <span class="token comment"># Cache all responses</span>        <span class="token string">"allow_stale"</span><span class="token punctuation">:</span> <span class="token boolean">True</span><span class="token punctuation">,</span>    <span class="token comment"># Serve stale cache if revalidation fails (good for offline/rate limits)</span>        <span class="token string">"cacheable_methods"</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">"GET"</span><span class="token punctuation">,</span> <span class="token string">"POST"</span><span class="token punctuation">]</span> <span class="token comment"># Methods to cache</span>    <span class="token punctuation">&#125;</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>By moving the lifespan function from <code>src/api/routers/dashboard_router.py</code> to <code>src/api/main.py</code> and applying it to the main FastAPI application, I ensured that <code>install_cached_client</code> is called once when the application starts.<br>This means that the HTTP client is configured with caching enabled right from the start, allowing it to cache responses effectively.</p><p>Also, the <code>lifespan</code> is a special async context manager that FastAPI executes on startup. Any code before the <code>yield</code> runs once when the server boots up. This is the perfect place to set up global resources like our HTTP client and its cache.</p><p>I refactored <code>src/api/main.py</code> to include this:</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> contextlib <span class="token keyword">import</span> asynccontextmanager<span class="token keyword">from</span> fastapi <span class="token keyword">import</span> FastAPI<span class="token keyword">from</span> edgar<span class="token punctuation">.</span>httpclient_cache <span class="token keyword">import</span> install_cached_client<span class="token keyword">from</span> pathlib <span class="token keyword">import</span> Path<span class="token decorator annotation punctuation">@asynccontextmanager</span><span class="token keyword">async</span> <span class="token keyword">def</span> <span class="token function">lifespan</span><span class="token punctuation">(</span>app<span class="token punctuation">:</span> FastAPI<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""    Lifespan event handler for the FastAPI application.    Initializes the Edgar HTTP client with caching ONCE on startup.    """</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Application startup: Initializing EDGAR client cache..."</span><span class="token punctuation">)</span>    cache_dir <span class="token operator">=</span> Path<span class="token punctuation">(</span><span class="token string">".cache/edgar"</span><span class="token punctuation">)</span>    cache_dir<span class="token punctuation">.</span>mkdir<span class="token punctuation">(</span>parents<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> exist_ok<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>        <span class="token comment"># This now runs only once!</span>    install_cached_client<span class="token punctuation">(</span>cache_dir<span class="token punctuation">)</span>        <span class="token keyword">yield</span>        <span class="token comment"># Code here would run on shutdown (e.g., for cleanup)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Application shutdown."</span><span class="token punctuation">)</span>app <span class="token operator">=</span> FastAPI<span class="token punctuation">(</span>lifespan<span class="token operator">=</span>lifespan<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="The-Result-A-Night-and-Day-Difference">The Result: A Night and Day Difference</h3><p><strong>Cache Directory:</strong> By default, src.data.edgar_cache.setup_python_edgar_caching will use the PYTHON_EDGAR_CACHE_DIR constant defined in edgar_cache.py (which is Path(“python_edgar_hishel_cache”)). This directory will be created if it doesn’t exist.</p><p><strong>Caching Behavior:</strong> The function configures edgartools to use hishel with force_cache=True and allow_stale=True. This means:</p><p><code>force_cache=True</code>: All responses from edgartools will be cached.<br><code>allow_stale=True</code>: If the cache has a response but an update fails (e.g., network issue), the stale cached response will be served. This is good for offline use or if you hit rate limits.</p><p>The impact was immediate.</p><ul><li><strong>First Request for a Ticker:</strong> Takes around 20s with cached OpenAI response as it downloads and caches the filings.</li><li><strong>Subsequent Requests for the Same Ticker:</strong> Nearly halved to 10s per analysis. The data is pulled directly from the local disk cache.</li></ul><p>This not only dramatically improves the user experience but also makes the application a better citizen by reducing the load on the SEC’s servers and slashing my data transfer.</p><h3 id="What’s-Next">What’s Next?</h3><p>With a fast and reliable data pipeline, we can finally start to trust our development process. The broken cache was a major obstacle, and fixing it was a necessary step before we could move on to the most important part: evaluation the model’s output as I’ve noticed there are some discrepency in 10-K filings and the model’s output.</p>]]></content>
      
      
      <categories>
          
          <category> SEC Analysis </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
            <tag> FastAPI </tag>
            
            <tag> Architecture </tag>
            
            <tag> AI </tag>
            
            <tag> Caching </tag>
            
            <tag> Optimization </tag>
            
            <tag> Copilot </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Measuring What Matters: Building an Evaluation and Cost-Tracking Framework for my AI Financial Analyst</title>
      <link href="/2025/06/07/AI%20LLM/measuring-what-matters-evaluation-and-cost-tracking.html"/>
      <url>/2025/06/07/AI%20LLM/measuring-what-matters-evaluation-and-cost-tracking.html</url>
      
        <content type="html"><![CDATA[<p>In this post, we will dive a bit into model evaluation. Building with Large Language Models (LLMs) presents two major challenges:</p><ol><li><strong>How do you know if the output is any good?</strong></li><li><strong>How do you prevent API costs from spiraling out of control?</strong></li></ol><p>Today, I tackled both of these head-on by building a robust evaluation and cost-tracking framework. It’s an (again) interesting learning journey but important step for moving from a fun prototype to a reliable tool.</p><span id="more"></span><h3 id="The-Goal-Confidence-in-Quality-and-Cost">The Goal: Confidence in Quality and Cost</h3><p>I wanted a system that could answer these questions automatically:</p><ul><li>When I change a prompt, does the analysis quality get better or worse?</li><li>Which part of the analysis is the most expensive?</li><li>Can I quantify the trade-off between cost and quality for different LLM models?</li></ul><p>To do this, My AI assistants have helped me build three key components: a golden data generator, a model evaluator, and a centralized cost registry.</p><h3 id="Step-1-Establishing-the-“Golden-Truth”">Step 1: Establishing the “Golden Truth”</h3><p>You can’t evaluate quality without a benchmark. I created a script, <code>create_golden_data.py</code>, to generate a “golden set” of ideal analysis outputs for a few representative tickers (like AMZN, MSFT, MA).</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># scripts/create_golden_data.py</span><span class="token keyword">async</span> <span class="token keyword">def</span> <span class="token function">create_golden_file</span><span class="token punctuation">(</span>ticker<span class="token punctuation">:</span> <span class="token builtin">str</span><span class="token punctuation">,</span> email<span class="token punctuation">:</span> <span class="token builtin">str</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""Run comprehensive analysis and save results as a golden file."""</span>    logger<span class="token punctuation">.</span>info<span class="token punctuation">(</span><span class="token string">"Creating golden file for ticker: %s"</span><span class="token punctuation">,</span> ticker<span class="token punctuation">)</span>    <span class="token comment"># ... setup ...</span>    response <span class="token operator">=</span> <span class="token keyword">await</span> comprehensive_analysis<span class="token punctuation">(</span>        ticker<span class="token operator">=</span>ticker<span class="token punctuation">,</span>        include_financials_history<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>        <span class="token comment"># ... other flags ...</span>    <span class="token punctuation">)</span>    <span class="token comment"># Save the complete, ideal JSON output</span>    output_path <span class="token operator">=</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>join<span class="token punctuation">(</span>GOLDEN_DIR<span class="token punctuation">,</span> <span class="token string-interpolation"><span class="token string">f"</span><span class="token interpolation"><span class="token punctuation">&#123;</span>ticker<span class="token punctuation">&#125;</span></span><span class="token string">.json"</span></span><span class="token punctuation">)</span>    <span class="token keyword">with</span> <span class="token builtin">open</span><span class="token punctuation">(</span>output_path<span class="token punctuation">,</span> <span class="token string">"w"</span><span class="token punctuation">,</span> encoding<span class="token operator">=</span><span class="token string">"utf-8"</span><span class="token punctuation">)</span> <span class="token keyword">as</span> f<span class="token punctuation">:</span>        json<span class="token punctuation">.</span>dump<span class="token punctuation">(</span>response<span class="token punctuation">.</span>model_dump<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> f<span class="token punctuation">,</span> indent<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>    <span class="token comment"># ...</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>This script runs the full analysis pipeline and saves the structured JSON output. This becomes the “correct” answer that future runs will be compared against.</p><h3 id="Step-2-Tracking-Every-Penny-with-CostTracker-and-CostRegistry">Step 2: Tracking Every Penny with <code>CostTracker</code> and <code>CostRegistry</code></h3><p>LLM costs are measured per million tokens, which can add up deceptively fast. I needed a way to track this meticulously.</p><p>First, I created <code>CostTracker</code>, a simple class to record each API request’s model, token counts, and purpose.</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># src/utils/cost_tracker.py</span><span class="token decorator annotation punctuation">@dataclass</span><span class="token keyword">class</span> <span class="token class-name">LLMRequest</span><span class="token punctuation">:</span>    model<span class="token punctuation">:</span> <span class="token builtin">str</span>    prompt_tokens<span class="token punctuation">:</span> <span class="token builtin">int</span>    completion_tokens<span class="token punctuation">:</span> <span class="token builtin">int</span>    request_type<span class="token punctuation">:</span> <span class="token builtin">str</span><span class="token decorator annotation punctuation">@dataclass</span><span class="token keyword">class</span> <span class="token class-name">CostTracker</span><span class="token punctuation">:</span>    requests<span class="token punctuation">:</span> List<span class="token punctuation">[</span>LLMRequest<span class="token punctuation">]</span> <span class="token operator">=</span> field<span class="token punctuation">(</span>default_factory<span class="token operator">=</span><span class="token builtin">list</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">add_request</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> request<span class="token punctuation">:</span> LLMRequest<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> <span class="token boolean">None</span><span class="token punctuation">:</span>        self<span class="token punctuation">.</span>requests<span class="token punctuation">.</span>append<span class="token punctuation">(</span>request<span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">get_total_cost</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> <span class="token builtin">float</span><span class="token punctuation">:</span>        <span class="token comment"># ... calculates cost based on model pricing ...</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>But with multiple analyzers (for Business, MD&amp;A, Risks) running concurrently, passing a <code>CostTracker</code> instance around would be messy. The solution? A singleton <code>CostRegistry</code>.</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># src/utils/cost_registry.py</span><span class="token keyword">class</span> <span class="token class-name">CostRegistry</span><span class="token punctuation">:</span>    _instance <span class="token operator">=</span> <span class="token boolean">None</span>    _lock <span class="token operator">=</span> threading<span class="token punctuation">.</span>Lock<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">__new__</span><span class="token punctuation">(</span>cls<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token comment"># Singleton pattern implementation</span>        <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>    <span class="token keyword">def</span> <span class="token function">register_tracker</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> component_name<span class="token punctuation">:</span> <span class="token builtin">str</span><span class="token punctuation">,</span> tracker<span class="token punctuation">:</span> CostTracker<span class="token punctuation">)</span><span class="token punctuation">:</span>        self<span class="token punctuation">.</span>trackers<span class="token punctuation">[</span>component_name<span class="token punctuation">]</span> <span class="token operator">=</span> tracker    <span class="token keyword">def</span> <span class="token function">get_total_cost</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> <span class="token builtin">float</span><span class="token punctuation">:</span>        <span class="token comment"># ... aggregates costs from all registered trackers ...</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>Now, each <code>BaseAnalyzer</code> and <code>BaseFormatter</code> automatically registers its own <code>CostTracker</code> with the central registry upon initialization. This is clean, decoupled, and works seamlessly with concurrent execution.</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># src/utils/base_analyzer.py</span><span class="token keyword">class</span> <span class="token class-name">BaseAnalyzer</span><span class="token punctuation">(</span>Generic<span class="token punctuation">[</span>T<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        self<span class="token punctuation">.</span>cost_tracker <span class="token operator">=</span> CostTracker<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token comment"># Register this analyzer's cost tracker with the registry</span>        component_name <span class="token operator">=</span> self<span class="token punctuation">.</span>__class__<span class="token punctuation">.</span>__name__<span class="token punctuation">.</span>lower<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>replace<span class="token punctuation">(</span><span class="token string">"analyzer"</span><span class="token punctuation">,</span> <span class="token string">""</span><span class="token punctuation">)</span>        CostRegistry<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>register_tracker<span class="token punctuation">(</span>component_name<span class="token punctuation">,</span> self<span class="token punctuation">.</span>cost_tracker<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="Step-3-The-Evaluation-Harness">Step 3: The Evaluation Harness</h3><p>With the golden set and cost tracking in place, the final piece was the evaluation script, <code>model_evaluation.py</code>. This script:</p><ol><li>Takes a list of tickers to test.</li><li>Runs the full, current analysis pipeline for each one.</li><li>Loads the corresponding “golden” file.</li><li><strong>Compares the results</strong>, calculating metrics like <code>parse_success_rate</code> (did the section get extracted at all?) and <code>field_fill_rate</code> (how many optional fields were successfully populated?).</li><li><strong>Pulls cost data</strong> from the <code>CostRegistry</code>.</li><li>Saves a detailed JSON report with metrics and cost breakdowns.</li></ol><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># scripts/model_evaluation.py</span><span class="token keyword">async</span> <span class="token keyword">def</span> <span class="token function">evaluate_ticker</span><span class="token punctuation">(</span>ticker<span class="token punctuation">:</span> <span class="token builtin">str</span><span class="token punctuation">,</span> email<span class="token punctuation">:</span> <span class="token builtin">str</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token comment"># Reset the cost registry for a clean run</span>    cost_registry <span class="token operator">=</span> CostRegistry<span class="token punctuation">(</span><span class="token punctuation">)</span>    cost_registry<span class="token punctuation">.</span>reset<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token comment"># Run the analysis</span>    actual <span class="token operator">=</span> <span class="token keyword">await</span> comprehensive_analysis<span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span>    <span class="token comment"># Load the benchmark</span>    golden <span class="token operator">=</span> load_golden<span class="token punctuation">(</span>ticker<span class="token punctuation">)</span>    <span class="token comment"># Compare and calculate metrics</span>    parse_rate<span class="token punctuation">,</span> fill_rate <span class="token operator">=</span> compare_results<span class="token punctuation">(</span>golden<span class="token punctuation">,</span> actual<span class="token punctuation">.</span>model_dump<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token comment"># Get costs from the central registry</span>    total_cost <span class="token operator">=</span> cost_registry<span class="token punctuation">.</span>get_total_cost<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token keyword">return</span> <span class="token punctuation">&#123;</span>        <span class="token string">"ticker"</span><span class="token punctuation">:</span> ticker<span class="token punctuation">,</span>        <span class="token string">"parse_success_rate"</span><span class="token punctuation">:</span> parse_rate<span class="token punctuation">,</span>        <span class="token string">"field_fill_rate"</span><span class="token punctuation">:</span> fill_rate<span class="token punctuation">,</span>        <span class="token string">"cost"</span><span class="token punctuation">:</span> <span class="token punctuation">&#123;</span><span class="token string">"total_cost_usd"</span><span class="token punctuation">:</span> total_cost<span class="token punctuation">,</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">&#125;</span>    <span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="The-Result-Data-Driven-Development">The Result: Data-Driven Development</h3><p>Now, after I make a change—whether it’s tweaking a prompt, swapping a model, or refactoring code—I can run a single command:</p><p><code>python scripts/model_evaluation.py --tickers AAPL MSFT --email &quot;my@email.com&quot;</code></p><p>And get a report like this:</p><pre class="line-numbers language-json" data-language="json"><code class="language-json"><span class="token punctuation">&#123;</span>  <span class="token property">"run_time"</span><span class="token operator">:</span> <span class="token string">"2025-06-07T20:57:18.945306"</span><span class="token punctuation">,</span>  <span class="token property">"elapsed_seconds"</span><span class="token operator">:</span> <span class="token number">326.41</span><span class="token punctuation">,</span>  <span class="token property">"tickers"</span><span class="token operator">:</span> <span class="token punctuation">[</span><span class="token string">"MA"</span><span class="token punctuation">,</span> <span class="token string">"ARWR"</span><span class="token punctuation">,</span> <span class="token string">"VST"</span><span class="token punctuation">,</span> <span class="token string">"MSFT"</span><span class="token punctuation">,</span> <span class="token string">"AMZN"</span><span class="token punctuation">,</span> <span class="token string">"UBER"</span><span class="token punctuation">]</span><span class="token punctuation">,</span>  <span class="token property">"results"</span><span class="token operator">:</span> <span class="token punctuation">[</span>    <span class="token punctuation">&#123;</span>      <span class="token property">"ticker"</span><span class="token operator">:</span> <span class="token string">"MA"</span><span class="token punctuation">,</span>      <span class="token property">"parse_success_rate"</span><span class="token operator">:</span> <span class="token number">0.8333333333333334</span><span class="token punctuation">,</span>      <span class="token property">"field_fill_rate"</span><span class="token operator">:</span> <span class="token number">1.0</span><span class="token punctuation">,</span>      <span class="token property">"latency_s"</span><span class="token operator">:</span> <span class="token number">280.47</span><span class="token punctuation">,</span>      <span class="token property">"errors"</span><span class="token operator">:</span> <span class="token punctuation">&#123;</span><span class="token punctuation">&#125;</span><span class="token punctuation">,</span>      <span class="token property">"cost"</span><span class="token operator">:</span> <span class="token punctuation">&#123;</span>        <span class="token property">"total_cost_usd"</span><span class="token operator">:</span> <span class="token number">0.0813</span><span class="token punctuation">,</span>        <span class="token property">"breakdown"</span><span class="token operator">:</span> <span class="token punctuation">&#123;</span>          <span class="token property">"base"</span><span class="token operator">:</span> <span class="token punctuation">&#123;</span>            <span class="token property">"total_cost"</span><span class="token operator">:</span> <span class="token number">0.0258</span><span class="token punctuation">,</span>            <span class="token property">"total_tokens"</span><span class="token operator">:</span> <span class="token number">41638</span><span class="token punctuation">,</span>            <span class="token property">"cost_by_type"</span><span class="token operator">:</span> <span class="token punctuation">&#123;</span>              <span class="token property">"BaseAnalyzer"</span><span class="token operator">:</span> <span class="token number">0.0258</span>            <span class="token punctuation">&#125;</span><span class="token punctuation">,</span>            <span class="token property">"tokens_by_type"</span><span class="token operator">:</span> <span class="token punctuation">&#123;</span>              <span class="token property">"BaseAnalyzer"</span><span class="token operator">:</span> <span class="token number">41638</span>            <span class="token punctuation">&#125;</span><span class="token punctuation">,</span>            <span class="token property">"request_count"</span><span class="token operator">:</span> <span class="token number">1</span>          <span class="token punctuation">&#125;</span><span class="token punctuation">,</span>          <span class="token property">"incomestatement"</span><span class="token operator">:</span> <span class="token punctuation">&#123;</span>            <span class="token property">"total_cost"</span><span class="token operator">:</span> <span class="token number">0.0135</span><span class="token punctuation">,</span>            <span class="token property">"total_tokens"</span><span class="token operator">:</span> <span class="token number">20608</span><span class="token punctuation">,</span>            <span class="token property">"cost_by_type"</span><span class="token operator">:</span> <span class="token punctuation">&#123;</span>              <span class="token property">"IncomeStatementFormatter"</span><span class="token operator">:</span> <span class="token number">0.0135</span>            <span class="token punctuation">&#125;</span><span class="token punctuation">,</span>            <span class="token property">"tokens_by_type"</span><span class="token operator">:</span> <span class="token punctuation">&#123;</span>              <span class="token property">"IncomeStatementFormatter"</span><span class="token operator">:</span> <span class="token number">20608</span>            <span class="token punctuation">&#125;</span><span class="token punctuation">,</span>            <span class="token property">"request_count"</span><span class="token operator">:</span> <span class="token number">13</span>          <span class="token punctuation">&#125;</span><span class="token punctuation">,</span>          <span class="token property">"balancesheet"</span><span class="token operator">:</span> <span class="token punctuation">&#123;</span>            <span class="token property">"total_cost"</span><span class="token operator">:</span> <span class="token number">0.0196</span><span class="token punctuation">,</span>            <span class="token property">"total_tokens"</span><span class="token operator">:</span> <span class="token number">30754</span><span class="token punctuation">,</span>            <span class="token property">"cost_by_type"</span><span class="token operator">:</span> <span class="token punctuation">&#123;</span>              <span class="token property">"BalanceSheetFormatter"</span><span class="token operator">:</span> <span class="token number">0.0196</span>            <span class="token punctuation">&#125;</span><span class="token punctuation">,</span>            <span class="token property">"tokens_by_type"</span><span class="token operator">:</span> <span class="token punctuation">&#123;</span>              <span class="token property">"BalanceSheetFormatter"</span><span class="token operator">:</span> <span class="token number">30754</span>            <span class="token punctuation">&#125;</span><span class="token punctuation">,</span>            <span class="token property">"request_count"</span><span class="token operator">:</span> <span class="token number">13</span>          <span class="token punctuation">&#125;</span><span class="token punctuation">,</span>          <span class="token property">"cashflowstatement"</span><span class="token operator">:</span> <span class="token punctuation">&#123;</span>            <span class="token property">"total_cost"</span><span class="token operator">:</span> <span class="token number">0.0198</span><span class="token punctuation">,</span>            <span class="token property">"total_tokens"</span><span class="token operator">:</span> <span class="token number">30983</span><span class="token punctuation">,</span>            <span class="token property">"cost_by_type"</span><span class="token operator">:</span> <span class="token punctuation">&#123;</span>              <span class="token property">"CashFlowStatementFormatter"</span><span class="token operator">:</span> <span class="token number">0.0198</span>            <span class="token punctuation">&#125;</span><span class="token punctuation">,</span>            <span class="token property">"tokens_by_type"</span><span class="token operator">:</span> <span class="token punctuation">&#123;</span>              <span class="token property">"CashFlowStatementFormatter"</span><span class="token operator">:</span> <span class="token number">30983</span>            <span class="token punctuation">&#125;</span><span class="token punctuation">,</span>            <span class="token property">"request_count"</span><span class="token operator">:</span> <span class="token number">13</span>          <span class="token punctuation">&#125;</span><span class="token punctuation">,</span>          <span class="token property">"equitystatement"</span><span class="token operator">:</span> <span class="token punctuation">&#123;</span>            <span class="token property">"total_cost"</span><span class="token operator">:</span> <span class="token number">0.0026</span><span class="token punctuation">,</span>            <span class="token property">"total_tokens"</span><span class="token operator">:</span> <span class="token number">3910</span><span class="token punctuation">,</span>            <span class="token property">"cost_by_type"</span><span class="token operator">:</span> <span class="token punctuation">&#123;</span>              <span class="token property">"EquityStatementFormatter"</span><span class="token operator">:</span> <span class="token number">0.0026</span>            <span class="token punctuation">&#125;</span><span class="token punctuation">,</span>            <span class="token property">"tokens_by_type"</span><span class="token operator">:</span> <span class="token punctuation">&#123;</span>              <span class="token property">"EquityStatementFormatter"</span><span class="token operator">:</span> <span class="token number">3910</span>            <span class="token punctuation">&#125;</span><span class="token punctuation">,</span>            <span class="token property">"request_count"</span><span class="token operator">:</span> <span class="token number">7</span>          <span class="token punctuation">&#125;</span>        <span class="token punctuation">&#125;</span><span class="token punctuation">,</span>        <span class="token property">"metrics"</span><span class="token operator">:</span> <span class="token punctuation">&#123;</span>          <span class="token property">"parse_success_per_dollar"</span><span class="token operator">:</span> <span class="token number">10.2501</span><span class="token punctuation">,</span>          <span class="token property">"field_fill_per_dollar"</span><span class="token operator">:</span> <span class="token number">12.3001</span>        <span class="token punctuation">&#125;</span>      <span class="token punctuation">&#125;</span>    <span class="token punctuation">&#125;</span>  <span class="token punctuation">]</span><span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>This framework gives a better view of the project’s performance and cost dynamics. I can see immediately if a change breaks something or has a significant impact on cost. It’s been a good learning experience building this LLM-based application, and I’m excited to see how it helps guide the project forward.</p>]]></content>
      
      
      <categories>
          
          <category> SEC Analysis </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
            <tag> LLM </tag>
            
            <tag> AI </tag>
            
            <tag> Evaluation </tag>
            
            <tag> Cost Management </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Anatomy of an AI Financial Analyst: From Raw Filing to Interactive Dashboard</title>
      <link href="/2025/06/07/AI%20LLM/anatomy-of-an-ai-financial-analyst.html"/>
      <url>/2025/06/07/AI%20LLM/anatomy-of-an-ai-financial-analyst.html</url>
      
        <content type="html"><![CDATA[<p>In the last post, I briefly outlined the project for an AI-powered tool to analyze SEC filings. In this blog, I’m diving deep into the architecture and core components that form the engine of this financial analyst.</p><p>We’ll dissect the journey of a single user request, from a ticker symbol entered into a form all the way to a rich, interactive dashboard filled with AI-generated insights.</p><span id="more"></span><h3 id="The-Architectural-Blueprint">The Architectural Blueprint</h3><p>A robust application needs a clean separation of concerns. I’ve structured the project with AI assistants into three main layers, each with a distinct responsibility:</p><ol><li><strong>The Data Layer</strong>: Responsible for fetching, caching, and serving the raw financial documents.</li><li><strong>The Analysis (AI) Layer</strong>: The “brain” of the operation, where LLMs interpret the raw text and extract structured information.</li><li><strong>The Presentation Layer</strong>: A <code>FastAPI</code> web server that orchestrates the process and displays the results in a user-friendly UI.</li></ol><p>Let’s break down each one.</p><h3 id="1-The-Foundation-The-Data-Layer">1. The Foundation: The Data Layer</h3><p>Before any analysis can happen, we need data. The U.S. Securities and Exchange Commission (SEC) provides this through its EDGAR database.</p><p>The <code>CompanyDataManager</code> class is the gateway to this data. It uses the <code>edgartools</code> library to:</p><ul><li>Identify a company by its ticker symbol (e.g., <code>TSLA</code>).</li><li>Fetch a list of its recent filings (10-Ks for annual reports, 10-Qs for quarterly).</li><li>Extract the raw text content of specific sections, like “Business” or “Risk Factors.”</li></ul><p>A crucial consideration here is performance and cost. Hitting the SEC’s servers for the same large document repeatedly is inefficient. To solve this, I’ve implemented a caching layer using <code>hishel</code>, which automatically saves downloaded filings to disk.</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># src/data/company_data.py</span><span class="token keyword">class</span> <span class="token class-name">CompanyDataManager</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> email<span class="token punctuation">:</span> <span class="token builtin">str</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token comment"># Configure caching for all EDGAR requests</span>        httpx<span class="token punctuation">.</span>Client <span class="token operator">=</span> EdgarCache<span class="token punctuation">.</span>CachedClient        set_identity<span class="token punctuation">(</span>email<span class="token punctuation">)</span>        <span class="token comment"># ...</span>    <span class="token keyword">def</span> <span class="token function">load_company</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> ticker<span class="token punctuation">:</span> <span class="token builtin">str</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        self<span class="token punctuation">.</span>company <span class="token operator">=</span> Company<span class="token punctuation">(</span>ticker<span class="token punctuation">)</span>        <span class="token comment"># Fetches a list of 10-K and 10-Q filings</span>        self<span class="token punctuation">.</span>load_multiple_filings<span class="token punctuation">(</span><span class="token punctuation">[</span>FilingType<span class="token punctuation">.</span>FORM_10K<span class="token punctuation">,</span> FilingType<span class="token punctuation">.</span>FORM_10Q<span class="token punctuation">]</span><span class="token punctuation">)</span>        <span class="token comment"># ...</span>    <span class="token keyword">def</span> <span class="token function">get_item_text</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> item_name<span class="token punctuation">:</span> <span class="token builtin">str</span><span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> Optional<span class="token punctuation">[</span><span class="token builtin">str</span><span class="token punctuation">]</span><span class="token punctuation">:</span>        <span class="token comment"># ... returns the raw text of a filing section ...</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="2-The-Brain-The-AI-Analysis-Core">2. The Brain: The AI Analysis Core</h3><p>This is where the LLM magic happens. I’ve designed a pattern using a <code>BaseAnalyzer</code> class that can be extended for different types of analysis.</p><p>The key to getting reliable, structured data from an LLM is a combination of two things: a <strong>clear prompt</strong> and a <strong>strict output schema</strong>.</p><ol><li><strong>The Prompts</strong>: Each analyzer (<code>BusinessAnalyzer</code>, <code>RiskFactorAnalyzer</code>, etc.) has a highly specific system prompt that tells the LLM its role and what to look for.</li><li><strong>The Schema with Pydantic</strong>: I define the exact JSON structure I want using Pydantic models. This schema is then injected directly into the system prompt. This tells the LLM <em>exactly</em> how to format its response.</li></ol><p>For example, to analyze risk factors, I have a <code>RiskFactor</code> Pydantic model:</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># src/models/risk_factor_model.py</span><span class="token keyword">class</span> <span class="token class-name">RiskSeverity</span><span class="token punctuation">(</span><span class="token builtin">str</span><span class="token punctuation">,</span> Enum<span class="token punctuation">)</span><span class="token punctuation">:</span>    LOW <span class="token operator">=</span> <span class="token string">"Low"</span>    MODERATE <span class="token operator">=</span> <span class="token string">"Moderate"</span>    HIGH <span class="token operator">=</span> <span class="token string">"High"</span>    CRITICAL <span class="token operator">=</span> <span class="token string">"Critical"</span><span class="token keyword">class</span> <span class="token class-name">RiskFactor</span><span class="token punctuation">(</span>BaseModel<span class="token punctuation">)</span><span class="token punctuation">:</span>    category<span class="token punctuation">:</span> RiskCategory    title<span class="token punctuation">:</span> <span class="token builtin">str</span>    description<span class="token punctuation">:</span> <span class="token builtin">str</span>    severity<span class="token punctuation">:</span> RiskSeverity    likelihood<span class="token punctuation">:</span> RiskLikelihood    <span class="token comment"># ... and other fields</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>The <code>RiskFactorAnalyzer</code> uses this model to generate its prompt and validate the LLM’s output. If the response doesn’t match the schema, Pydantic raises an error, ensuring data integrity. This makes the system far more robust than just parsing free-form text.</p><h3 id="3-The-Face-The-API-and-Frontend">3. The Face: The API and Frontend</h3><p>With structured data in hand, the final step is to present it to the user. I chose <code>FastAPI</code> for its speed and simplicity.</p><p>A single, powerful endpoint <code>/analyze/&#123;ticker&#125;</code> orchestrates the entire process:</p><ul><li>It initializes the <code>CompanyDataManager</code>.</li><li>It runs the requested analyzers (<code>Business</code>, <code>MD&amp;A</code>, <code>Risks</code>) in parallel using <code>asyncio.gather()</code> for maximum speed.</li><li>It aggregates the results.</li><li>It passes the final, structured templates for rendering.</li></ul><p>The frontend really showed the power of <strong>Claude Sonnet 3.7</strong>. The design is intentionally simple: server-side rendered HTML with Bootstrap and a touch of JavaScript for interactivity (like the financial charts). This keeps the focus on the data.</p><p>React is in consideration in the future learning work but for now the frontend only serves the demo purpose.</p><h3 id="The-Scaffolding-Guiding-the-AI">The Scaffolding: Guiding the AI</h3><p>To make this partnership even more effective, I established a set of ground rules. One of the first files I created was <code>.github/copilot-instructions.md</code>. This file tells my AI assistant about the project’s standards, preferred libraries, and architectural patterns.</p><p>For example, I specified:</p><ul><li><strong>Adhere to Project Standards</strong>: Follow the coding style and patterns already in the project.</li><li><strong>Use Existing Code</strong>: Leverage existing functions and classes where appropriate.</li><li><strong>Error Handling</strong>: Include basic, sensible error handling.</li><li><strong>Testing</strong>: Generate tests using <code>pytest</code>.</li></ul><p>This context file acts like a “project constitution” for my AI partner, ensuring its suggestions are consistent and high-quality.</p><h3 id="What’s-Next">What’s Next?</h3><p>We’ve gone from concept to a working end-to-end application. The human-AI collaboration has proven to be a massive productivity boost. But is the <em>output</em> of our application any good? And what’s the damage to my OpenAI bill?</p><p>In the next post, we’ll answer those questions by building a robust evaluation and cost-tracking framework.</p>]]></content>
      
      
      <categories>
          
          <category> SEC Analysis </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
            <tag> FastAPI </tag>
            
            <tag> LLM </tag>
            
            <tag> Pydantic </tag>
            
            <tag> Architecture </tag>
            
            <tag> AI </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>LLM Learning Journey: Building an LLM-powered Financial Analyst with an AI Coding Partner</title>
      <link href="/2025/06/07/AI%20LLM/project-inception-ai-financial-analyst.html"/>
      <url>/2025/06/07/AI%20LLM/project-inception-ai-financial-analyst.html</url>
      
        <content type="html"><![CDATA[<p>It’s been a while, but I’m happy to be back to blogging (helped by my AI assistants) with a new project and a new series! This is a dual experiment:</p><ol><li>Can we build an AI-powered tool to automate the analysis of complex financial documents for the purpose of learning the latest AI LLM techniques?</li><li>How far can a developer push a project by truly <em>partnering</em> with AI coding assistants?</li></ol><p>I’ll be sharing the journey right here.</p><h3 id="The-Problem-Information-Overload-in-Finance">The Problem: Information Overload in Finance</h3><p>Before the project I had not much experience with financial documents especially company filings, only a basic understanding of their importance and relevance. After spending some time digging into them, I realized the challenge: You’re faced with mountains of dense, jargon-filled documents like 10-Ks and 10-Qs. Buried within these SEC filings is a goldmine of information about a company’s performance, strategy, and risks. But extracting and understanding it is a tedious, manual process and never even mention that not all SEC filings themselves are not standardized.</p><p>There are many websites have information about companies like Yahoo Finance, Google Finance, and others. But to build a platform with LLMs that can truly understand and analyze these documents, it’s essential to work directly with the raw data. This is where the SEC’s EDGAR database comes in, providing access to all public filings.</p><span id="more"></span><h3 id="The-Vision-An-AI-Analyst-Built-by-a-Human-AI-Team">The Vision: An AI Analyst, Built by a Human-AI Team</h3><p>The goal is to create a web application where a user can input a stock ticker and receive a comprehensive, AI-driven analysis. This isn’t just about pulling numbers; it’s about understanding the narrative and context behind them.</p><p>The application will break down the analysis into four key areas:</p><ol><li><strong>Financial History</strong>: Interactive charts and trends from historical financial statements.</li><li><strong>Business Analysis</strong>: A deep dive into the company’s operations, products, competitive advantages, and strategic initiatives.</li><li><strong>MD&amp;A (Management’s Discussion &amp; Analysis)</strong>: Key takeaways and performance indicators directly from management’s perspective.</li><li><strong>Risk Factors</strong>: A prioritized overview of the risks the company faces, complete with a severity and likelihood matrix.</li></ol><h3 id="The-Tech-Stack">The Tech Stack</h3><p>I’ve chosen a Python stack to bring this to life:</p><ul><li><strong>AI Coding Assistants</strong>: My partners in crime. I’m using <strong>GitHub Copilot</strong> integrated with various powerful models like <strong>Google’s Gemini 2.5 Pro</strong> and <strong>Anthropic’s Claude 3.7 Sonnet</strong> to accelerate <strong>every</strong> part of the development process.</li><li><strong>Backend</strong>: <code>FastAPI</code> for a high-performance, asynchronous API.</li><li><strong>Data Sourcing</strong>: The excellent <code>edgartools</code> library to fetch filings directly from the SEC EDGAR database.</li><li><strong>The “AI Brain”</strong>: <code>OpenAI</code>’s API (specifically models like GPT-4o-mini) to read and interpret the raw text from the filings.</li><li><strong>Data Modeling</strong>: <code>Pydantic</code> to enforce strict, structured JSON outputs from the LLM, ensuring data quality and reliability.</li><li><strong>Frontend</strong>: Simple, server-side rendered HTML with Bootstrap so far to create a clean, functional dashboard. This will be updated ideally with React as currently UI is for the purpose of demo.</li></ul><h3 id="How-It-Works-A-High-Level-Flow">How It Works: A High-Level Flow</h3><ol><li><strong>User Input</strong>: The user provides a stock ticker (e.g., <code>AAPL</code>) and an email (required for SEC API access) on the web interface.</li><li><strong>Data Fetching</strong>: The <code>CompanyDataManager</code> class uses <code>edgartools</code> to find the company and download its latest 10-K and 10-Q filings.</li><li><strong>Section Extraction</strong>: The application identifies and extracts the raw text from key sections like “Business,” “Risk Factors,” and “MD&amp;A”.</li><li><strong>AI Analysis</strong>: Each section’s text is passed to a specialized “Analyzer” class (e.g., <code>BusinessAnalyzer</code>, <code>RiskFactorAnalyzer</code>). These analyzers use tailored prompts to instruct the LLM to extract specific information and structure it according to predefined Pydantic models. This is where the magic happens!</li><li><strong>Data Structuring</strong>: The LLM’s JSON output is validated by the Pydantic models, ensuring the data is clean and consistent.</li><li><strong>Display</strong>: The structured data is then passed to the Jinja2 templates, which render the final interactive dashboard for the user.</li></ol><p>Here’s a look at the project’s core structure:</p><pre class="line-numbers language-none"><code class="language-none">src&#x2F;├── api&#x2F;          # FastAPI, routers, and templates├── analysis&#x2F;     # The LLM-powered analyzer classes├── data&#x2F;         # EDGAR data fetching and management├── financial&#x2F;    # Financial statement extraction and formatting├── models&#x2F;       # Pydantic models for structured data└── utils&#x2F;        # Common utilities, including the base analyzer<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="What’s-Next">What’s Next?</h3><p>This is just the beginning. The foundation is laid, but building a reliable AI system requires more than just calling an API. Next post will dive a bit more into the architecture and the core components of the application, including how the AI assistants are integrated into the development process.</p>]]></content>
      
      
      <categories>
          
          <category> SEC Analysis </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
            <tag> FastAPI </tag>
            
            <tag> LLM </tag>
            
            <tag> AI </tag>
            
            <tag> OpenAI </tag>
            
            <tag> FinTech </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Inomad Diary-04-Infrastructure-02-Implementations-03-Create Resources</title>
      <link href="/2024/07/23/Inomad%20Dairy/04-Infrastructure/02-Implementation/Create%20Resources.html"/>
      <url>/2024/07/23/Inomad%20Dairy/04-Infrastructure/02-Implementation/Create%20Resources.html</url>
      
        <content type="html"><![CDATA[<h3 id="🔎-Intro"><strong>🔎 Intro</strong></h3><p>In this post, I will start creating resources in Azure using Pulumi. The full process is quite complex, so I will try to break it down into smaller parts and raise the key points here during my implementation.</p><span id="more"></span><h3 id="📁-Resource-Group"><strong>📁 Resource Group</strong></h3><p>The resources in Azure are organized into resource groups. A resource group is a logical container for resources deployed on Azure. You can deploy, update, and delete multiple resources in a resource group together. More about <a href="https://docs.microsoft.com/en-us/azure/azure-resource-manager/management/manage-resource-groups-portal">Azure Resource Group</a>.</p><p>Probably more to be added while it’s going but at this moment the key resources I am using are:</p><ul><li>Container App</li><li>Container Registry</li><li>SQL Server</li><li>SQL Database</li><li>Key Vault</li><li>Storage Account</li><li>Blob Container</li><li>DevOps</li><li>Network</li><li>Insights</li></ul><p>I’ve organized my resources into three resource groups:</p><h3 id="1-web-app-rg">1. <code>web_app_rg</code>:</h3><ul><li>Container App</li><li>Container Registry</li><li>SQL Server</li><li>SQL Database</li><li>Key Vault</li><li>Storage Account</li><li>Blob Container</li></ul><p><strong>The pros of organizing resources into a resource group are:</strong></p><ul><li>This group has all application-realted resources, making it easier to manage and monitor the web application.</li><li>It’s easier to mange updates, scaling, and eployments since all app components are in one place.</li><li>Simplifies tracking and optimizing costs specifically associated with the web application itself.</li><li>It grants the easier managing permissions for developers and operators who need access only to the application resources.</li></ul><h3 id="2-devops-rg">2. <code>devops_rg</code>:</h3><ul><li>DevOps</li></ul><p><strong>The pros of organizing resources into a resource group are:</strong></p><ul><li>It keeps DevOps resources reparate from the actual application resources, reducing the risk of accidental interference.</li><li>It enhanced security by isolating DevOps tools and credentials, ensuring taht only DevOps access.</li><li>It simplifies scaling DevOps tools independently of the application resources.</li></ul><h3 id="3-network-and-monitoring-rg">3. <code>network_and_monitoring_rg</code>:</h3><ul><li>Network</li><li>Insights</li></ul><p><strong>The pros of organizing resources into a resource group are:</strong></p><ul><li>Network resources like VNETS, subnets, and security groups are in one place, making it easier to manage network configurations and security.</li><li>It centralized location for all monitoring tools, which simplifies the process of setting up and managing alerts, dashboards, and logs.</li><li>It enhanced security byisolating network and monitoring resources, ensuring network configurations are not tampered with unintentially.</li></ul><p>Organizing resources into resource groups is a good practice for managing and monitoring resources in Azure. It depends on the complexity of the application and the team’s requirements bae on the resources’ nature and purpose. Resources in different resource groups can communicate with each other, which makes it possible if you need more resource groups in the future.</p><h3 id="📦-Container-Registry"><strong>📦 Container Registry</strong></h3><p>Azure Container Registry is a managed, private Docker registry service based on the open-source Docker Registry 2.0. It allows you to store and manage container images for all types of container deployments. More about <a href="https://docs.microsoft.com/en-us/azure/container-registry/">Azure Container Registry</a>.</p><p>The Pulumi code to create a container registry in Azure can be find <a href="https://www.pulumi.com/registry/packages/azure-native/api-docs/containerregistry/registry/">here</a></p><h3 id="Things-to-consider-when-creating-a-container-registry">Things to consider when creating a container registry:</h3><p><strong>1. Assign role-based access control (RBAC):</strong> RBAC roles to the registry to control access to the registry like <code>AcrPull</code> and <code>AcrPush</code>.</p><p>⚠️ Note here, when assigning the roles using <code>pulumi_azure_native.authorization.RoleAssignment</code> to groups or uses in Pulumi Azure, there is one thing quite confusing:<br><code>roleDefinitionId</code>: This is the Identifier of the role definition to assign to the principal, which uses the ID of the roles in Azure as part of it.<br>example of the Identifier: <code>&quot;/subscriptions/$&#123;subscriptionId&#125;/providers/Microsoft.Authorization/roleDefinitions/00000000-0000-0000-0000-000000000000&quot;</code> and <code>providers/Microsoft.Authorization/roleDefinitions/00000000-0000-0000-0000-000000000000&quot;</code>is the ID of the role in Azure you are going to assign.<br>Currently in Pulumi latest azure-native module, it does not support retrieving the role ID from the role name, so you need to define a function using Azure CLI and use it in the Pulumi code. For example:</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">get_role_id_by_name</span><span class="token punctuation">(</span>role_name<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">try</span><span class="token punctuation">:</span>        result <span class="token operator">=</span> subprocess<span class="token punctuation">.</span>run<span class="token punctuation">(</span>            <span class="token punctuation">[</span>                <span class="token string">"az"</span><span class="token punctuation">,</span>                <span class="token string">"role definition"</span><span class="token punctuation">,</span>                <span class="token string">"list"</span><span class="token punctuation">,</span>                <span class="token string">"--query"</span><span class="token punctuation">,</span>                <span class="token string-interpolation"><span class="token string">f"[?roleName=='</span><span class="token interpolation"><span class="token punctuation">&#123;</span>role_name<span class="token punctuation">&#125;</span></span><span class="token string">'].id"</span></span><span class="token punctuation">,</span>                <span class="token string">"--output"</span><span class="token punctuation">,</span>                <span class="token string">"json"</span><span class="token punctuation">,</span>            <span class="token punctuation">]</span><span class="token punctuation">,</span>            capture_output<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>            text<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>            check<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>        <span class="token punctuation">)</span>        roles <span class="token operator">=</span> json<span class="token punctuation">.</span>loads<span class="token punctuation">(</span>result<span class="token punctuation">.</span>stdout<span class="token punctuation">)</span>        <span class="token keyword">if</span> roles<span class="token punctuation">:</span>            <span class="token keyword">return</span> roles<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>        <span class="token keyword">return</span> <span class="token boolean">None</span>    <span class="token keyword">except</span> subprocess<span class="token punctuation">.</span>CalledProcessError <span class="token keyword">as</span> e<span class="token punctuation">:</span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"Error running az command: </span><span class="token interpolation"><span class="token punctuation">&#123;</span>e<span class="token punctuation">&#125;</span></span><span class="token string">"</span></span><span class="token punctuation">)</span>        <span class="token keyword">return</span> <span class="token boolean">None</span>    <span class="token keyword">except</span> json<span class="token punctuation">.</span>JSONDecodeError <span class="token keyword">as</span> e<span class="token punctuation">:</span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"Error decoding JSON: </span><span class="token interpolation"><span class="token punctuation">&#123;</span>e<span class="token punctuation">&#125;</span></span><span class="token string">"</span></span><span class="token punctuation">)</span>        <span class="token keyword">return</span> <span class="token boolean">None</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li><strong>2. SKU:</strong> Choose the SKU based on the requirements of the application. The SKU determines the capabilities and features of the registry. Here it has <code>Basic</code> and <code>Standard</code> SKUs.</li><li><strong>3. Registry Name:</strong> The name of the registry must be unique within Azure. It must be between 5 and 50 characters long alphanumeric.<br>(⚠️ Note here, since the registry name must be unique within Azure, so it’s a good idea to include a unique name like your organization name or project name in it. (e.g., <code>&lt;your-project-name&gt;containerregistry</code>). This also applies to other resources in Azure. It’s also a good practice if you can add corresponding environment names to the resources to make it easier to identify which resources belong to which environment.)</li></ul><h3 id="🖥️-SQL-Server-and-Database"><strong>🖥️ SQL Server and Database</strong></h3><p>Azure SQL Database is a fully managed relational database service that provides a broad variety of features to enable you to scale, secure, and monitor your database. More about <a href="https://docs.microsoft.com/en-us/azure/azure-sql/database/">Azure SQL Database</a>.</p><p>The Pulumi code to create a SQL Server and SQL Database in Azure can be find <a href="https://www.pulumi.com/registry/packages/azure-native/api-docs/sql/">here</a>.</p><h3 id="Things-to-consider-when-creating-a-SQL-Server-and-SQL-Database">Things to consider when creating a SQL Server and SQL Database:</h3><ul><li><strong>1. KeyVault Creation:</strong> Create a <code>KeyVault</code> and assign the Server credentials to the KeyVault to secure them. Of course you need to assign the proper RBAC roles to the KeyVault to allow the SQL Server to access the credentials.</li></ul><p>⚠️ Note here, pulumi_azure_native does not suppport retireving the actual key value from the KeyVault, so you need to use Azure CLI to retrieve the secret value from the KeyVault and use it in the Pulumi code. For example:</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">get_kv_secret</span><span class="token punctuation">(</span>vault_name<span class="token punctuation">,</span> secret_name<span class="token punctuation">,</span> access_token<span class="token punctuation">,</span> secret_version<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    url <span class="token operator">=</span> <span class="token string-interpolation"><span class="token string">f"https://</span><span class="token interpolation"><span class="token punctuation">&#123;</span>vault_name<span class="token punctuation">&#125;</span></span><span class="token string">.vault.azure.net/secrets/</span><span class="token interpolation"><span class="token punctuation">&#123;</span>secret_name<span class="token punctuation">&#125;</span></span><span class="token string">"</span></span>    <span class="token keyword">if</span> secret_version<span class="token punctuation">:</span>        url <span class="token operator">+=</span> <span class="token string-interpolation"><span class="token string">f"/</span><span class="token interpolation"><span class="token punctuation">&#123;</span>secret_version<span class="token punctuation">&#125;</span></span><span class="token string">"</span></span>    url <span class="token operator">+=</span> <span class="token string">"?api-version=7.0"</span>    headers <span class="token operator">=</span> <span class="token punctuation">&#123;</span>        <span class="token string">"Authorization"</span><span class="token punctuation">:</span> <span class="token string-interpolation"><span class="token string">f"Bearer </span><span class="token interpolation"><span class="token punctuation">&#123;</span>access_token<span class="token punctuation">&#125;</span></span><span class="token string">"</span></span><span class="token punctuation">,</span>    <span class="token punctuation">&#125;</span>    response <span class="token operator">=</span> requests<span class="token punctuation">.</span>get<span class="token punctuation">(</span>url<span class="token punctuation">,</span> headers<span class="token operator">=</span>headers<span class="token punctuation">,</span> timeout<span class="token operator">=</span><span class="token number">30</span><span class="token punctuation">)</span>    <span class="token keyword">if</span> response<span class="token punctuation">.</span>status_code <span class="token operator">!=</span> <span class="token number">200</span><span class="token punctuation">:</span>        <span class="token keyword">raise</span> requests<span class="token punctuation">.</span>exceptions<span class="token punctuation">.</span>HTTPError<span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"Failed to get secret: </span><span class="token interpolation"><span class="token punctuation">&#123;</span>response<span class="token punctuation">.</span>text<span class="token punctuation">&#125;</span></span><span class="token string">"</span></span><span class="token punctuation">)</span>    secret <span class="token operator">=</span> response<span class="token punctuation">.</span>json<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span>json<span class="token punctuation">.</span>dumps<span class="token punctuation">(</span>secret<span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token keyword">return</span> secret<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="🫙-Storage-Account-and-Blob-Container"><strong>🫙 Storage Account and Blob Container</strong></h3><p>This is the straghtforward part of the implementation. The Pulumi code to create a Storage Account and Blob Container in Azure can be find <a href="https://www.pulumi.com/registry/packages/azure-native/api-docs/storage/">here</a>.</p><h3 id="🚚-Content-Delivery-Network-CDN"><strong>🚚 Content Delivery Network (CDN)</strong></h3><p>Azure Content Delivery Network (CDN) is a global content delivery network solution for delivering high-bandwidth content. <a href="https://www.pulumi.com/registry/packages/azure-native/api-docs/cdn/">Pulumi Docs</a>.</p><h3 id="Things-to-consider-when-creating-a-Container-App">Things to consider when creating a Container App:</h3><p><strong>1. CDN Profile:</strong> The CDN profile is a container for the CDN endpoints. It defines the pricing tier and the CDN provider. The profile should be global.</p><p><strong>2. CDN Endpoint:</strong> The CDN endpoint is the actual CDN service that delivers content to end-users. It is associated with a CDN profile and a custom domain name.</p><ul><li><code>origin</code>: The origin of the CDN endpoint. This is the source of the content that the CDN delivers to end-users. The origin can be a storage account, a web app, or any other public endpoint.<br>(⚠️ Note, in <a href="https://learn.microsoft.com/en-us/cli/azure/cdn/endpoint?view=azure-cli-latest#az-cdn-endpoint-create">Micsoft doc</a>, when you create an endpoint, the <code>origin</code> means the <code>custom domain</code> which in Pulumi, is <code>origin_host_header</code>.)</li><li><code>origin_group</code>: The origin group is a collection of origins that the CDN endpoint uses to deliver content. The CDN endpoint can use multiple origins to deliver content to end-users. The origin group can be used to define failover and load balancing rules for the CDN endpoint.</li></ul><h3 id="🛢️-ContainerApp"><strong>🛢️ ContainerApp</strong></h3><p>Azure Container Apps is a fully managed serverless container service that enables you to deploy and run containerized applications without having to manage the underlying infrastructure. More about <a href="https://learn.microsoft.com/en-us/azure/templates/microsoft.app/containerapps?pivots=deployment-language-bicep#container">Azure Container Apps</a>.</p><p>The Pulumi code to create a Container App in Azure can be find <a href="https://www.pulumi.com/registry/packages/azure-native/api-docs/app/containerapp/">here</a>.</p><h3 id="Things-to-consider-when-creating-a-Container-App-2">Things to consider when creating a Container App:</h3><p><strong>1. Docker Image:</strong> First, you need to create a image source from the container registry you created earlier and here you also need to install a new module <code>pulumi_docker</code>. Example can be found here in <a href="https://www.pulumi.com/docs/reference/pkg/docker/image/">Pulumi Docs</a>.</p><p>⚠️ Note here, in <code>RegistryArgs</code> you need to fill in the registry credentials to access the container registry. If you’ve created your registry without having credentials, you can use <code>az acr login</code> with your Microsoft Entra ID which you should have assigned acr permissions to or a Service Principle to access the registry.</p><p><strong>2. Container App:</strong> Defining the container app requires more configuration. Below is the example of the configuration from the <a href="https://www.pulumi.com/registry/packages/azure-native/api-docs/app/containerapp/">Pulumi Docs</a>:</p><p>⚠️ Concepts I picked up here:</p><ul><li><code>Microsoft.web</code> and <code>Microsoft.app</code> are the namespaces for the different resources, you can find the full list <a href="https://learn.microsoft.com/en-us/azure/azure-resource-manager/management/azure-services-resource-providers">here</a>. Another confusing part is, you can still find <code>Microsoft.web.ContainerApp</code> resource but it is an old version of this resource and it’s better to use the new version <code>Microsoft.app.ContainerApp</code> as it is stated <a href="https://learn.microsoft.com/en-us/azure/templates/microsoft.web/containerapps?pivots=deployment-language-bicep">here</a> in Microsoft Docs.</li><li><code>dapr</code>: Distributed Application Runtime (Dapr) is a portable, event-driven runtime that makes it easy for developers to build resilient, microservice applications.</li><li><code>ingress</code>: Ingress is the process of routing external HTTP(S) traffic to the correct service in your cluster. In ingress there are multiple concepts need attention.</li><li><code>client_cerfiticate_mode</code> is the mode of client certificate.<br><code>IGNORE</code>: means the client certificate is ignored. This is the default mode where only server-side SSL/TLS is enforced, ensuring that the server is authenticated but not the client. <code>ACCEPT</code>: The application accepts client certificates if presented, but does not require them. If a client certificate is provided, the server will validate it. This mode is useful for applications that can optionally support mTLS for clients that have the necessary certificates but do not require it for all clients. <code>REQUIRE</code>: The application requires all clients to present a valid client certificate to establish a connection. This mode enforces strict mutual authentication, ensuring that only clients with valid certificates can access the application. It is suitable for highly secure environments where all clients are expected to be authenticated.</li><li><code>cors_policy</code>: This defines Cross-Origin Resource Sharing policy for Azure Container App. CORS is a security feature that allows or restricts web applications running at one origin (domain) from interacting with resources from different origins.</li><li><code>custom_domains</code>: This is the domain that you want to use to access the Azure Container App. <code>binding_type</code>: The type of binding for the custom domain. <code>certificate</code>: The certificate that you want to use for the custom domain gained from associated Container App envrionment. <code>host_name</code>: The host name that you want to use for the custom domain.</li><li><code>probes</code> property of <code>containers</code> in `templates: Probes are mechanisms used to check the halth and status of a container. There are three types of probes in Kubernetes and other container orchestrators: liveness probes, readiness probes, and startup probes. Liveness probes are used to determine if a container is running correctly. Readiness probes determine if a container is ready to accept traffic. Startup probes are used to check if an application within a container has started.</li></ul><p><strong>3. Managed Envrionment:</strong> This is the environment where the container app is going to run. The pulumi code to create a managed environment can be found <a href="https://www.pulumi.com/registry/packages/azure-native/api-docs/app/managedenvironment/">here</a>. And the Microsoft docs for the managed environment can be found <a href="https://learn.microsoft.com/en-us/azure/templates/microsoft.app/managedenvironments?pivots=deployment-language-bicep">here</a>.</p><p>⚠️ Concepts I picked up here:</p><ul><li><code>app_logs_configuration</code>: Cluster configuration which enables the log daemon to export app logs to a destination. Currently only “log-analytics” is supported</li><li><code>log_analytics_configuration</code>: Log Analytics configuration, must only be provided when destination is configured as ‘log-analytics’. This leads to the creation of another resource <code>LogAnalyticsWorkspace</code> which is a workspace for storing log data. <a href="https://learn.microsoft.com/en-us/azure/container-apps/log-monitoring?tabs=bash">Microsoft doc</a>. <a href="https://www.pulumi.com/registry/packages/azure-native/api-docs/operationalinsights/workspace/">Pulumi doc</a>.</li><li><code>custom_domain_configuration</code>: Custom domain configuration allows you to map a custom domain (e.g., <a href="http://www.my-name.com">www.my-name.com</a>) to your Azure managed environment or specific resources within it. This is often necessary when you want your application to be accessible via a branded domain name rather than the default Azure-generated domain name.</li></ul><h3 id="🛜-Network"><strong>🛜 Network</strong></h3><p>Network resources in Azure are used to connect and isolate resources in Azure. They provide security and control over the traffic flow between resources. The network itself is a huge topic, here I will just list the basic network settings and points to configure Azure infrastructure with Pulumi. More about <a href="https://docs.microsoft.com/en-us/azure/virtual-network/">Azure Network</a>.</p><p>Things to consider when creating a Network:</p><p><strong>1. Virtual Network (VNET):</strong> A virtual network is a network that is logically isolated from other networks in Azure. It allows you to control the flow of traffic between resources in the network and between the network and the internet. More about <a href="https://learn.microsoft.com/en-us/azure/virtual-network/virtual-networks-overview">Azure Virtual Network</a>.</p><p>⚠️ Concepts I picked up here:</p><ul><li><code>address_space</code>: The address space of the virtual network. This is the range of IP addresses that the virtual network can use. Normally, you can use the default address space <code>10.0.0./16</code>.</li><li><code>subnets</code>: The subnets of the virtual network. Subnets are used to divide the virtual network into smaller networks. Each subnet has its own range of IP addresses. Normally, you can use the default subnet <code>10.0.1.0/24</code> for <code>web-subnet</code> and <code>10.0.1.0/24</code> for <code>db-subnet</code>. <a href="https://techcommunity.microsoft.com/t5/itops-talk-blog/configuring-azure-virtual-network-subnets-with-cidr-notation/ba-p/2047809">Here</a> is a good article about CIDR notation.</li><li><code>security_group</code>: Network Security Groups are a fundamental feature in Azure used to manage and control network traffic to and from Azure resources in a virtual network. NSGs act as a firewall at the subnet or network interface level, allowing or deniying traffic based on security rules. <a href="https://learn.microsoft.com/en-us/azure/virtual-network/security-overview">More</a> about Azure Network Security. <code>priority</code> in <code>security_rule</code> is the priority of the rule. The lower the number, the higher the priority. It does not imply the importance of one rule over another but rather dictates which rule is evaluated first when a network packet is processed.</li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> Inomad </tag>
            
            <tag> Infrastructure </tag>
            
            <tag> Azure </tag>
            
            <tag> Pulumi </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Inomad Diary-04-Infrastructure-02-Implementations-02-Create a Pulumi Project</title>
      <link href="/2024/07/19/Inomad%20Dairy/04-Infrastructure/02-Implementation/Create%20a%20Pulumi%20Project.html"/>
      <url>/2024/07/19/Inomad%20Dairy/04-Infrastructure/02-Implementation/Create%20a%20Pulumi%20Project.html</url>
      
        <content type="html"><![CDATA[<h3 id="🔎-Intro"><strong>🔎 Intro</strong></h3><p>In the last post, I discussed some points of setting up Azure for Inomad’s infrastructure. In this post, I will be creating a Pulumi project and discuss some interesting points about the initial setup and considerations.</p><span id="more"></span><h3 id="❓-What-is-Pulumi"><strong>❓ What is Pulumi?</strong></h3><p>Pulumi is an open-source infrastructure as code tool that allows you to define, deploy, and manage cloud infrastructure using familiar programming languages. It supports multiple cloud providers, including Azure, AWS, Google Cloud, and Kubernetes. Pulumi enables you to write infrastructure code using languages like Python, TypeScript, Go, and C#, providing a more flexible and powerful way to manage cloud resources, which in contrast to traditional IaC tools like Terraform or CloudFormation that use declarative configuration files.</p><h3 id="📚-Initial-Steps"><strong>📚 Initial Steps</strong></h3><h3 id="1-Pulumi-Projects-and-Stacks">1. Pulumi Projects and Stacks</h3><p>During the initial setup of a Pulumi project, you will encouter some new concepts from Pulumi such as <a href="https://www.pulumi.com/docs/concepts/projects/">projects</a> and <a href="https://www.pulumi.com/docs/concepts/stack/">stacks</a>.</p><h3 id="2-Pulumi-State-Management">2. Pulumi State Management</h3><p>One another one worth mentioning is when you create a stack, the <code>orgName</code> depends on whether you choose to use <a href="https://www.pulumi.com/docs/concepts/state/#using-a-self-managed-backend">Pulumi Cloud or Self-managed Backends</a> to mange your Pulumi state files. Since I am using self-managed backend, below are some key points to consider.</p><p><strong>Where to Store State Files</strong>:<br>Decide where to store your state files: You can store your state files locally, in a shared file system, or in a cloud storage bucket like Azure Blob Storage or AWS S3.</p><p>If you choose Azure Blog Storage like me, here are the links to get started:</p><ul><li><a href="%5D(https://www.pulumi.com/docs/concepts/state/#using-a-self-managed-backend)">Azure Blob Storage for Pulumi state</a></li><li><a href="https://azure.microsoft.com/en-gb/products/storage/blobs">Azure Blob Storage</a></li></ul><p>The general steps to set up an Azure Blob Storage for Pulumi state are:</p><ul><li>Create a resource group if you haven’t done so to hold the storage account (e.g., <code>rg-dev-storage</code>).</li><li>Create a storage account in the resource group. (e.g., <code>dev-storage</code>)<br>⚠️ Note here, for Azure storage account, you can only name it with Lowercase letters and numbers and must be between 3 and 24 characters long. It is also universally unique so it’s a good idea to include a unique name like your organization name or project name in it.(e.g., <code>&lt;your-project-name&gt;devstorage</code>)Sometimes the error message is not clear and you might get stuck here. <a href="https://learn.microsoft.com/en-us/azure/azure-resource-manager/management/resource-name-rules">Resource Name Rules</a></li><li>Create a Blob container in the storage account to hold the state files specifically for your Pulumi project.(e.g., <code>state-container</code>)</li><li>Get the storage account key and set environment variable <code>AZURE_STORAGE_KEY</code> to the name of it for Pulumi to use to store the state files in the Azure Blob Storage.</li><li>Create and Microsoft Entra user group for developers in the current subscription and assign the <code>Storage Blob Data Contributor</code> role to the group. This is to allow developers to access the storage account and container to read and write the state files.</li><li>Login to Pulumi backend using the Azure Blob Storage account and container URL and the storage account key.</li></ul><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">pulumi login azblob://<span class="token operator">&lt;</span>blob-container-name<span class="token operator">></span>?storage_account<span class="token operator">=</span><span class="token operator">&lt;</span>storage-account-name<span class="token operator">></span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h3 id="3-Pulumi-Stack-Initialization">3. Pulumi Stack Initialization</h3><p>Once you have set up the backends for Pulumi state, you can now initialize a Pulumi project and stack.</p><ul><li>Create a new stack for the project. Try to use the same name as the environment you are working on (e.g., <code>dev</code>, <code>staging</code>, <code>prod</code>). So the suggested practice here is you creating a stack for each environment you are working on.</li></ul><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">pulumi stack init devpulumi config <span class="token builtin class-name">set</span> azure:subscriptionId <span class="token operator">&lt;</span>subscription-id<span class="token operator">></span> <span class="token parameter variable">--stack</span> devpulumi stack <span class="token keyword">select</span> dev<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><ul><li>When you create a stack you will be asked to specify a secret provider. You can choose to use the default secret provider <code>passphrase</code> or use a custom secret provider. If you choose to use a custom secret provider, you will need to provide the <a href="https://www.pulumi.com/docs/cli/commands/pulumi_stack_change-secrets-provider/">URL</a> of the secret provider. For example, you can use Azure Key Vault as a secret provider. <a href="https://azure.microsoft.com/en-gb/services/key-vault/">Azure Key Vault</a></li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> Inomad </tag>
            
            <tag> Infrastructure </tag>
            
            <tag> Azure </tag>
            
            <tag> Pulumi </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Inomad Diary-04-Infrastructure-02-Implementations-01-Azure Setup</title>
      <link href="/2024/07/18/Inomad%20Dairy/04-Infrastructure/02-Implementation/Azure%20Setup.html"/>
      <url>/2024/07/18/Inomad%20Dairy/04-Infrastructure/02-Implementation/Azure%20Setup.html</url>
      
        <content type="html"><![CDATA[<h3 id="🔎-Intro"><strong>🔎 Intro</strong></h3><p>This post is the beginning of the implementation phase for Inomad’s infrastructure on Azure. As the usual like other posts in Inomad series, I will just highlight the key points I’ve encountered instead of making the series an instruction of ‘How to’. This post will cover the initial steps and considerations for deploying Inomad’s infrastructure on Azure.</p><span id="more"></span><h3 id="❓-Why-Azure"><strong>❓ Why Azure?</strong></h3><p>Azure is chosen as the cloud provider for Inomad’s infrastructure due to one simple fact that it seems to be the best choice to integrate with the existing Microsoft ecosystem. As Microsoft provides a wide range of services like Office 365, Teams, and Azure, it makes sense to leverage Azure for Inomad’s infrastructure. Also comparing AWS, Azure seems to have more cost-effective database solutions and better integration for SQL based applications.</p><h3 id="📚-Initial-Steps"><strong>📚 Initial Steps</strong></h3><h3 id="1-Azure-Account-Setup">1. Azure Account Setup</h3><p>Setting up an Azure account is quite straightforward. But miandering through the Azure portal and Microsoft 365 admin center can be a bit tricky as you are managing multiple services at once.</p><h3 id="2-Creating-Multiple-Environments">2. Creating Multiple Environments</h3><p><strong>Development Environments:</strong></p><ul><li>Development Environment: This environment is where the development team will work on the application. It will have the necessary resources for development and testing.</li><li>Staging Environment: This environment is used for testing the application before it goes live. It will have a replica of the production environment to ensure that the application works as expected.</li><li>Production Environment: This is the live environment where the application will be hosted and accessed by users.</li></ul><p><strong>Reasongs for Multiple Environments:</strong></p><ul><li>Development and Testing: Developers need a dedicated envrionments (dev) to build and test new features without affecting the live product. A staging environment allows for thorough testing in conditions similar to production before deploying changes.</li><li>Quality Assurance: Ensures that new features and bug fixes are tested in isolation, preventing untested code from reaching production. QA teams can validate functionality and performance in a controlled setting.</li><li>Risk Mitigation: Reduces the risk of introducing bugs or performance issues in the live environment. Allows for rollback and troubleshooting in a non-critical environment if issues arise.</li><li>Performance Testing: Staging environments can simulate real-world loads and conditions to test application performance and scalability.</li><li>User Acceptance Testing: Provides a space for end-users or stakeholders to validate new features and changes before they go live. Ensures that the changes meet business requirements and user expectations.</li><li>Configuration Managements: Different environments can be configured with different settings, allowing for configuration testing and validation. Ensures that configuration changes do not negatively impact the production envrionment.</li><li>CI/CD: Support automtaed build, test, and deployment pipelines, enabling faster and more reliable releases. Each environment can have its own CI/CD pipeline, ensuring taht code is properly tested at each stage.</li><li>Compliance and Security: Provides separate environments to ensure compliance with security and regulatory requirements. Allows for security testing and validation in an isolated environment.</li></ul><h3 id="3-Creating-Environments-in-Azure">3.Creating Environments in Azure:</h3><p>There are main two ways to create environments in Azure:<br><strong>Resource Groups:</strong><br>Resource groups are logical containers that hold related resources for an Azure solution. They help manage and organize resources, control access, and simplify billing. You can create a resource group for each environment (dev, staging, production) and deploy resources to the respective group. <a href="https://docs.microsoft.com/en-us/azure/azure-resource-manager/management/manage-resource-groups-portal">More on Resource Groups</a></p><p><strong>Pros</strong>:</p><ul><li>Simplified Management: Easier to manage and navigate within a single subscription.</li><li>Cost Efficiency: Single subscription billing can simplify tracking and possibly reduce costs.</li><li>Permissions Management: Fine-grained access control using Azure RBAC within the same subscription.</li><li>Resource Limits: Quicker setup and fewer resource limitations compared to separated subscriptions.</li></ul><p><strong>Cons</strong>:</p><ul><li>Isolation: Less isolation between environments; issues in one environment can potentially affect others.</li><li>Resource Limits: All environments share the subscription’s resource limits, which can be restrictive.</li><li>Complexity: Though setup is simple but actually managing multiple environments within a single subscription can be complex and require careful organization.</li></ul><p><strong>Azure Subscriptions:</strong><br>Azure subscriptions are used to manage billing, access control, and resource limits. You can create separate subscriptions for each environment to isolate resources and manage costs. This approach provides more granular control over resource usage and access. <a href="https://abouttmc.com/glossary/azure-subscription/">More on Azure Subscriptions</a></p><p><strong>Pros</strong>:</p><ul><li>Isolation: Complete isolation between environments, reducing risk of cross-environment impact.</li><li>Resource Limits: Each environment has its own set of resource limits, avoiding conflict.</li><li>Billing: Seprate billing and cost tracking for each environment, enhancing financial management.</li><li>Compliance: Easier to manage compliance and security requirements with separate subscriptions.</li></ul><p><strong>Cons</strong>:</p><ul><li>Mangagement: More complex to manage multiple subscriptions, requiring careful organization and access control.</li><li>Cost: Potentially higher costs when scaling up due to separate billing and resource limits for each environment.</li><li>Permissions Management: requires separate RBAC configurations for each subscriptoin, which can be cumbersome.</li><li>Complex Setup: Initial setup and configuration for multiple subscriptions can be more tim-econsuming and complex.</li></ul><p>Depending on the specific requirements and constraints of the project, you can choose the most suitable approach for creating environments in Azure.</p><p>Once the envrionments are created, we can move to next steps.</p><h3 id="⚠️-Next-Steps"><strong>⚠️ Next Steps</strong></h3><p>In the next step, I will be creating a Pulumi project to manage the infrastructure for Inomad on Azure. During the initial steps, we will come back to Azure to create necessary resource, namely Azure Blob Storage for storing the state file of the Pulumi project.</p>]]></content>
      
      
      
        <tags>
            
            <tag> Infrastructure </tag>
            
            <tag> Inomad Diary </tag>
            
            <tag> Azure </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Inomad Diary-04-Infrastructure-01-Readings-Auditing Monitoring and Logging</title>
      <link href="/2024/07/07/Inomad%20Dairy/04-Infrastructure/01-Readings/Auditing%20Monitoring%20and%20Logging.html"/>
      <url>/2024/07/07/Inomad%20Dairy/04-Infrastructure/01-Readings/Auditing%20Monitoring%20and%20Logging.html</url>
      
        <content type="html"><![CDATA[<h3 id="🔍-Intro"><strong>🔍 Intro</strong></h3><p>Auditing, monitoring, and logging are critical components in the realm of cloud computing, ensuring operational integrity, security, and compliance. In this post, I will discuss the importance of auditing, monitoring, and logging in the cloud, and give it a detailed overview and study of these technologies.</p><span id="more"></span><p>Auditing, monitoring, and logging technologies play a crucial role in cloud infrastructure, providing visibility into system activities, detecting security incidents, and ensuring compliance with regulatory requirements. A comprehensive approach to auditing, monitoring, and logging is essential for organizations to maintain operational efficiency, protect sensitive information, and respond to security threats effectively.</p><h3 id="🔍-Auditing"><strong>🔍 Auditing</strong></h3><p>Auditing in clouding computing involves the systematic examination of cloud resources, configurations, and activities to verify compliance with organizational policies, regulatory requirements, and security best practices.</p><h3 id="1-Purpose-of-Auditing">1. Purpose of Auditing:</h3><p>Auditing ensures transparency, accountability, and governance across cloud envrionments by tracking and analyzing actions and changes.</p><h3 id="2-Key-Components-of-Auditing">2. Key Components of Auditing:</h3><ul><li>Audit Logs: Detailed records of actions taken within cloud services, capturing events such as resource provisioning, configuration changes, and access attempts.</li><li>Compliance Audits: Scheduled or ad-hoc assessments to validate adherence to regulatory standards (e.g., PCI DSS, GDPR) and internal policies.</li></ul><h3 id="3-Technologies">3. Technologies:</h3><ul><li>Cloud-native Tools: Platform-specific auditing solutions provided by cloud service providers (CSPs) like AWS CloudTrail, Azure Monitor, and Google CLoud Audit Logs.</li><li>Thrid-party SOlutions: Enhanced auditing capabilities for multi-cloud environments or specific compliance needs.</li></ul><h3 id="4-Best-Practices">4. Best Practices:</h3><ul><li>Configure logging for all critical services and ensure logs are retained securely.</li><li>Implement automated audit check and alerts for suspicious activites.</li><li>Regularly review audit logs and conduct audits to identify and address security gaps.</li></ul><h3 id="👀-Monitoring"><strong>👀 Monitoring</strong></h3><p>Cloud Monitoring involves real-time observation and analysis of cloud infrastructure, applications, and services to ensure performance, availability, and reliability.</p><h3 id="1-Purpose-of-Monitoring">1. Purpose of Monitoring:</h3><p>Detect and resolve issues proactively, optimize resource utilization, and meet service-level objective (SLOs) and service-level aggreements (SLAs).</p><h3 id="2-Key-Components-of-Monitoring">2. Key Components of Monitoring:</h3><ul><li>Metrics: Quantitative data points (CPU usage, latency, etc) used to assess performance and health.</li><li>Alerts: Automated notifications triggered by predefined thresholds or anomalies.</li><li>Dashboards: Visual representations of key metrics for quick insights and troubleshooting.</li></ul><h3 id="3-Technologies-2">3. Technologies:</h3><ul><li>Cloud Provider Tools: AWS CloudWatch, Azure Monitor, Google Cloud Monitoring.</li><li>Third-party Monitoring Tools: Datadog, New RElic, Prometheus for specialized metrics and analysis.</li></ul><h3 id="4-Best-Practices-2">4. Best Practices:</h3><ul><li>Define meaningful metrics aligned with business goals and application requirements.</li><li>Establish alerting mechanisms with clear escalation paths for critical issues.</li><li>Leverage monitoring data for capacity planning and performance optimization.</li></ul><h3 id="📝-Logging"><strong>📝 Logging</strong></h3><p>Logging involves the systematic recording of events, activities, and errors within cloud environments for troubleshooting, analysis, and compliance.</p><h3 id="1-Purpose-of-Logging">1. Purpose of Logging:</h3><p>Capture detailed records of operations, security events, and application behavior for retrospective analysis and audting.</p><h3 id="2-Key-Components-of-Logging">2. Key Components of Logging:</h3><ul><li>Log Types: Application logs, system logs, securityl logs, and audit logs.</li><li>Log Formats: Strructured (JSON, XML) or unstructured (text-based) formats.</li><li>Log Aggregation: Centralized collection and storage of logs for analysis and retention.</li></ul><h3 id="3-Technologies-3">3. Technologies:</h3><ul><li>Cloud-native Logging Services: AWS CloudWatch Logs, Azure Monitor Logs, Google Cloud Logging.</li><li>Open-source Logging Tools: ELK Stack (Elasticsearch, Logstash, Kibana), Fluentd, Graylog.</li></ul><h3 id="4-Best-Practices-3">4. Best Practices:</h3><ul><li>Implement log aggregation to simplify log management and analysis.</li><li>Enforce log retention policies to comply with regulatory requirements.</li><li>Use log data for troubleshooting, security incident reponse, and performance tuning.</li></ul><p>Auditing, monitoring, and logging technologies are indispensable for maintaining operational excellence and security in cloud environments. By integrating these practices into cloud deployment strategies, organizations can enhance transparency, mitigate risks, and improve overall operational efficiency.</p>]]></content>
      
      
      
        <tags>
            
            <tag> Inomad </tag>
            
            <tag> Infrastructure </tag>
            
            <tag> Auditing </tag>
            
            <tag> Monitoring </tag>
            
            <tag> Logging </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Inomad Diary-04-Infrastructure-01-Readings-Security Compliance and Governance</title>
      <link href="/2024/07/06/Inomad%20Dairy/04-Infrastructure/01-Readings/Security%20Compliance%20and%20Governance.html"/>
      <url>/2024/07/06/Inomad%20Dairy/04-Infrastructure/01-Readings/Security%20Compliance%20and%20Governance.html</url>
      
        <content type="html"><![CDATA[<h3 id="🔎-Intro"><strong>🔎 Intro</strong></h3><p>In this post, I will explore the concepts of security, compliance, and governance in modern cloud infrastructure and their importance in ensuring the confidentiality, integrity, and availability of data and services. Security, compliance, and governance are critical aspects of cloud infrastructure that help organizations protect their assets, comply with regulations, and maintain operational efficiency.</p><span id="more"></span><p>Security, compliance, and governance technologies are critical components in cloud computing, ensuring data protection, regulatory adherence, and opeartional integrity. A comprehensive approach to security, compliance, and governance is essential for organizations to mitigate risks, protect sensitive information, and maintain trust with customers and stakeholders.</p><h3 id="🔒-Security"><strong>🔒 Security</strong></h3><h3 id="1-Identity-and-Access-Mangement-IAM">1. Identity and Access Mangement (IAM):</h3><ul><li>Purpose: Controls who can access what resources in the clou denvironment.</li><li>Components: Role-based access control (RBAC), multi-facotr authentication (MFA), identity federation.</li><li>Best Practice: Principle of least privileage, regular review and audit of access permissions, integration with centralized identity providers (IdP)</li></ul><h3 id="2-Encryption">2. Encryption:</h3><ul><li>Purpose: Protects data condientiality and integrity both at trest and in transit.</li><li>Technologies: Transport Layer Security (TLS), Advanced Encryption Standard (AES), Public Key Infrastructure (PKI), Key Management Services (KMS).</li><li>Best Practices: Encrypt sensitive data before storing or transmitting, manage encryption keys securely, ensure compliance with encryption standards and regulations (e.g., GDPR, FIPS 140-2).</li></ul><h3 id="3-Network-Security">3. Network Security:</h3><ul><li>Purpose: Protects cloud network infrastructure from unauthorized access and attacks.</li><li>Technologies: Virtual Private Clouds (VPCs), network firewalls, VPNs, intrsion detection/prevention systems (IDS/IPS).</li><li>Best Practices: Implement security groups and ACLs to control traffic, monitor network traffic for anomalies, regularly update security configurations.</li></ul><h3 id="4-Endpoint-Security">4. Endpoint Security:</h3><ul><li>Purpose: Secures devices accessing cloud services to prevent malware and unauthorized access.</li><li>Thecnologies: Antivirus software, endpoint detection and response (EDR) tools, mobile device management (MDM).</li><li>Best Practices: Enforce endpoint security policies, regularly update endpoint software and patches, implement device encryption and remote wipe capabilities.</li></ul><h3 id="5-Security-Information-and-Event-Management-SIEM">5.Security Information and Event Management (SIEM):</h3><ul><li>Purpose: Centralizes logging and analysis of security events for threat detection and response.</li><li>Technologies: SIEM platform (e.g., Splunk, ELK Stack), log aggregation, correlation, and alerting.</li><li>Best Practices: Monitor and correlate security logs from multiple sources, establish baseline activity profiles, automate alerts and responses to security incidents.</li></ul><h3 id="📜-Compliance"><strong>📜 Compliance</strong></h3><h3 id="1-Audit-and-Reporting-Tools">1. Audit and Reporting Tools:</h3><ul><li>Purpose: Automates compliance assessments and generates audit report to demonstrate adherence to regulatory requirements.</li><li>Technologies: Compliance management platforms (e.g., AWS Config, Azure Policy), audit log snad reporting features.</li><li>Best Practices: Configure automated compliance checks for regulatory frameworks (e.g., GDPR, HIPAA), regularly review audit logs, and generate compliance reports for stakeholders.</li></ul><h3 id="2-Data-Loss-Prevention-DLP">2. Data Loss Prevention (DLP):</h3><ul><li>Purpose: Monitors and protects sensitive dta to prevent unauthorized access, us, or transmission.</li><li>Technologies: DLP solutions (e.g., Symantec DLP, Microsoft DLP), data classification and tagging, policy enforcement.</li><li>Best Practices: Classify sensitive data, implement data loss prevention policies and controls (e.g., encryption, data masking), monitor data access and movement.</li></ul><h3 id="3-Legal-Hold-and-E-Discovery">3. Legal Hold and E-Discovery:</h3><ul><li>Purpose: Facilitates data retention, search, and retireval for leagal and regulatory compliance purposes.</li><li>Technologies: Legal hold capabilities in cloud storage services, e-discovery tools (e.g., Microsoft 365 Compliance Center).</li><li>Best Practices: Implement retention policies for different data types, ensure legal hold mechanisms are effective and auditable, prepare for e-discovery requests with data search and retrieval capabilities.</li></ul><h3 id="4-Compliance-Automation">4. Compliance Automation:</h3><ul><li>Purpose: Enforces policies and controls through automated workflows to maintain regulatory compliance.</li><li>Technolgogies: Policy enforcement mechanisms (e.g., AWS Config Rules, Azure Policy), compliance as code, compliance automation scripts.</li><li>Best Practices: Define compliance policies as code (infrastructure as Code), automate compliance checks and remediation, integrate compliance automation with CI/CD pipelines.</li></ul><h3 id="📚-Governance"><strong>📚 Governance</strong></h3><h3 id="1-Policy-Management">1. Policy Management:</h3><ul><li>Purpose: Defines and enforces policies for resource provisioning, usage limits, and configurations to maintain control and compliance.</li><li>Technologies: Cloud-native policy engines (e.g., AWS IAM Policies, Azure Policy), custom scripts and automation tools.</li><li>Best Practices: Establish clear policies for resource allocation, enforece policies using automated control, regularly review and updtae policies based on business requirements and regulatory chagnes.</li></ul><h3 id="2-Cost-Management">2. Cost Management:</h3><ul><li>Purpose: Tracks and optimizes cloud expenditures to align with budget constraints and financial goals.</li><li>Technologies: Cost management tools (e.g., AWS Cost Expolrer, Azure Cost Management), buddgeting and forecasting features.</li><li>Best Practices: Monitor cloud spending in real-time, implement cost allocation tags, optimize resource usage (e.g., rightsizing, reserved instances), set budget alerts and limits.</li></ul><h3 id="3-Performance-Monitoring">3. Performance Monitoring:</h3><ul><li>Purpose: Monitors service levels, uptime, and application performance to ensure adherence to governance policies and SLAs (Service Level Agreements).</li><li>Technologies: Cloud monitoring and observability platform (e.g., AWS CloudWatch, Azure Monitor), application performance management (APM) tools.</li><li>Best Practices: Set up monitoring ddashboards for key metrics, configure alerts for performance anomalies, conduct regular performance reviews and optimization.</li></ul><h3 id="4-Risk-Management">4. Risk Management:</h3><ul><li>Purpose: Identifies, assesses, and mitigates risks to cloud infrastructure, data, and services.</li><li>Technologies: Risk assessment tools, vulnerability scanning, threat intelligence platforms.</li><li>Best Practices: Conduct regular risk assessments, implement security controls based on risk profiles, monitor threat intelligence sources for emerging risks, and vulnerabilities.</li></ul><h3 id="5-Change-Mangement">5. Change Mangement:</h3><ul><li>Purpose: Mange changes to cloud environments through standardized processes to minimize disruptions and maintain stability.</li><li>Technologies: Change mangement systems (e.g., AWS CloudFormation, Azure DevOps), version control systems.</li><li>Best Practices: implement change approval workflows, automate deployment processes using CI/CD pipelines, maintain documentation and rollback procedures for changes.</li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> Inomad </tag>
            
            <tag> Infrastructure </tag>
            
            <tag> Security </tag>
            
            <tag> Compliance </tag>
            
            <tag> Governance </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Inomad Diary-04-Infrastructure-01-Readings-Messaging</title>
      <link href="/2024/07/05/Inomad%20Dairy/04-Infrastructure/01-Readings/Messaging.html"/>
      <url>/2024/07/05/Inomad%20Dairy/04-Infrastructure/01-Readings/Messaging.html</url>
      
        <content type="html"><![CDATA[<h3 id="🔎-Intro"><strong>🔎 Intro</strong></h3><p>In this post, I will explore the role of messaging systems in modern cloud infrastructure and their importance in enabling communication between distributed components, services, and applications. Messaging systems play a crucial role in building scalable, reliable, and decoupled architectures that can handle complex workflows and interactions in cloud environments.</p><span id="more"></span><p>Messaging is a fundamental concept in distributed systems, allowing components to communicate asynchronously and decoupling senders and receivers. Messaging systems are essential components of modern cloud infrastructure, providing a reliable and efficient way to enable communication between distributed components, services, and applications. These systems facilitate asynchronous communication, decoupling senders and receivers, and ensuring reliable message delivery in distributed environments.</p><h3 id="📩-Messaging-Protocols-and-Standards"><strong>📩 Messaging Protocols and Standards</strong></h3><h3 id="1-AMQP-Advanced-Message-Queuing-Protocol">1. AMQP (Advanced Message Queuing Protocol)</h3><ul><li>Architecture: Standardized protocol for message-oriented middleware.</li><li>Use Cases: Enterprise messaging, IoT, financial systems.</li><li>Advantages: Interoperability, reliability, support for complex routing.</li><li>LimitationsL: Complexity, performance overhead in some use cases.</li></ul><h3 id="2-MQTT-Message-Queuing-Telemetry-Transport">2. MQTT (Message Queuing Telemetry Transport)</h3><ul><li>Architecture: Lightweight, publish-subscribe network protocol.</li><li>Use Cases: IoT, mobile messaging, low-bandwidth or unreliable networks.</li><li>Advantages: Lightweight, efficient for constrained environments.</li><li>Limitations: Limited quality of service options, not ideal for high-throughpout scenarios.</li></ul><h3 id="3-STOMP-Simple-Text-Oriented-Messaging-Protocol">3. STOMP (Simple Text Oriented Messaging Protocol)</h3><ul><li>Architecture: Simple, text-based protocol for message-oriented middleware.</li><li>Use Cases: WebSockets, real-time appllications.</li><li>Advantages: Simplicity, ease of use.</li><li>Limitations: Limited features compared to AMQP, less secure.</li></ul><h3 id="💬-Messaging-Patterns"><strong>💬 Messaging Patterns</strong></h3><h3 id="1-Point-to-Point">1. Point-to-Point</h3><ul><li>Description: One sender and one receiver.</li><li>Use Cases: task queues, load distribtuion.</li><li>Examples: AWS SQS, RabbitMQ (queue mode).</li></ul><h3 id="2-Publish-Subscribe">2. Publish-Subscribe</h3><ul><li>Description: One sender and multiple receivers.</li><li>Use Cases: Event distribution, notification systems.</li><li>Examples: Apache Kafka, AWS SNS, Google Pub/Sub</li></ul><h3 id="⚙️-Messaging-Technologies"><strong>⚙️ Messaging Technologies</strong></h3><h3 id="1-Apache-Kafka">1. Apache Kafka</h3><ul><li>Architecture: Distributed event streaming platform.</li><li>Use Cases: Real-time data pipelines, stream processing, log aggregation.</li><li>Advantages: High throughput, fault tolerance, scalabitliy.</li><li>Limitations: Requires management of Kafka brokers and ZooKeeper, complex setup.</li></ul><h3 id="2-RabbitMQ">2. RabbitMQ</h3><ul><li>Architecture: Open-source message broker implementing AMQP.</li><li>Use Cases: Task queues, microservices communication.</li><li>Advantages: Easy to use, supports multiple messaging protocols and flexibility.</li><li>Limitations: Performance overhead with complex routing, single point of failure without clustering.</li></ul><h3 id="3-Amazon-SQS">3. Amazon SQS</h3><ul><li>Architecture: Fully managed message queuing service.</li><li>Use Cases: Decoupling applications, distributed systems.</li><li>Advantages: Serverless, fully managed, integrated with AWS ecosystem.</li><li>Limitations: Limited to point-to-point communication, eventual consistency.</li></ul><h3 id="4-Google-Pub-Sub">4. Google Pub/Sub</h3><ul><li>Architecture: Scalable event ingestion and delivery service, fully managed, real-time messaging.</li><li>Use Cases: Real-time analytics, event-driven architectures.</li><li>Advantages: Scalability, reliability, global availability and integration with Google Cloud services.</li><li>Limitations: Limited to Google Cloud Platform, pricing based on usage.</li></ul><h3 id="🔴-Comparisons"><strong>🔴 Comparisons</strong></h3><table><thead><tr><th><em>Feature</em></th><th><em>Apache Kafka</em></th><th><em>RabbitMQ</em></th><th><em>Amazon SQS</em></th><th><em>Google Pub/Sub</em></th></tr></thead><tbody><tr><td><strong>Message Model</strong></td><td>Log-based, Pub/Sub</td><td>Queue, Pub/Sub</td><td>Queue</td><td>Pub/Sub</td></tr><tr><td><strong>Delivery Guarantees</strong></td><td>At least once, Exactly once</td><td>At most once, At least once</td><td>At least once</td><td>At least once</td></tr><tr><td><strong>Scalability</strong></td><td>High</td><td>Moderate</td><td>High</td><td>High</td></tr><tr><td><strong>Ease of Use</strong></td><td>Moderate</td><td>Moderate</td><td>High</td><td>High</td></tr><tr><td><strong>Latency</strong></td><td>Low</td><td>Moderate</td><td>Low</td><td>Moderate</td></tr><tr><td><strong>Throughput</strong></td><td>High</td><td>Moderate</td><td>Moderate</td><td>High</td></tr><tr><td><strong>Management Overhead</strong></td><td>High</td><td>Moderate</td><td>Low</td><td>Low</td></tr><tr><td><strong>Best For</strong></td><td>Real-time data streaming</td><td>Task queues, microservices</td><td>Simple queues, AWS integration</td><td>Event-driven architectures</td></tr></tbody></table><h3 id="✅-Best-Practices"><strong>✅ Best Practices</strong></h3><h3 id="1-Design-Considerations">1. Design Considerations</h3><ul><li>Decoupling: Ensure loose coupling between services to enhance scalability and resilience.</li><li>Message Durability: Choose appropriate message durability options based on use cases. (e.g. persistent queues for critical data).</li><li>Scalability: Design for horizontal scalability to handle increased load.</li><li>Security: Implement encryption, authentication, and authorization to secure messaging systems.</li></ul><h3 id="2-Monitoring-and-Mangaement">2. Monitoring and Mangaement</h3><ul><li>metrics: Monitor key metrics like message rate, latency, queue depth, and error rates.</li><li>Alerts: Set up alerts for critical thresholds to ensure timely response to issues.</li><li>Logging: Maintain comprehensive logs for debugging and autidting purposes.</li></ul><h3 id="3-Performance-Optimization">3. Performance Optimization</h3><ul><li>Batching: Use message batching to reduce overhead and increase throughput.</li><li>Compression: Enable compression to reduce message size and bandwith usasge.</li><li>Partitioning: Implement partitioning strategies for load balancing and parallel processing.</li></ul><p>Understanding and leveraging messaging technologies is crucial for building scalable, resilient, and efficient cloud architectures. By selecting the appropriate technology and implementing best practices, cloud engineers can ensure robust communication between services in diverse and demanding environments.</p>]]></content>
      
      
      
        <tags>
            
            <tag> Inomad </tag>
            
            <tag> Infrastructure </tag>
            
            <tag> Messaging </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Inomad Diary-04-Infrastructure-01-Readings-Development and Deployment</title>
      <link href="/2024/07/04/Inomad%20Dairy/04-Infrastructure/01-Readings/Development%20and%20Deployment.html"/>
      <url>/2024/07/04/Inomad%20Dairy/04-Infrastructure/01-Readings/Development%20and%20Deployment.html</url>
      
        <content type="html"><![CDATA[<h3 id="🔎-Intro"><strong>🔎 Intro</strong></h3><p>In this post, I will explore the role of development and deployment technologies in modern cloud infrastructure and their importance in building scalable, reliable, and efficient web applications and services. Development technologies encompass a wide range of tools, frameworks, and practices that enable developers to create, deploy, and maintain software solutions in the cloud.</p><span id="more"></span><p>Development and deployment technologies play a crucial role in modern cloud infrastructure by providing developers with the tools and frameworks needed to build scalable, reliable, and efficient web applications and services. These technologies enable developers to write code, test applications, deploy updates, and monitor performance in a cloud environment, ensuring that software solutions meet the needs of users and organizations.</p><h3 id="🔗-Programming-Languages"><strong>🔗 Programming Languages</strong></h3><ul><li>Python: Widely used for its simplicity and readability. Strong support in automation, scripting, and integrating with cloud services.</li><li>Java: Robust, platform-independent, and widely used in large-scale enterprise applications. Supports multithreading and high-performance systems.</li><li>Go: Known for its performance and efficiency in cloud-native applications. It’s statically typed and compiled, which help inbuilding reliable and efficient software.</li><li>JavaScript/Node.js: Essential for serverless architectures and building APIs. Offers a non-blocking I/O model, making it efficient for real-time applications.</li></ul><h3 id="📦-Frameworks-and-Libraries"><strong>📦 Frameworks and Libraries</strong></h3><ul><li>Flask/Django (Python): Flask is a lightweight web framework, while Django is a full-stack framework with built-in ORM and admin interface.</li><li>Spring Boot (Java): Simplifies microservices development. Integrates well with cloud platform like AWS and Azure.</li><li>Express.js (Node.js): Minimalist web framework for building RESTful APIs and server-side applications.</li><li>Gin (Go): Lightweight and fast web framework for building high-performance APIs and microservices.</li></ul><h3 id="🧑‍💻-Integrated-Development-Environments-IDEs"><strong>🧑‍💻 Integrated Development Environments (IDEs)</strong></h3><ul><li>VS Code: Highly extensible with numerous plugins for cloud development, including Docker, Kubernetes, and cloud service integrations.</li><li>IntelliJ IDEA: Comprehensive support for Java and other JVM languages. Rich features for cloud development and integration.</li><li>PyCharm: Best suited for Python development with advanced features for debugging, testing, and deployment.</li><li>Eclipse: Versatile IDE supporting multiple languages, popular for Java development.</li></ul><h3 id="🎮-Version-Control"><strong>🎮 Version Control</strong></h3><ul><li>Git: Widely used for distributed version control. Essential for collaboration, code review, and continuous integration/continuous deployment (CI/CD) pipelines.</li><li>GitHub/GitLab: Popular platform for hosting Git repositories, enabling collaboration, issue tracking, and project management, and integrated with cloud services.</li></ul><h3 id="🔧-Development-Practices"><strong>🔧 Development Practices</strong></h3><ul><li>CI/CD pipelines automate the build, test, and deployment processes, ensuring code quality and reliability.</li><li>Infrastructure as Code (IaC): Managing and provisioning computing infrastructure through machine-readable definition files. (e.g. Terraform, CloudFormation, Pulumi)</li></ul><h3 id="🫙-Containerization-and-Orchestration"><strong>🫙 Containerization and Orchestration</strong></h3><ul><li>Docker: Standar for containerizing applications. Allows for consistent environments across development, testing, and production.</li><li>Kubernetes: Industry-standard for managging containerized applications at scale. Offers features like automated deployment, scaling, and mangement of containerized applications.</li></ul><h3 id="🖥️-Serverless-Architecture"><strong>🖥️ Serverless Architecture</strong></h3><ul><li>AWS Lambda: Allows running code without provisioning servers. Integrated with other AWS services.</li><li>Azure Functions: Event-driven serverless compute service. Supports multiple languages and integrates with Azure services.</li><li>Google Cloud Functions: Lightweight, event-based, asynchronous compute solution for cloud applications.</li></ul><h3 id="📁-Databases"><strong>📁 Databases</strong></h3><ul><li>Relational Databases: SQL-based databases like MySQL, PostgreSQL, Oracle, and Microsoft SQL Server.</li><li>NoSQL Databases: Document stores (MongoDB, CouchDB), Key-value stores (Redis, DynamoDB), Column-family stores (Cassandra, HBase), Graph databases (Neo4j, Amazon Neptune).</li></ul><p>More details on databases can be found in the [previous post](<a href="https://dogecat0.github.io/2024/07/04/Inomad%20Dairy/Inomad">https://dogecat0.github.io/2024/07/04/Inomad Dairy/Inomad</a> Diary-04-Infrastructure-01-Readings05-Database.html).</p><h3 id="📈-Monitoring-and-Logging"><strong>📈 Monitoring and Logging</strong></h3><ul><li>Prometheus: Open-source monitoring and alerting toolkit. Collects metrics from monitored targets and stores them in a time-series database.</li><li>Grafana: Visualization tool for monitoring data. Integrates with various data sources and provides customizable dashboards.</li><li>ELK Stack (Elasticsearch, Logstash, Kibana): Centralized logging and log analysis solution. Elasticsearch stores and indexes logs, Logstash processes and forwards logs, and Kibana visualizes log data.</li></ul><h3 id="🔒-Security-and-Compliance"><strong>🔒 Security and Compliance</strong></h3><ul><li>OWASP Top 10: List of the most critical security risks to web applications. Includes vulnerabilities like injection, broken authentication, and sensitive data exposure.</li><li>SAST (Static Application Security Testing) and DAST (Dynamic Application Security Testing) tools help identify and remediate security vulnerabilities in applications.(e.g. SonarQube, OWASP ZAP)</li></ul><h3 id="✅-Best-Practices"><strong>✅ Best Practices</strong></h3><ul><li>Automate Everything: Automate all aspects of deployment, from code commits to production rollouts, to reduce human error and increase consistency.</li><li>Use Version Control: Ensure all configurations and scripts are stored in version control systems like Git to track changes and facilitate rollback if needed.</li><li>Immutable Infrastrucutre: Deploy applications on immutable infrastructure where servers are not modified post-deployment. Instaed, deploy new serves with the new configuration.</li><li>Blue-Green Deployment: Minimize downtime and reduce risk by deploying new versions alongside exsiting ones and swtiching traffic only when the new version is verified.</li><li>Canary Releases: Gradually roll out chagnes to a small subset of users before full deployment, allowing for real-world testing and reducing risk.</li><li>Monitoring and Logging: Implement robust monitoring and logging to ensure visibility into application performance and quick detection of issues.</li><li>Security and Compliance: Integrate security checks into the CI/CD pipeline to ensure compliance with security standards and policies.</li></ul><p>Understanding and effectively utilizing development and deployment technologies is crucial for cloud engineers. These tools and practices not only enhance the efficiency and reliability of software delivery but also ensure that applications are secure, scalable, and resilient.</p>]]></content>
      
      
      
        <tags>
            
            <tag> Inomad </tag>
            
            <tag> Infrastructure </tag>
            
            <tag> Development </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Inomad Diary-04-Infrastructure-01-Readings-Database</title>
      <link href="/2024/07/04/Inomad%20Dairy/04-Infrastructure/01-Readings/Database.html"/>
      <url>/2024/07/04/Inomad%20Dairy/04-Infrastructure/01-Readings/Database.html</url>
      
        <content type="html"><![CDATA[<h3 id="🔎-Intro"><strong>🔎 Intro</strong></h3><p>In this post, I will explore the role of databases in modern cloud infrastructure and their importance in storing, managing, and retrieving data for web applications and services. Databases are a critical component of cloud infrastructure, providing a structured and efficient way to store and access data for various use cases.</p><span id="more"></span><p>Databases play a crucial role in modern cloud infrastructure by providing a structured and efficient way to store, manage, and retrieve data for web applications and services. Databases are essential components of cloud-based solutions, enabling organizations to store and access data securely and efficiently.</p><h3 id="🔗-Relational-Databases-RDBMS"><strong>🔗 Relational Databases (RDBMS)</strong></h3><h3 id="1-SQL-based-Databases">1. SQL-based Databases:</h3><ul><li>MySQL: Open-source, widely used for web applications. It supports various storage engines like InnoDb and MyISAM.</li><li>PostgreSQL: Open-source, known for its advanced features like full ACID compliance, complex queries, and extensibility.</li><li>Oracle Database: Enterprise-level database with robust performace, scalability, and extensive features for security and data management.</li><li>Microsoft SQL Server: Enterprise database taht integrates well with other Microsoft products and provides strong BI capabilities.</li></ul><h3 id="2-Advantages">2. Advantages:</h3><ul><li>ACID (Atomicity, Consistency, Isolation, Durability) compliance for transactional integrity.</li><li>Strong data consistency and referential integrity.</li><li>SQL provides powerful querying capabilities.</li></ul><h3 id="3-Use-Cases">3. Use Cases:</h3><ul><li>E-commerce platforms for product catalog and order management.</li><li>Financial applications for transaction processing and reporting.</li><li>Customer relationship management (CRM) systems for customer data management.</li><li>Enterprise resource planning (ERP) systems for inventory and supply chain management.</li></ul><h3 id="📁-NoSQL-Databases"><strong>📁 NoSQL Databases</strong></h3><h3 id="1-Document-Stores">1. Document Stores:</h3><ul><li>MongoDB: Schema-less, stores data in JSON-like documents. It’s suitable for applications requiring flexible schema design.</li><li>CouchDB: Also sotres data in JSON format and supports ACID transactions but sues a more robust replication model.</li></ul><h3 id="2-Key-Value-Stores">2. Key-Value Stores:</h3><ul><li>Redis: In-memory data store with support for complex data types like lists, sets, and sorted sets. It’s used for caching, session management, and real-time analytics.</li><li>DynamoDB: Fully managed NoSQL database service by AWS, designed for high availability and scalability.</li></ul><h3 id="3-Column-Family-Stores">3. Column-Family Stores:</h3><ul><li>Cassandra: Designed for high write and read throughput, suitable for large-scale distributed systems.</li><li>HBase: Built on top of Hadoop, provides real-time read/write access to large datasets.</li></ul><h3 id="4-Graph-Databases">4. Graph Databases:</h3><ul><li>Neo4j: Optimized for graph-based queries, suitable for social networks, fraud detection, etc.</li><li>Amazon Neptune: Fully managed graph database service by AWS, supports property graph and RDF graph models.</li></ul><h3 id="5-Advantages">5. Advantages:</h3><ul><li>Flexible schema design.</li><li>High scalability andperformance for specific use cases.</li><li>Ofent optimized for specific types of queries. (e.g. graph queries, key-value lookups)</li></ul><h3 id="6-Use-Cases">6. Use Cases:</h3><ul><li>Real-time big data analytics.</li><li>Content management systems for flexible content storage.</li><li>Social networks for relationship mapping.</li><li>IoT applications for time-series data storage.</li></ul><h3 id="🆕-NewSQL-Databases"><strong>🆕 NewSQL Databases</strong></h3><h3 id="1-Fetures">1. Fetures:</h3><ul><li>Aim to combine the ACID gurantees of traditional RDBMS with the scalability of NoSQL databases.</li><li>Provide SQL as the query language.</li></ul><h3 id="2-Examples">2. Examples:</h3><ul><li>Google Spanner: Golbally distributed, stronlgy consistent databae service by Google.</li><li>CockroachDB: Distributed SQL database with horizontal scalability and strong consistency.</li><li>WoltDB: In-memory database designed for high throughput and low latency.</li></ul><h3 id="3-Advantages">3. Advantages:</h3><ul><li>Strong consistency with high availability.</li><li>Support for complex queries with SQL.</li><li>Often provide automatic sharding and relication.</li></ul><h3 id="4-Use-Cases">4. Use Cases:</h3><ul><li>Financial applications requireing strong consistency.</li><li>Real-time analytics with complex queries.</li><li>E-commerce platforms with high transaction volumes.</li></ul><h3 id="☁️-Cloud-Native-Databases"><strong>☁️ Cloud Native Databases</strong></h3><h3 id="1-Managed-Servives">1. Managed Servives:</h3><ul><li>AWS RDS: Managed relational database service by AWS, supports MySQL, PostgreSQL, Oracle, SQL Server, etc.</li><li>Azure SQL Database: Fully managed relational database service by Microsoft Azure.</li><li>Google Cloud SQL: manged MySQL and PostgreSQL database service by Google Cloud.</li></ul><h3 id="2-Serverless-Databases">2. Serverless Databases:</h3><ul><li>Amazon Aurora Serverless: On-demand, auto-scaling relational database service by AWS.</li><li>Azure Cosmos DB: Globally distributed, multi-model database service wiht serverless capabilities by Microsoft Azure.</li><li>Google Firestore: Serverless, NoSQL database service by Google Cloud.</li></ul><h3 id="3-Advantages-2">3. Advantages:</h3><ul><li>Reduced operational overhead with managed services.</li><li>Auto-scaling and high availability built-in.</li><li>Integration with cloud-antive tools and services.</li></ul><h3 id="4-Use-Cases-2">4. Use Cases:</h3><ul><li>Applications iwth variable or unpredictable workloads.</li><li>Microservices architectures requiring independent data stores.</li><li>Fast-growing startups needing quick scalability.</li></ul><h3 id="✅-Best-Practices-for-Database-Management"><strong>✅ Best Practices for Database Management</strong></h3><h3 id="1-Data-Security">1. Data Security:</h3><ul><li>Encrypt data at rest and in transit.</li><li>Implement role-based access control.</li><li>Regularly audit and monitor database activity.</li></ul><h3 id="2-Performance-Optimization">2. Performance Optimization:</h3><ul><li>Use indexing to speed up queries.</li><li>Optimize queries and schema design.</li><li>Monitor database performance and tune configurations.</li></ul><h3 id="3-Scalability">3. Scalability:</h3><ul><li>Implement horizontal scaling where possible.</li><li>Use caching mechanisms to reduce load on the database.</li><li>Use sharding for distributed databases.</li></ul><h3 id="4-Backup-and-Recovery">4. Backup and Recovery:</h3><ul><li>Implement replication and automated failover.</li><li>Regularly back up data and test restoration procedures.</li><li>Use geographically distributed backups for disaster recovery.</li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> Inomad </tag>
            
            <tag> Infrastructure </tag>
            
            <tag> Database </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Inomad Diary-04-Infrastructure-01-Readings-CDN and Networking</title>
      <link href="/2024/06/30/Inomad%20Dairy/04-Infrastructure/01-Readings/CDN%20and%20Networking.html"/>
      <url>/2024/06/30/Inomad%20Dairy/04-Infrastructure/01-Readings/CDN%20and%20Networking.html</url>
      
        <content type="html"><![CDATA[<h3 id="🔎-Intro"><strong>🔎 Intro</strong></h3><p>In this post, I will explore Content Delivery Networks (CDNs) and networking technologies that play a crucial role in delivering content efficiently and securely over the internet. CDNs and networking solutions are essential components of modern cloud infrastructure, enabling fast and reliable access to web applications and services.</p><span id="more"></span><p>Content Delivery and Networking technologies play a critical role in ensuring that content and applications are delivered to users efficiently, reliably, and with low latency. These technologies encompass various services and solutions that optimize the delivery and performance of content and applications across the globe.</p><h3 id="🌐-Content-Delivery-Networks-CDNs"><strong>🌐 Content Delivery Networks (CDNs)</strong></h3><ul><li>Definition: A CDN is a distributed network of servers strategically placed around the globe to deliver content to users more quickly and reliably by caching content closer to the end-users.</li></ul><h3 id="Key-Features-of-CDNs">Key Features of CDNs:</h3><ul><li>Caching: Stores copies of content in multiple locations to reduce latency and improve load times.</li><li>Load Balancing: Distributes traffic across multiple servers to avoid overloading a single server.</li><li>Security: Provides DDoS protection, secure socket layer (SSL) encryption, and other security measures.</li><li>Content Optimization: Compresses and optimizes content for faster delivery and better performance.</li><li>Edge Computing: Enables processing and computation closer to the end-users for reduced latency.</li></ul><h3 id="Use-Cases-of-CDNs">Use Cases of CDNs:</h3><ul><li>Accelerating website performance and load times.</li><li>Streaming high-quality video content.</li><li>Distributing software updates and patches.</li><li>Protecting against DDoS attacks and other security threats.</li></ul><h3 id="🔗-Cloud-based-Networking-Technologies"><strong>🔗 Cloud-based Networking Technologies</strong></h3><ul><li>Definition: Cloud-based networking involves using cloud resources to manage, monitor, and control network traffic and services.</li></ul><p>Key Features of Cloud-based Networking:</p><h3 id="Virtual-Private-Cloud-VPC">Virtual Private Cloud (VPC)</h3><ul><li>Definition: A VPC is a private network within a cloud environment that allows users to define their own virtual network topology, including IP addresses, subnets, and routing tables.</li><li>Features:</li></ul><h3 id="1-Subnets-Divides-the-VPC-into-smaller-networks-for-better-organization-and-security">1. Subnets: Divides the VPC into smaller networks for better organization and security.</h3><h3 id="2-Route-Tables-Customizable-routing-to-control-traffic-flow-within-the-VPC-and-between-the-VPC-and-external-networks">2. Route Tables: Customizable routing to control traffic flow within the VPC and between the VPC and external networks.</h3><h3 id="3-Network-Access-Control-Lists-NACLs-Stateful-and-stateless-filtering-of-incoming-and-outgoing-traffic">3. Network Access Control Lists (NACLs): Stateful and stateless filtering of incoming and outgoing traffic.</h3><h3 id="4-Internet-Gateways-Enables-communication-between-the-VPC-and-the-internet">4. Internet Gateways: Enables communication between the VPC and the internet.</h3><h3 id="5-NAT-Gateways-Allows-instances-in-private-subnets-to-access-the-internet-while-remaining-private">5. NAT Gateways: Allows instances in private subnets to access the internet while remaining private.</h3><h3 id="Software-Defined-Networking-SDN">Software-Defined Networking (SDN)</h3><ul><li>Definition: SDN is an approach to networking that separates the control plane from the data plane, allowing network administrators to programmatically control network behavior through software applications.</li><li>Features:</li></ul><h3 id="1-Centralized-Control-Unified-management-console-for-configuring-and-managing-network-devices-and-services">1. Centralized Control: Unified management console for configuring and managing network devices and services.</h3><h3 id="2-Programmability-APIs-for-automating-network-configuration-and-management">2. Programmability: APIs for automating network configuration and management.</h3><h3 id="3-Dynamic-Routing-Real-time-traffic-management-and-path-optimization-based-on-network-conditions">3. Dynamic Routing: Real-time traffic management and path optimization based on network conditions.</h3><h3 id="4-Policy-Based-Security-Granular-control-over-network-access-and-security-policies">4. Policy-Based Security: Granular control over network access and security policies.</h3><h3 id="Elastic-Load-Balancing-ELB">Elastic Load Balancing (ELB)</h3><ul><li>Definition: ELB automatically distributes incoming application traffic across multiple targets, such as EC2 instances, containers, and IP addresses, to ensure optimal performance and availability.</li><li>Features:</li></ul><h3 id="1-Automatic-Scaling-Adjusts-capacity-to-handle-varying-traffic-levels">1. Automatic Scaling: Adjusts capacity to handle varying traffic levels.</h3><h3 id="2-Health-Checks-Monitors-the-health-of-registered-targets-and-routes-traffic-only-to-healthy-instances">2. Health Checks: Monitors the health of registered targets and routes traffic only to healthy instances.</h3><h3 id="3-Types-of-Load-Balancers-Includes-application-network-and-classic-load-balancers-tailored-for-different-use-cases">3. Types of Load Balancers: Includes application, network, and classic load balancers tailored for different use cases.</h3><h3 id="4-Cross-Zone-Load-Balancing-Distributes-traffic-evenly-across-multiple-availability-zones-for-high-availability">4. Cross-Zone Load Balancing: Distributes traffic evenly across multiple availability zones for high availability.</h3><h3 id="Global-Networking-and-Interconnectivity">Global Networking and Interconnectivity</h3><ul><li>Definition: Cloud providers offer global networking services that enable organizations to connect their on-premises data centers to cloud resources and establish secure, high-speed connections between different regions.</li><li>Features:</li></ul><h3 id="1-Private-Connectivity-Direct-connections-between-on-premises-networks-and-cloud-resources-without-traversing-the-public-internet">1. Private Connectivity: Direct connections between on-premises networks and cloud resources without traversing the public internet.</h3><h3 id="2-Global-Accelerator-Usesthe-global-network-of-a-cloud-provider-to-optimize-routing-and-improve-performance-for-internet-facing-applications">2. Global Accelerator: Usesthe global network of a cloud provider to optimize routing and improve performance for internet-facing applications.</h3><h3 id="3-Anycast-IP-Routes-user-requests-to-the-nearest-edge-location-for-reduced-latency-and-improved-performance">3. Anycast IP: Routes user requests to the nearest edge location for reduced latency and improved performance.</h3><h3 id="Network-Security">Network Security</h3><ul><li>Definition: Cloud-based networking solutions provide comprehensive security measures integrated into the cloud network infrastructure to protect against cyber threats and unauthorized access.</li><li>Features:</li></ul><h3 id="1-DDoS-Protection-Automated-defense-against-Distributed-Denial-of-Service-attacks">1. DDoS Protection: Automated defense against Distributed Denial of Service attacks.</h3><h3 id="2-Firewall-as-a-Service-FWaaS-Cloud-based-firewalls-that-protect-network-traffic">2. Firewall as a Service(FWaaS): Cloud-based firewalls that protect network traffic.</h3><h3 id="3-Encryption-Data-encryption-at-rest-and-in-transit-to-ensure-data-confidentiality">3. Encryption: Data encryption at rest and in transit to ensure data confidentiality.</h3><h3 id="4-Identity-and-Access-Management-IAM-Role-based-access-control-and-multi-factor-authentication-for-secure-network-access">4. Identity and Access Management(IAM): Role-based access control and multi-factor authentication for secure network access.</h3><h3 id="Edge-Computing">Edge Computing</h3><ul><li>Definition: Edge computing brings computation and data storage closer to the location where it is needed, reducing latency and improving performance for applications that require real-time processing.</li><li>Features:</li></ul><h3 id="1-Edge-Nodes-Locatoins-that-cache-content-and-provide-compute-power-closer-to-users">1. Edge Nodes: Locatoins that cache content and provide compute power closer to users.</h3><h3 id="2-Latency-Reduction-Processes-data-at-edge-locaitons-to-minimize-round-trip-time">2. Latency Reduction: Processes data at edge locaitons to minimize round-trip time.</h3><h3 id="3-Real-Time-Data-Processing-Enables-real-time-analytics-and-decision-making-at-the-edge">3. Real-Time Data Processing: Enables real-time analytics and decision-making at the edge.</h3><h3 id="Automation-and-Orchestration">Automation and Orchestration</h3><ul><li>Definition: Cloud-native automation and orchestration tools and APIs can automate the deployment, management, and operation of network resources, reducing manual intervention and improving efficiency.</li><li>Features:</li></ul><h3 id="1-Infrastructure-as-Code-IaC-Use-of-scripts-and-templates-to-provision-and-manage-network-infrastructure">1. Infrastructure as Code (IaC): Use of scripts and templates to provision and manage network infrastructure.</h3><h3 id="2-Auto-Scaling-Automatically-adjusts-network-resources-based-on-demand">2. Auto-Scaling: Automatically adjusts network resources based on demand.</h3><h3 id="3-Monitoring-and-Logging-Continuous-monitoring-and-logging-of-network-performance-and-events-for-troubleshooting-and-optimization">3. Monitoring and Logging: Continuous monitoring and logging of network performance and events for troubleshooting and optimization.</h3><h3 id="4-Self-Healing-Automated-detection-and-remediation-of-network-issues-without-human-intervention">4. Self-Healing: Automated detection and remediation of network issues without human intervention.</h3><h2 id="Service-Level-Agreements-SLAs">Service Level Agreements (SLAs)</h2><ul><li>Definition: Cloud providers offer SLAs that define the level of service, performance, and availability guarantees for network services, ensuring reliability and accountability.</li><li>Features:</li></ul><h3 id="1-Uptime-Guarantees-Commitments-to-network-availability-and-performance">1. Uptime Guarantees: Commitments to network availability and performance.</h3><h3 id="Performance-Metrics-Defined-benchmarks-for-network-latency-throughput-and-response-times">Performance Metrics: Defined benchmarks for network latency, throughput, and response times.</h3><h3 id="3-Compensation-Details-of-compensation-for-service-downtime-or-performance-issues">3. Compensation: Details of compensation for service downtime or performance issues.</h3><h2 id="🚀-Benefits-of-Cloud-based-Networking"><strong>🚀 Benefits of Cloud-based Networking:</strong></h2><ul><li>Flexibility: Rapid deployment and scaling of network resources.</li><li>Security: Enhanced security features and encryption for data in transit.</li><li>Cost-Efficiency: Pay-as-you-go pricing models and reduced hardware costs.</li><li>Automation: Automated provisioning, monitoring, and management of network resources.</li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> Inomad </tag>
            
            <tag> Infrastructure </tag>
            
            <tag> Cloud Computing </tag>
            
            <tag> CDN </tag>
            
            <tag> Networking </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Inomad Diary-04-Infrastructure-01-Readings-Storage Technologies in Cloud</title>
      <link href="/2024/05/29/Inomad%20Dairy/04-Infrastructure/01-Readings/Storage%20Technologies%20in%20Cloud.html"/>
      <url>/2024/05/29/Inomad%20Dairy/04-Infrastructure/01-Readings/Storage%20Technologies%20in%20Cloud.html</url>
      
        <content type="html"><![CDATA[<h3 id="🔎-Intro"><strong>🔎 Intro</strong></h3><p>In this post, I will explore the key storage technologies that underpin cloud computing. Storage is a critical component of any cloud infrastructure, enabling data persistence, retrieval, and management for applications and services.</p><span id="more"></span><p>Cloud storage technologies enable the storage, management, and retrieval of data in a scalable, flexible, and cost-effective manner. Here are the key types of cloud storage technologies:</p><h3 id="🔧-Key-Storage-Technologies-in-Cloud-Computing"><strong>🔧 Key Storage Technologies in Cloud Computing</strong></h3><h3 id="1-Object-Storage">1. Object Storage</h3><ul><li>Definition: Object storage manages data as objects, each containing the data itself, metadata, and a unique identifier.</li><li>Key Features: Scalability, durability, metadata management, cost-effectiveness.</li><li>Use Cases: Storing unstructured data, media files, backups, archives.</li></ul><h3 id="2-Block-Storage">2. Block Storage</h3><ul><li>Definition: Block storage divides data into fixed-sized blocks and stores them as separate pieces, each with its own address but without metadata.</li><li>Key Features: High performance, low latency, data consistency.</li></ul><h3 id="3-File-Storage">3. File Storage</h3><ul><li>Definition: File storage organizes data into a hierarchical structure of files and folders, similar to how data is stored on local file systems.</li><li>Key Features: Shared access, file-level operations, compatibility with existing applications.</li></ul><h3 id="4-Cloud-Database-Storage">4. Cloud Database Storage</h3><ul><li>Definition: Cloud databases provide scalable, high-performance storage for structured data, enabling efficient querying, indexing, and transaction processing.</li><li>Key Features: ACID compliance, scalability, fault tolerance.</li></ul><h3 id="5-Hybrid-Storage-Solutions">5. Hybrid Storage Solutions</h3><ul><li>Definition: Hybrid cloud storage combines on-premises storage with cloud storage services, allowing organizations to leverage the benefits of both environments.</li><li>Key Features: Flexibility, cost management, and data locality for performance-sensitive applications.</li></ul><h3 id="🚩-Considerations-for-Choosing-Storage-Technologies"><strong>🚩 Considerations for Choosing Storage Technologies</strong></h3><h3 id="1-Performance-Requirements">1. Performance Requirements</h3><ul><li>Consider the performance characteristics required by your applications, such as throughput, latency, and IOPS.</li></ul><h3 id="2-Scalability-and-Elasticity">2. Scalability and Elasticity</h3><ul><li>Evaluate the scalability and elasticity of storage solutions to accommodate growing data volumes and changing workloads.</li></ul><h3 id="3-Durability-and-Reliability">3. Durability and Reliability</h3><ul><li>Assess the durability and reliability of storage technologies to ensure data integrity and availability.</li></ul><h3 id="4-Cost-Efficiency">4. Cost Efficiency</h3><ul><li>Compare the cost structures of different storage options, including storage capacity, data transfer, and access fees.</li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> Inomad </tag>
            
            <tag> Infrastructure </tag>
            
            <tag> Cloud Computing </tag>
            
            <tag> Storage </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Inomad Diary-04-Infrastructure-01-Readings-Technologies in Cloud Computing</title>
      <link href="/2024/05/29/Inomad%20Dairy/04-Infrastructure/01-Readings/Technologies%20in%20Cloud%20Computing.html"/>
      <url>/2024/05/29/Inomad%20Dairy/04-Infrastructure/01-Readings/Technologies%20in%20Cloud%20Computing.html</url>
      
        <content type="html"><![CDATA[<h3 id="🔎-Intro"><strong>🔎 Intro</strong></h3><p>In this post, I will delve into the key infrastructure technologies that underpin cloud computing. Understanding these technologies is essential for building scalable, reliable, and secure cloud-based applications.</p><span id="more"></span><h3 id="🔧-Key-Infrastructure-Technologies-in-Cloud-Computing"><strong>🔧 Key Infrastructure Technologies in Cloud Computing</strong></h3><h3 id="1-Virtualization">1. Virtualization</h3><ul><li>Definition: Virtualization technology allows for the creation of multiple virtual environments from a single physical hardware system, enabling better utilization of resources and isolation between different environments.</li></ul><p>Types of Virtualization:</p><ul><li>Server Virtualization: Divides a physical server into multiple virtual servers, each running its own operating system and applications.</li><li>Storage Virtualization: Pools physical storage from multiple devices into a single logical storage unit.</li><li>Network Virtualization: Combines hardware and software network resources and network functionality into a single, software-based administrative entity.</li></ul><h3 id="2-Containers">2. Containers</h3><ul><li>Definition: Containers are lightweight, standalone, executable packages that contain everything needed to run an application, including code, runtime, system tools, libraries, and settings. They provide a consistent environment for applications to run across different computing environments.</li></ul><h3 id="3-Microservices-Architecture">3. Microservices Architecture</h3><ul><li>Definition: Microservices architecture is an architectural style that structures an application as a collection of small, loosely coupled services. Each service is independently deployable, scalable, and maintainable, implementing a specific business capability.</li><li>Benifits: Scalability, resilience, flexibility, and ease of deployment.</li></ul><h3 id="4-Serverless-Computing">4. Serverless Computing</h3><ul><li>Definition: Serverless computing allows developers to build and run applications without managing servers. The cloud provider automatically provisions, scales, and manages the infrastructure required to run the code.</li></ul><h3 id="5-DevOps-and-Automation-Tools">5. DevOps and Automation Tools</h3><ul><li>Definition: Tools and practices that combine software development (Dev) and IT operations (Ops) to shorten the systems development life cycle and provide continuous delivery with high software quality.</li></ul><h3 id="6-Networking-and-Content-Devliery">6. Networking and Content Devliery</h3><ul><li>Definition: Technologies that manage network infrastructure, traffic distribution, and deliver content globally with low latency and high transfer speeds.</li></ul><h3 id="7-Security-and-identity-Management">7. Security and identity Management</h3><ul><li>Definition: Technologies and practices that protect cloud infrastructure, applications, and data, and manage user identities and access rights.</li></ul><h3 id="8-Monitoring-and-Logging">8. Monitoring and Logging</h3><ul><li>Definition: Tools that monitor cloud resources, track performance metrics, and log events for troubleshooting, performance optimization, and compliance.</li></ul><h3 id="9-Data-Management-and-Analytics">9. Data Management and Analytics</h3><ul><li>Definition: Technologies that store, process, and analyze large volumes of data to extract meaningful insights and support decision-making.</li></ul><p>In the upcoming posts, I will explore each of these infrastructure technologies in more detail.🌞</p>]]></content>
      
      
      
        <tags>
            
            <tag> Inomad </tag>
            
            <tag> Infrastructure </tag>
            
            <tag> Cloud Computing Technologies </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Inomad Diary-04-Infrastructure-01-Readings-Foundations of Cloud Computing</title>
      <link href="/2024/05/27/Inomad%20Dairy/04-Infrastructure/01-Readings/Foundations%20of%20Cloud%20Computing.html"/>
      <url>/2024/05/27/Inomad%20Dairy/04-Infrastructure/01-Readings/Foundations%20of%20Cloud%20Computing.html</url>
      
        <content type="html"><![CDATA[<h3 id="🔎-Intro"><strong>🔎 Intro</strong></h3><p>From this post onwards, I will start exploring the related knowledge of infrastructure for software applications, particularly in the cloud. This post will cover the basics of cloud computing.</p><span id="more"></span><h3 id="☁️-Core-Foundations-of-Cloud-Computing"><strong>☁️ Core Foundations of Cloud Computing</strong></h3><p>Cloud computing is a technology paradigm that allows users to access and store data and applications on remote servers over the internet rather than on local servers or personal computers. This model offers flexibility, scalability, and cost efficiency. Here are the core foundations of cloud computing:</p><h3 id="1-On-Demand-Self-Service">1. On-Demand Self-Service</h3><ul><li>Definition: Users can provision computing capabilities, such as server time and network storage, as needed automatically without requiring human interaction with each service provider.</li><li>Key Features: Web-based interfaces, automation tools, API access.</li></ul><h3 id="2-Broad-Network-Access">2. Broad Network Access</h3><ul><li>Definition: Cloud services are available over the network and accessed through standard mechanisms that promote use by heterogeneous thin or thick client platforms (e.g., mobile phones, tablets, laptops, and workstations).</li><li>Key Features: Accessibility from any device, anywhere.</li></ul><h3 id="3-Resource-Pooling">3. Resource Pooling</h3><ul><li>Definition: The provider’s computing resources are pooled to serve multiple consumers using a multi-tenant model, with different physical and virtual resources dynamically assigned and reassigned according to consumer demand.</li><li>Key Features: Location independence, resource sharing, virtualization.</li></ul><h3 id="4-Rapid-Elasticity">4. Rapid Elasticity</h3><ul><li>Definition: Capabilities can be elastically provisioned and released to scale rapidly outward and inward with demand. To the consumer, the capabilities available for provisioning often appear to be unlimited and can be appropriated in any quantity at any time.</li><li>Key Features: Scalability, flexibility, auto-scaling.</li></ul><h3 id="5-Measured-Service">5. Measured Service</h3><ul><li>Definition: Cloud systems automatically control and optimize resource use by leveraging a metering capability at some level of abstraction appropriate to the type of service (e.g., storage, processing, bandwidth, and active user accounts). Resource usage can be monitored, controlled, and reported, providing transparency for both the provider and consumer of the utilized service.</li><li>Key Features: Pay-per-use, metered service, resource monitoring, billing.</li></ul><p>These core foundations are the building blocks of cloud computing that enable businesses to scale and grow without the need for significant capital investment in infrastructure. Next, I will explore the cloud service models and deployment models.</p><h3 id="🧩-Cloud-Service-Model"><strong>🧩 Cloud Service Model</strong></h3><p>Cloud computing services are typically categorized into three service models based on the level of abstraction and control they provide to users. These models are Infrastructure as a Service (IaaS), Platform as a Service (PaaS), and Software as a Service (SaaS).</p><h3 id="Infrastructure-as-a-Service-IaaS">Infrastructure as a Service (IaaS)</h3><ul><li>Definition: Provides virtualized computing resources over the internet. It offers fundamental computing resources such as virtual machines, storage, and networks.</li><li>Examples: Amazon Web Services (AWS), Microsoft Azure, Google Cloud Platform (GCP).</li><li>Use Cases: Hosting websites, running enterprise applications, backup and disaster recovery.</li></ul><h3 id="Platform-as-a-Service-PaaS">Platform as a Service (PaaS)</h3><ul><li>Definition: Provides a platform allowing customers to develop, run, and manage applications without the complexity of building and maintaining the infrastructure.</li><li>Examples: Heroku, Google App Engine, Microsoft Azure App Service.</li><li>Use Cases: Web application development, mobile application development, API development.</li></ul><h3 id="Software-as-a-Service-SaaS">Software as a Service (SaaS)</h3><ul><li>Definition: Delivers software applications over the internet, on a subscription basis, eliminating the need for organizations to install and run applications on their own computers or data centers.</li><li>Examples: Salesforce, Google Workspace, Microsoft Office 365.</li><li>Use Cases: CRM, email services, collaboration tools.</li></ul><h3 id="🚀-Cloud-Deployment-Models"><strong>🚀 Cloud Deployment Models</strong></h3><p>Cloud deployment models define the type of cloud environment based on ownership, size, and access. The common deployment models are Public Cloud, Private Cloud, Hybrid Cloud, and Community Cloud.</p><h3 id="Public-Cloud">Public Cloud</h3><ul><li>Definition: Services are delivered over the public internet and shared across multiple organizations.</li><li>Benefits: Cost-effective, scalable, no maintenance overhead.</li><li>Examples: AWS, Azure, GCP.</li></ul><h3 id="Private-Cloud">Private Cloud</h3><ul><li>Definition: Services are maintained on a private network for a single organization, providing greater control and security.</li><li>Benefits: Enhanced security, compliance, customizable.</li><li>Examples: VMware vSphere, OpenStack.</li></ul><h3 id="Hybrid-Cloud">Hybrid Cloud</h3><ul><li>Definition: Combines public and private clouds, allowing data and applications to be shared between them.</li><li>Benefits: Flexibility, cost-efficiency, improved security.</li><li>Examples: Microsoft Azure Stack, AWS Outposts.</li></ul><h3 id="Community-Cloud">Community Cloud</h3><ul><li>Definition: A collaborative effort in which infrastructure is shared between several organizations from a specific community with common concerns (e.g., security, compliance).</li><li>Benefits: Cost-sharing, improved collaboration.</li><li>Examples: Government cloud services.</li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> Inomad </tag>
            
            <tag> Infrastructure </tag>
            
            <tag> Cloud Computing </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Inomad Dairy-03-Dynamic Formset</title>
      <link href="/2024/04/30/Inomad%20Dairy/03-Dynamic%20Formset.html"/>
      <url>/2024/04/30/Inomad%20Dairy/03-Dynamic%20Formset.html</url>
      
        <content type="html"><![CDATA[<h3 id="🔎-Intro"><strong>🔎 Intro</strong></h3><p>When developing web applications that require flexible and dynamic user inputs, utilizing formsets in Django alongside JavaScript can significantly enhance user experience and functionality.</p><span id="more"></span><h3 id="Introduction"><strong>Introduction</strong></h3><p>When developing web applications that require flexible and dynamic user inputs, utilizing formsets in Django alongside JavaScript can significantly enhance user experience and functionality. This approach allows users to dynamically add or remove form instances, making the interface adaptable to various use cases. Below, I’ll share insights from implementing dynamic formsets for product components and images in a Django application.</p><h3 id="Implementing-Dynamic-Formsets-in-Django"><strong>Implementing Dynamic Formsets in Django</strong></h3><p>Formsets in Django are a powerful feature that facilitates the management of multiple forms on a single page. This is particularly useful for scenarios where you need to handle multiple instances of a form, like adding multiple product components or images before submitting them all at once. <a href="https://docs.djangoproject.com/en/5.0/topics/forms/formsets/">More details</a></p><p>Example code:</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> django<span class="token punctuation">.</span>forms <span class="token keyword">import</span> formset_factory<span class="token keyword">from</span> <span class="token punctuation">.</span>forms <span class="token keyword">import</span> ProductComponentFormProductComponentFormSet <span class="token operator">=</span> formset_factory<span class="token punctuation">(</span>ProductComponentForm<span class="token punctuation">,</span> extra<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>This code snippet initializes a formset for product components, with one extra form displayed by default.</p><p>To handle dynamic additions and deletions of form fields, you often also need initialize an empty form template in your HTML, often hidden using CSS.</p><pre class="line-numbers language-markup" data-language="markup"><code class="language-markup"><span class="token comment">&lt;!-- Hidden template for dynamic addition --></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>div</span> <span class="token attr-name">id</span><span class="token attr-value"><span class="token punctuation attr-equals">=</span><span class="token punctuation">"</span>empty-form-template<span class="token punctuation">"</span></span> <span class="token special-attr"><span class="token attr-name">style</span><span class="token attr-value"><span class="token punctuation attr-equals">=</span><span class="token punctuation">"</span><span class="token value css language-css"><span class="token property">display</span><span class="token punctuation">:</span> none<span class="token punctuation">;</span></span><span class="token punctuation">"</span></span></span><span class="token punctuation">></span></span>  &#123;&#123; formset.empty_form &#125;&#125;<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>div</span><span class="token punctuation">></span></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><h3 id="Integrating-JavaScript-for-Dynamic-Form-Management"><strong>Integrating JavaScript for Dynamic Form Management</strong></h3><p>Use JavaScript to handle user actions like adding a new form or removing an existing one. This involves cloning the hidden template and updating attributes to maintain unique form identifiers.</p><pre class="line-numbers language-javascript" data-language="javascript"><code class="language-javascript">document<span class="token punctuation">.</span><span class="token function">getElementById</span><span class="token punctuation">(</span><span class="token string">"add-button"</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">addEventListener</span><span class="token punctuation">(</span><span class="token string">"click"</span><span class="token punctuation">,</span> <span class="token keyword">function</span> <span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>  <span class="token keyword">var</span> totalForms <span class="token operator">=</span> document<span class="token punctuation">.</span><span class="token function">getElementById</span><span class="token punctuation">(</span><span class="token string">"id_form-TOTAL_FORMS"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>  <span class="token keyword">var</span> formIdx <span class="token operator">=</span> <span class="token function">parseInt</span><span class="token punctuation">(</span>totalForms<span class="token punctuation">.</span>value<span class="token punctuation">)</span><span class="token punctuation">;</span>  <span class="token keyword">var</span> newForm <span class="token operator">=</span> document<span class="token punctuation">.</span><span class="token function">getElementById</span><span class="token punctuation">(</span><span class="token string">"empty-form-template"</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">cloneNode</span><span class="token punctuation">(</span><span class="token boolean">true</span><span class="token punctuation">)</span><span class="token punctuation">;</span>  newForm<span class="token punctuation">.</span>style<span class="token punctuation">.</span>display <span class="token operator">=</span> <span class="token string">"block"</span><span class="token punctuation">;</span>  newForm<span class="token punctuation">.</span>innerHTML <span class="token operator">=</span> newForm<span class="token punctuation">.</span>innerHTML<span class="token punctuation">.</span><span class="token function">replace</span><span class="token punctuation">(</span><span class="token regex"><span class="token regex-delimiter">/</span><span class="token regex-source language-regex">__prefix__</span><span class="token regex-delimiter">/</span><span class="token regex-flags">g</span></span><span class="token punctuation">,</span> formIdx<span class="token punctuation">)</span><span class="token punctuation">;</span>  document<span class="token punctuation">.</span><span class="token function">getElementById</span><span class="token punctuation">(</span><span class="token string">"form-container"</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">appendChild</span><span class="token punctuation">(</span>newForm<span class="token punctuation">)</span><span class="token punctuation">;</span>  totalForms<span class="token punctuation">.</span>value <span class="token operator">=</span> formIdx <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">;</span><span class="token punctuation">&#125;</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>It’s also practical to set a maximum number of forms that can be added to prevent overwhelming the user and the server.</p><pre class="line-numbers language-javascript" data-language="javascript"><code class="language-javascript"><span class="token keyword">if</span> <span class="token punctuation">(</span><span class="token function">parseInt</span><span class="token punctuation">(</span>totalForms<span class="token punctuation">.</span>value<span class="token punctuation">)</span> <span class="token operator">>=</span> maxForms<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>  <span class="token function">alert</span><span class="token punctuation">(</span><span class="token string">"Maximum number of forms reached"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>  <span class="token keyword">return</span><span class="token punctuation">;</span><span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><h3 id="Handling-Deletions"><strong>Handling Deletions</strong></h3><p>Provide users the ability to remove forms, which involves adjusting the total form count and potentially handling the re-indexing of form IDs to maintain continuity.</p><pre class="line-numbers language-javascript" data-language="javascript"><code class="language-javascript">document<span class="token punctuation">.</span><span class="token function">addEventListener</span><span class="token punctuation">(</span><span class="token string">"click"</span><span class="token punctuation">,</span> <span class="token keyword">function</span> <span class="token punctuation">(</span><span class="token parameter">event</span><span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>  <span class="token keyword">if</span> <span class="token punctuation">(</span>event<span class="token punctuation">.</span>target<span class="token punctuation">.</span>classList<span class="token punctuation">.</span><span class="token function">contains</span><span class="token punctuation">(</span><span class="token string">"delete-button"</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>    <span class="token keyword">var</span> formToRemove <span class="token operator">=</span> event<span class="token punctuation">.</span>target<span class="token punctuation">.</span><span class="token function">closest</span><span class="token punctuation">(</span><span class="token string">".form-instance"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    formToRemove<span class="token punctuation">.</span><span class="token function">remove</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token keyword">var</span> totalForms <span class="token operator">=</span> document<span class="token punctuation">.</span><span class="token function">getElementById</span><span class="token punctuation">(</span><span class="token string">"id_form-TOTAL_FORMS"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    totalForms<span class="token punctuation">.</span>value <span class="token operator">=</span> <span class="token function">parseInt</span><span class="token punctuation">(</span>totalForms<span class="token punctuation">.</span>value<span class="token punctuation">)</span> <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">;</span>  <span class="token punctuation">&#125;</span><span class="token punctuation">&#125;</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="Summary"><strong>Summary</strong></h3><p>Integrating dynamic form elements into Django projects enhances user interactions and flexibility, allowing for a more responsive and intuitive interface. Using JavaScript alongside Django’s formsets makes managing complex forms straightforward, improving both the developer’s and the user’s experience.</p><p>By mastering these techniques, developers can implement more complex and user-friendly interfaces that cater to the needs of modern web applications.</p>]]></content>
      
      
      
        <tags>
            
            <tag> Inomad </tag>
            
            <tag> Django </tag>
            
            <tag> Form </tag>
            
            <tag> Formset </tag>
            
            <tag> JavaScript </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Inomad Dairy-02-Complex ProductModel</title>
      <link href="/2024/04/29/Inomad%20Dairy/02-Complex%20ProductModel.html"/>
      <url>/2024/04/29/Inomad%20Dairy/02-Complex%20ProductModel.html</url>
      
        <content type="html"><![CDATA[<h4 id="🔎-Intro"><strong>🔎 Intro</strong></h4><p>In this blog post, I’ll walk through how to model complex relationships between products using Django, specifically focusing on a scenario where products can be composed of other products, and vice versa.</p><span id="more"></span><p>This is a common requirement in industries like manufacturing, where products are often made up of various parts, each potentially being a product in itself.</p><h4 id="Modeling-the-Product-Relationships"><strong>Modeling the Product Relationships</strong></h4><p>We start by defining our Product model, which needs to handle relationships where a product can have components and also be a component of other products. Here’s how we can efficiently structure this in Django:</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> django<span class="token punctuation">.</span>db <span class="token keyword">import</span> models<span class="token keyword">class</span> <span class="token class-name">Product</span><span class="token punctuation">(</span>models<span class="token punctuation">.</span>Model<span class="token punctuation">)</span><span class="token punctuation">:</span>    name <span class="token operator">=</span> models<span class="token punctuation">.</span>CharField<span class="token punctuation">(</span>max_length<span class="token operator">=</span><span class="token number">255</span><span class="token punctuation">)</span><span class="token keyword">class</span> <span class="token class-name">ProductComponent</span><span class="token punctuation">(</span>models<span class="token punctuation">.</span>Model<span class="token punctuation">)</span><span class="token punctuation">:</span>    parent <span class="token operator">=</span> models<span class="token punctuation">.</span>ForeignKey<span class="token punctuation">(</span>Product<span class="token punctuation">,</span> on_delete<span class="token operator">=</span>models<span class="token punctuation">.</span>CASCADE<span class="token punctuation">,</span> related_name<span class="token operator">=</span><span class="token string">'components'</span><span class="token punctuation">)</span>    part <span class="token operator">=</span> models<span class="token punctuation">.</span>ForeignKey<span class="token punctuation">(</span>Product<span class="token punctuation">,</span> on_delete<span class="token operator">=</span>models<span class="token punctuation">.</span>CASCADE<span class="token punctuation">,</span> related_name<span class="token operator">=</span><span class="token string">'part_of'</span><span class="token punctuation">)</span>    quantity <span class="token operator">=</span> models<span class="token punctuation">.</span>IntegerField<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>In this setup, ProductComponent acts as a linking model between Product instances, defining a many-to-many relationship from a product to its components through the foreign keys to Product.</p><h3 id="Admin-Interface-Setup"><strong>Admin Interface Setup</strong></h3><p>To manage these relationships effectively in Django’s admin interface, we set up the models to include inlines, allowing administrators to edit product components directly within product entries:</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> django<span class="token punctuation">.</span>contrib <span class="token keyword">import</span> admin<span class="token keyword">from</span> <span class="token punctuation">.</span>models <span class="token keyword">import</span> Product<span class="token punctuation">,</span> ProductComponent<span class="token keyword">class</span> <span class="token class-name">ProductComponentInline</span><span class="token punctuation">(</span>admin<span class="token punctuation">.</span>TabularInline<span class="token punctuation">)</span><span class="token punctuation">:</span>    model <span class="token operator">=</span> ProductComponent    extra <span class="token operator">=</span> <span class="token number">1</span><span class="token keyword">class</span> <span class="token class-name">ProductAdmin</span><span class="token punctuation">(</span>admin<span class="token punctuation">.</span>ModelAdmin<span class="token punctuation">)</span><span class="token punctuation">:</span>    list_display <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token string">'name'</span><span class="token punctuation">,</span><span class="token punctuation">)</span>    inlines <span class="token operator">=</span> <span class="token punctuation">[</span>ProductComponentInline<span class="token punctuation">]</span>admin<span class="token punctuation">.</span>site<span class="token punctuation">.</span>register<span class="token punctuation">(</span>Product<span class="token punctuation">,</span> ProductAdmin<span class="token punctuation">)</span>admin<span class="token punctuation">.</span>site<span class="token punctuation">.</span>register<span class="token punctuation">(</span>ProductComponent<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>This configuration makes it straightforward to manage the complex hierarchies of products and their parts.</p><h3 id="Form-Handling"><strong>Form Handling</strong></h3><p>Handling forms for such a relationship involves using Django’s formsets to manage multiple product components within a single form interface. Here’s how you set up the forms:</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> django <span class="token keyword">import</span> forms<span class="token keyword">from</span> django<span class="token punctuation">.</span>forms <span class="token keyword">import</span> inlineformset_factory<span class="token keyword">from</span> <span class="token punctuation">.</span>models <span class="token keyword">import</span> Product<span class="token punctuation">,</span> ProductComponent<span class="token keyword">class</span> <span class="token class-name">ProductForm</span><span class="token punctuation">(</span>forms<span class="token punctuation">.</span>ModelForm<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">class</span> <span class="token class-name">Meta</span><span class="token punctuation">:</span>        model <span class="token operator">=</span> Product        fields <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">'name'</span><span class="token punctuation">]</span>ProductComponentFormSet <span class="token operator">=</span> inlineformset_factory<span class="token punctuation">(</span>    parent<span class="token operator">=</span>Product<span class="token punctuation">,</span>    model<span class="token operator">=</span>ProductComponent<span class="token punctuation">,</span>    fields<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'part'</span><span class="token punctuation">,</span> <span class="token string">'quantity'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>    extra<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span>    can_delete<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>And integrating this into a view:</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> django<span class="token punctuation">.</span>shortcuts <span class="token keyword">import</span> render<span class="token punctuation">,</span> redirect<span class="token keyword">def</span> <span class="token function">manage_product</span><span class="token punctuation">(</span>request<span class="token punctuation">,</span> product_id<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">if</span> product_id<span class="token punctuation">:</span>        product <span class="token operator">=</span> Product<span class="token punctuation">.</span>objects<span class="token punctuation">.</span>get<span class="token punctuation">(</span>pk<span class="token operator">=</span>product_id<span class="token punctuation">)</span>        form <span class="token operator">=</span> ProductForm<span class="token punctuation">(</span>instance<span class="token operator">=</span>product<span class="token punctuation">)</span>        formset <span class="token operator">=</span> ProductComponentFormSet<span class="token punctuation">(</span>instance<span class="token operator">=</span>product<span class="token punctuation">)</span>    <span class="token keyword">else</span><span class="token punctuation">:</span>        form <span class="token operator">=</span> ProductForm<span class="token punctuation">(</span><span class="token punctuation">)</span>        formset <span class="token operator">=</span> ProductComponentFormSet<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token keyword">if</span> request<span class="token punctuation">.</span>method <span class="token operator">==</span> <span class="token string">'POST'</span><span class="token punctuation">:</span>        form <span class="token operator">=</span> ProductForm<span class="token punctuation">(</span>request<span class="token punctuation">.</span>POST<span class="token punctuation">,</span> instance<span class="token operator">=</span>product <span class="token keyword">if</span> product_id <span class="token keyword">else</span> <span class="token boolean">None</span><span class="token punctuation">)</span>        formset <span class="token operator">=</span> ProductComponentFormSet<span class="token punctuation">(</span>request<span class="token punctuation">.</span>POST<span class="token punctuation">,</span> instance<span class="token operator">=</span>product <span class="token keyword">if</span> product_id <span class="token keyword">else</span> <span class="token boolean">None</span><span class="token punctuation">)</span>        <span class="token keyword">if</span> form<span class="token punctuation">.</span>is_valid<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">and</span> formset<span class="token punctuation">.</span>is_valid<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>            created_product <span class="token operator">=</span> form<span class="token punctuation">.</span>save<span class="token punctuation">(</span><span class="token punctuation">)</span>            formset<span class="token punctuation">.</span>instance <span class="token operator">=</span> created_product            formset<span class="token punctuation">.</span>save<span class="token punctuation">(</span><span class="token punctuation">)</span>            <span class="token keyword">return</span> redirect<span class="token punctuation">(</span><span class="token string">'product_list'</span><span class="token punctuation">)</span>    <span class="token keyword">return</span> render<span class="token punctuation">(</span>request<span class="token punctuation">,</span> <span class="token string">'product/manage_product.html'</span><span class="token punctuation">,</span> <span class="token punctuation">&#123;</span><span class="token string">'form'</span><span class="token punctuation">:</span> form<span class="token punctuation">,</span> <span class="token string">'formset'</span><span class="token punctuation">:</span> formset<span class="token punctuation">&#125;</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="Summary"><strong>Summary</strong></h3><p>This product model setup allows for a flexible and scalable way to manage complex product relationships in Django, making it easier to handle intricate product hierarchies and compositions for products like those in the manufacturing industry that require parent/component relationships.</p>]]></content>
      
      
      
        <tags>
            
            <tag> Inomad </tag>
            
            <tag> Django </tag>
            
            <tag> ProductModel </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Inomad-Dairy-01-ProductModel</title>
      <link href="/2024/04/23/Inomad%20Dairy/01-ProductModel.html"/>
      <url>/2024/04/23/Inomad%20Dairy/01-ProductModel.html</url>
      
        <content type="html"><![CDATA[<h3 id="🔎-Intro"><strong>🔎 Intro</strong></h3><p>As the business model of Inomad is updating constantly especially the product lines during the scaling-up stage, the product model also requires to be updated frequently.</p><span id="more"></span><p>It’s been a while since last time I update the Inomad series. The current demo on GitHub for <a href="https://github.com/Dogecat0/inomad-demo">Inomad</a> is here, but maybe slightly out of date as I’ve been constantly updating Inomad. The product lines and the product model have been the main updates recently. The categories, the materials, the colors, the sizes, the main product and variants, all these core attributes of the product model are fun to work with. Also another important part of Inomad product is the sustainable mateiral and recylcing idea. Hopefully I can share more about them soon and put website into production.</p>]]></content>
      
      
      
        <tags>
            
            <tag> Inomad </tag>
            
            <tag> Django </tag>
            
            <tag> ProductModel </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Inomad-Dairy-00-Introduction</title>
      <link href="/2023/11/10/Inomad%20Dairy/00-SES.html"/>
      <url>/2023/11/10/Inomad%20Dairy/00-SES.html</url>
      
        <content type="html"><![CDATA[<h3 id="🥳🎊"><strong>🥳🎊</strong></h3><p>I have been thinking to start a series about the e-commerce platform <a href="https://github.com/Dogecat0/inomad-demo">Inomad</a> for quite a while now. However, I was not sure about the content of the series and how to structure it. The thinking is till going on but I’ve decided to start it and modify the series as I go.</p>]]></content>
      
      
      
        <tags>
            
            <tag> Inomad </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
